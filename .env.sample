# Docker & Service Ports (defaults, change if necessary)
BACKEND_PORT=8000
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001
CHROMA_PORT_HOST=8008 # Renamed from CHROMA_PORT to avoid conflict if CHROMA_PORT is used for container internal
# COQUI_TTS_PORT=5002 # Example if Coqui TTS is added later

# Minio Configuration
MINIO_URL=minio:9000 # For access from host, use minio:9000 from within docker network
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET_RAW_DOCS=raw-documents
MINIO_BUCKET_PROCESSED_TEXT=processed-text
MINIO_BUCKET_SUMMARIES=summaries
MINIO_BUCKET_STRATEGY_NOTES=strategy-notes
MINIO_BUCKET_AUDIO_OUTPUTS=audio-outputs

# ChromaDB Configuration
CHROMA_HOST=localhost # For access from host, use chromadb from within docker network
CHROMA_PORT=8008
# CHROMA_PORT_HOST is the one exposed on the host (8008 in docker-compose.yml)
CHROMA_COLLECTION_MAIN=claims_main
CHROMA_COLLECTION_PRECEDENTS=claims_precedents
CHROMA_COLLECTION_NAME=claims_documents_mvp
CHROMA_PRECEDENTS_COLLECTION_NAME=claims_precedents

# LM Studio / LLM Configuration
# PHI4_API_BASE should match your LM Studio server (http://localhost:1234/v1)
# Or from within Docker: http://host.docker.internal:1234/v1
PHI4_API_BASE=http://localhost:1234 # Defaulting to docker internal access
PHI4_MODEL_NAME=phi-4-reasoning-plus # Or the specific model identifier in LM Studio

# Embedding Model Configuration
EMBEDDING_MODEL_NAME=text-embedding-3-small
# OPENAI_API_KEY=your_openai_api_key_if_using_openai_embeddings # Uncomment if using OpenAI directly

# Application Settings
LOG_LEVEL=INFO

# PostgreSQL Configuration
POSTGRES_USER=claimsadmin
POSTGRES_PASSWORD=claimssecret
POSTGRES_DB=claimsdb
POSTGRES_HOST=postgres # This is the service name in docker-compose
POSTGRES_PORT=5432
DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}


OPENAI_API_KEY=lm-studio 
EMBEDDING_MODEL_NAME=nomic-embed-text # You can update this to reflect the model.
# Often, LM Studio ignores this if only one embedding model is served.
# The key is that an embedding model IS being served.
# FastAPI/Uvicorn
# BACKEND_HOST=0.0.0.0
# BACKEND_PORT=8000

LLM_TEMPERATURE=0.7
RAG_NUM_SOURCES=3
API_V1_STR=/api/v1
PROJECT_NAME="Claims-AI MVP"

COQUI_TTS_URL=http://tts:5002