{
    "original_filename": "sample2.pdf",
    "extraction_timestamp": "2025-05-09T15:34:29.177768",
    "source_file_extension": ".pdf",
    "file_size_bytes": 1006148,
    "sha256_hash": "d1ad87d1ca7a533627815b2b05ac8468217ffd1328d45aa0aa8d5f721ec21c9b",
    "content": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future\nDirections\nYimingDu1,2∗,WenyuHuang2∗,DannaZheng2∗,ZhaoweiWang3,\nSebastienMontella4,MirellaLapata2,Kam-FaiWong1,JeffZ.Pan2,4\n1TheChineseUniversityofHongKong2TheUniversityofEdinburgh3HKUST\n4PoissonLab,CSI,HuaweiUKR&DLtd.\nydu@se.cuhk.edu.hk,{w.huang, dzheng}@ed.ac.uk,\nhttps://knowledge-representation.org/j.z.pan/\nAbstract inanattempttoorganizepreviousworkssystemat-\nically. However,thosesurveysonlyfocusnarrowly\nMemory is a fundamental component of AI\nandpartiallyonthememoryproblem,suchaslong-\nsystems,underpinninglargelanguagemodels\ncontextmodeling(Huangetal.,2023b),long-term\n(LLMs)basedagents.Whilepriorsurveyshave\nfocusedonmemoryapplicationswithLLMs, memory (He et al., 2024c; Jiang et al., 2024b),\ntheyoftenoverlooktheatomicoperationsthat personalization(Liuetal.,2025a), orknowledge\nunderliememorydynamics. Inthissurvey,we editing(Wangetal.,2024g),andgenerallylacka\nfirst categorize memory representations into\nunifiedandcomprehensiveviewofmemoryopera-\nparametric,contextualstructured,andcontex-\ntions,aswellasin-depthdiscussionsoftechnical\ntual unstructured and then introduce six fun-\npathways. Forexample,Zhangetal.(2024d)cover\ndamentalmemoryoperations: Consolidation,\nonly high-level operations such as writing, man-\nUpdating,Indexing,Forgetting,Retrieval,and\nCompression. We systematically map these agement, and reading and miss some operations\noperations to the most relevant research top- likeindexing. Inaddition,mostexistingsurveysdo\nicsacrosslong-term,long-context,parametric notclarifytheoverallscopeofmemoryresearchin\nmodification, and multi-source memory. By AIandoverlookpracticalfoundationsforfurther\nreframing memory systems through the lens\nwork—such as structured benchmark categoriza-\nofatomicoperationsandrepresentationtypes,\ntionandcoverageofrelevanttools.\nthissurveyprovidesastructuredanddynamic\nperspective on research, benchmark datasets, Toaddressthesegaps,oursurveydividesmem-\nandtoolsrelatedtomemoryinAI,clarifying ory into three types: parametric memory (Wang\nthefunctionalinterplayinLLMsbasedagents etal.,2024c),contextual-structuredmemory(Ras-\nwhileoutliningpromisingdirectionsforfuture mussenetal.,2025),andcontextual-unstructured\nresearch1.\nmemory (Zhong et al., 2024). Drawing inspi-\n1 Introduction ration from cognitive psychology, we further\nclassify these memory types by their temporal\nMemory is a critical component of LLM-based\nspan: long-termmemory(i.e.,persistentmemory\nAIsystems(Wangetal.,2024j),enablingthemto\nsuch as multi-turn dialogue history (Zhong et al.,\nsustaincoherentandlong-terminteractions(Maha-\n2024)),externalenvironmentobservations(Lietal.,\nranaetal.,2024;Lietal.,2024a). Althoughrecent\n2024a),orinternalparameters(Wangetal.,2024c),\nstudies have explored various mechanisms such\nand short-term memory corresponds to transient,\nasmemorystorage(Zhongetal.,2024),memory\nsession-levelcontexts(Packeretal.,2023).\nretrieval (Qian et al., 2024; Wang et al., 2025a),\nBasedonthememorytypes, weclassifymem-\nandmemory-groundedgeneration(Luetal.,2023;\noryoperationsintosixfundamentaltypes,spanning\nYangetal.,2024;Leeetal.,2024b)—systematic\nbothmemorymanagementandutilization. Specifi-\nperspectivesthatunifythesecomponentsintoco-\ncally,memorymanagementcomprisesfouropera-\nhesivememoryarchitecturesarestillintheirearly\ntions: consolidation(integratingnewknowledge\nstagesofdevelopment(Heetal.,2024c).\ninto persistent memories (Feng et al., 2024)), in-\nRecently, a number of surveys have proposed\ndexing(efficientlyorganizingmemoryforretrieval\noperationalviewsofmemory(Zhangetal.,2024d)\n(Wu et al., 2024a)), updating (modifying mem-\n∗∗Theseauthorscontributedequally.\nory in response to new data (Chen et al., 2024b))\n1The paper list, datasets, methods and tools\nandforgetting(strategicallyremovingoutdatedor\nare available at https://github.com/Elvin-Yiming-\nDu/Survey_Memory_in_AI. lessrelevantmemories(Tianetal.,2024)). Mem-\n5202\nyaM\n1\n]LC.sc[\n1v57600.5052:viXra\nTaxonomy Operations Contextual Memory Application Agent User\nSession Time\nMy sister share with me her\ny ro tn\ne\nConsolidation Long Term favorite photos.\nm m\ne M\nla u\ne g an\naM Indexing\n（Help me build an album）\ntx e tn o C Unstructured y ro m e Updating … Multi-Source Here is the album for you.\nM …\nParametric Memory Utilization\nStructured\nSession Time\nKV cache\ny Forgetting\nro Create table of contents for the\nm e\nM\nc ir\nte\nm\ny ro\nm\ne\nM\nn\no ita\nzilitU Retrieval\nM\nP\no\nar\nd\na\ni\nm\nfic\ne\na\nt\nt\nr\nio\nic\nn\nfollowing files.\na Here is the table.\nra Compression\nP\nLong Context\nFigure1: AunifiedframeworkofmemoryTaxonomy,Operations,andApplicationsinAIsystems.\noryutilizationinvolvestwooperations: retrieval lizationeffectiveness(e.g.,long-contextcom-\n(accessingrelevantmemorycontentwhenneeded pression (Cheng et al., 2024; Jiang et al.,\n(Gutiérrezetal.,2024))andcompression(reduc- 2024a))inhandlingextendedsequences.\ningmemorysizewhilepreservingessentialinfor-\n• ParametricMemoryModification,focused\nmation for efficient storage and reasoning (Chen\non model editing (Fang et al., 2024; Meng\netal.,2024b)).\net al., 2022b; Wang et al., 2024c), unlearn-\nTobettergroundthistaxonomyandmapthere-\ning(Mainietal.,2024),andcontinuallearn-\nsearchlandscape,weconductacomprehensivelit-\ning(Wangetal.,2024j)foradaptinginternal\neratureanalysis. Priorsurveys(Zhangetal.,2024d;\nknowledgerepresentations.\nHeetal.,2024c;Jiangetal.,2024b)lackaclearly\ndefined scope of the collected literature, making\n• Multi-SourceMemory,emphasizingintegra-\nitdifficulttoassessimpactfulmemory-relatedre-\ntionacrossheterogeneoustextualsources(Li\nsearch or to chart future directions for memory-\net al., 2023; Hu et al., 2023) but also multi-\ncentric LLM-based agents. In response, we col-\nmodalinputs(Wangetal.,2025a)tofurther\nlectandreviewmorethan30Ktop-tierconference\nsupport robust and scene-awareness reason-\npapers2 published between 2022-2025. To iden-\ning.\ntifyinfluentialmemory-focusedstudies,weapply\ntheRelativeCitationIndex(RCI),whichrankspa-\nTheremainderofthepaperisorganizedasfol-\npersbynormalizedcitationimpactovertime(see\nlows. Section 2 introduces the memory taxon-\nAppendix A). This analysis reveals four primary\nomy,includingatemporalperspectivedistinguish-\nresearchtopicscriticaltomemoryinAIsystems:\ning short-term and long-term memory. Section 3\npresents key memory operations, clarifying how\n• Long-Term Memory, focusing on mem-\nmemory is managed and utilized across different\norymanagement,inference,andpersonaliza-\nresearch contexts. Section 4 maps representative\ntion in multi-session dialogue systems (Xu\nresearch topics to memory taxonomies and oper-\netal.,2021;Maharanaetal.,2024),retrieval-\nations,highlightsemergingtrendsbasedonhigh-\naugmented generation (RAG), personalized\nRCIpapers(RCI>1),andsummarizeskeymeth-\nagents (Li et al., 2024a), and question an-\nodsanddatasets(seeAppendix6). Wealsoinclude\nsweringtasks(Wuetal.,2024a;Zhongetal.,\nsome topic recommendations at the end of each\n2024).\nsub-section. In Section 5, we introduce a suite\n• Long-Context Memory, addressing both of practical tools, including components, frame-\nparametric efficiency (e.g. \"KV cache drop- works,applicationlayers,andreal-worldproducts\nping\"(Zhangetal.,2023d))andcontextuti- supportingmemoryintegrationintomodernAIap-\nplications. InSection6,weconcludebyoutlining\n2Across NeurIPS, ICLR, ICML, ACL, EMNLP, and\nNAACLconferences. long-termvisionsforthefutureofmemoryinAI.\n2 MemoryTaxonomy during interaction with the external environment.\nThese operations can be grouped into two func-\nFromtheperspectiveofmemoryrepresentation,we\ntionalcategories: MemoryManagementandMem-\noutlinethreerepresentativetypesofmemory: Para-\noryUtilization.\nmetricMemory,ContextualUnstructuredMemory,\nandContextualStructuredMemory.\n3.1 MemoryManagement\nParametricMemory referstotheknowledgeim-\nMemory management governs how memory is\nplicitlystoredwithinamodel’sinternalparameters\nstored, maintained, and pruned over time. It\n(Bergesetal.,2024;Wangetal.,2024c;Prashanth\nincludes four core operations: Consolidation,\netal.,2024). Acquiredduringpretrainingorpost-\nIndexing, Updating, and Forgetting. These\ntraining,thismemoryisembeddedinthemodel’s\noperations naturally incorporate the temporal\nweights and accessed through feedforward com-\nnatureofmemory,whereinformationevolvesover\nputation at inference time. It serves as a form of\ntime.\ninstant,long-term,andpersistentmemoryenabling\nConsolidation (Squire et al., 2015) refers to\nfast,context-freeretrievaloffactualandcommon-\ntransformingshort-termexperiencesintopersistent\nsenseknowledge. However,itlackstransparency\nmemory. This involves encoding interaction his-\nandisdifficulttoupdateselectivelyinresponseto\ntories(i.e. dialogs,trajectories,etc.) intodurable\nnewexperiencesortask-specificcontexts.\nforms such as model parameters (Wang et al.,\nContextualUnstructuredMemory referstoan 2024j), graphs (Zhao et al., 2025), or knowledge\nexplicit,modality-generalmemorysystemwhich bases(Luetal.,2023). Itisessentialforcontinual\nstores and retrieves information across heteroge- learning (Feng et al., 2024), personalization\nneousinputssuchastext(Zhongetal.,2024),im- (Zhang et al., 2024a), external MemoryBank\nages(Wangetal.,2025a),audio,andvideo(Wang construction(Zhongetal.,2024),andknowledge\netal.,2023b). Itenablesagentstogroundreason- graphconstruction(Xuetal.,2024c).\ninginperceptualsignalsandintegratemulti-modal Indexing (Maekawa et al., 2023) refers to the\ncontext (Li et al., 2024a). Depending on its tem- constructionofauxiliarycodes—suchasentities,\nporal scope, it is further divided into short-term attributes, or content-based representations (Wu\nand long-term. Short-term memory refers to the etal.,2024a)—thatserveasaccesspointstostored\nrecent observations like the current dialogue ses- memory. Beyond simple access, indexing also\nsion context, while long-term memory refers to enablestheencodingoftemporal(Maharanaetal.,\nthepersistentrecordsofcross-sessionconversation 2024)andrelationalstructures(Mehtaetal.,2022)\ndialoguesandpersonalpersistentknowledge. acrossmemories,allowingformoreefficientand\nsemanticallycoherentretrievalthroughtraversable\nContextualStructuredMemory denotesanex-\nindex paths. It supports scalable retrieval across\nplicit memory organized into predefined, inter-\nsymbolic,neural,andhybridmemorysystems.\npretable formats or schemata such as knowledge\nUpdating (Kiley and Parks, 2022) reactivates\ngraphs (Oguz et al., 2022), relational tables (Lu\nexistingmemoryrepresentationsandtemporarily\net al., 2023), or ontologies (Qiang et al., 2023)\nmodify them. Updating parametric memory\nwhile remaining easily queryable upon requests.\ntypically involves a locate-and-edit mechanism\nThese structures support symbolic reasoning and\n(Fang et al., 2024) that targets specific model\nprecise querying, often complementing the asso-\ncomponents. Meanwhile, contextual memory\nciativecapabilitiesofpretrainedlanguagemodels\nupdating involves summarization (Zhong et al.,\n(PLMs). Structured memory can be short-term,\n2024), pruning, or refinement (Bae et al., 2022)\nconstructed at inference for local reasoning, or\nto reorganize or replace outdated content. Those\nlong-term, storing curated knowledge across ses-\nupdatingoperationssupportcontinualadaptation\nsions.\nwhilemaintainingmemoryconsistency.\nForgetting (Davis and Zhong, 2017) is the\n3 MemoryOperations\nability to selectively suppress memory content\nToenabledynamicmemorybeyondstaticstorage, that may be outdated, irrelevant or harmful. In\nAIsystemsrequireoperationsthatgovernthelife- parametricmemory,itiscommonlyimplemented\ncycleofinformationandsupportitseffectiveuse through unlearning techniques (Jia et al., 2024a;\nLi et al., 2025) that modify model parameters to compressionfocusesonreducingmemoryatinfer-\nerase specific knowledge. In contextual memory, ence(Leeetal.,2024b).\nforgetting involves time-based deletion (Zhong\net al., 2024) or semantic filtering (Wang et al.,\n4 FromOperationstoSystem-Level\n2024e)todiscardcontentthatisnolongerrelevant.\nTopics\nTheseoperationshelpmaintainmemoryefficiency\nandreduceinterference.\nBuildingonthecoreoperationsintroducedabove,\nthissectionexamineshowreal-worldsystemscoor-\nHowever, these operations introduce inherent\ndinatetheseoperationstosupportcomplexmemory\nrisks and limitations. Attackers can exploit vul-\nusage patterns. For instance, many cross-session\nnerabilities to alter or poison memory contents.\ndialoguesystems(Maharanaetal.,2024;Wuetal.,\nOnce corrupted, memory fragments may persist\n2024a)adoptRAGframeworkswhereupdating,in-\nundetectedandlatertriggermaliciousactions. As\ndexing,retrieval,andcompressionworkinconcert\ndiscussedinSection6,suchthreatscallforrobust\nto generate robust responses. Meanwhile, many\napproachesthataddressnotonlythememoryoper-\napproaches (Packer et al., 2023) treat long-term\nationsbutalsotheentirememorylifecycle.\nmemoryaslong-contextinputs(Packeretal.,2023;\nXiaoetal.,2024),whereretrievalandcompression\n3.2 MemoryUtilization\nareessential.\nThewayinwhichstoredmemoryisretrievedand We categorize combined memory usages into\nused during inference is referred as memory uti- four key topics derived from memory types\nlization. Itincludestwooperations: retrievaland (Figure 2): long-term (all types), long-context\ncompression. (contextual-unstructured),parametricmodification\nRetrievalistheprocessofidentifyingandaccess- (parametric), and multi-source memory (contex-\ningrelevantinformationfrommemoryinresponse tual). The mapping between different memory\ntoinputs,aimingtosupportdownstreamtaskssuch operations and types under each topic is summa-\nasresponsegeneration,visualgrounding,orintent rizedinTable1. Eachsectionconcludeswithtopic-\nprediction. Inputscanrangefromasimplequery specificinsightsoncurrentlimitationsandfuture\n(Duetal.,2024)toacomplexmulti-turndialogue directions. The Appendix complements the sur-\ncontext(Wangetal.,2025a),andfrompurelytex- veywithrepresentativemethods,benchmarks,and\ntualinputstovisualcontent(Zhouetal.,2024)or curateddatasets—annotatedwithkeytechnicalde-\neven more modalities. Retrieval targets include tails,memoryoperations,evaluationprotocols,and\nmemoryfrommultiplesources(Tanetal.,2024a), usagescenarios.\nmodalities(Wangetal.,2025a),orevenparametric\nrepresentations(Luoetal.,2024)withinmodels.\n4.1 Long-termMemory\nCompressionenablesefficientcontextusageunder\nlimitedcontextwindowbyretainingsalientinfor- Long-term memory refers to the persistent stor-\nmationanddiscardingredundanciesbeforefeeding age of information acquired through interactions\nitintomodels. Itcanbebroadlydividedintopre- withtheenvironment,suchasmulti-turndialogues,\ninputcompressionandpost-retrievalcompression. browsing patterns, and agent decision paths. It\nPre-inputcompressionappliesinlong-contextmod- supportscapabilitiessuchasmemorymanagement,\nelswithoutretrieval,wherefull-contextinputsare utilization,andpersonalizationoverextendedinter-\nscored,filtered,orsummarizedtofitwithincontext actions,enablingagentstoperformcomplextasks\nconstraints (Yu et al., 2023; Chung et al., 2024). over time. We review representative datasets ad-\nPost-retrievalcompressionoperatesaftermemory dressinglong-termmemoryprocessingandperson-\naccess, reducing retrieved content either through alization (see Table 2). This section focuses on\ncontextual compression before model inference contextual long-term memory—structured or un-\n(Xuetal.,2024a)orthroughparametriccompres- structured—whichdiffersfromparametricmemory\nsionbyintegratingretrievedknowledgeintomodel storedinmodelweightsviacontinuallearningand\nparameters(SafayaandYuret,2024). Unlikemem- memoryediting. Expandedsummariesofdatasets\noryconsolidation,whichsummarizesinformation and methods are provided in Appendix Tables 2\nduringmemoryconstruction(Zhongetal.,2024), and6.\nOperations Parametric Contextual-Structured Contextual-Unstructured\nConsolidation ContinualLearning, Management, Management,\nPersonalization Personalization Personalization\nIndexing Utilization Utilization, Utilization,\nManagement, Management,\nMulti-modalCoordination Personalization\nUpdating KnowledgeEditing Cross-TextualIntegration, Cross-TextualIntegration,\nPersonalization, Personalization,\nManagement Management\nForgetting KnowledgeUnlearning, Management Management\nPersonalization\nRetrieval Utilization, Utilization,Personalization,Contextual Utilization,Personalization,Contextual\nParametricEfficiency Utilization, Utilization\nMulti-modalCoordination\nCompression ParametricEfficiency ContextualUtilization ContextualUtilization\nTable1: Alignmentofsub-topicswithmemorytypesandmemoryoperations. Sub-topicsarehighlightedwith\ncolorswithrespecttothetopics: Long-term,Long-context,Parametric,Multi-source.\n4.1.1 Management MemoryIndexing referstotheprocessofstruc-\nturingmemoryrepresentationstosupportefficient\nManagementinlong-termmemoryinvolvesoper- and accurate retrieval since standingas a founda-\nations such as consolidation, indexing, updating, tionalcomponentofmemoryusage. Recentwork\nandforgettingofacquiredexperiences. Here,mem- categorizesmemoryindexingintothreeparadigms:\noryisinstantiatedintwoforms: (1)accumulated graph-based,signal-enhanced,andtimeline-based\ndialogue histories from multi-turn conversations, approaches. HippoRAG (Gutiérrez et al., 2024)\nand(2)long-termobservationsanddecisionsmade modelsmemoryindexingafterhippocampaltheory\nby autonomous agents. These are often encoded byconstructinglightweightknowledgegraphsto\nbyLLMsandstoredinexternalmemoryreposito- explicitlyrevealtheconnectionbetweendifferent\nries for future access and reuse. The memory in knowledgefragments. LongMemEval(Wuetal.,\nthosetasksisroutinelyupdatedwithnewinforma- 2024a) enhances memory keys with timestamps,\ntion and pruned to remove outdated or irrelevant factual content, and summaries. Theanine (iunn\ncontent. Ongetal.,2025)organizesmemoriesalongevolv-\ning temporal and causal links, enabling dialogue\nagentstoretrieveinformationsegmentsbasedon\nMemoryConsolidation referstotheprocessof both relevance and timeline context, supporting\ntransforming short-term memory into long-term lifelonganddynamicpersonalization. Thesestrate-\nmemory. Thisofteninvolvessavingdialoguehis- gies highlight the need to integrate structure, re-\ntoryintopersistentmemory. Existingapproaches trievalsignals,andtemporaldynamicsforeffective\ncommonlyadoptsummarizationtechniquestogen- long-termmemorymanagement.\nerateunstructuredmemoryrepresentations,asseen\ninsystemslikeMemoryBank(Zhongetal.,2024) MemoryUpdating typicallydenotestheprocess\norChatGPT-RSum(Wangetal.,2025c). Tofacili- bywhichexternalmemoryeithercreatesnewen-\ntatetheextractionofkeytopicsandsalientmemory triesforunseeninformation(Chenetal.,2024b),\nelements,Luetal.(2023)utilizeLLMprompting orreorganizesandintegratescontentwithexisting\ntoidentifyandstructurerelevantinformation. Dif- memory representations (Bae et al., 2022). Re-\nferentfromsummarization,MyAgent(Houetal., cent research categorizes memory updating into\n2024)emphasizescontext-awarememorystrength- twooverarchingparadigms: intrinsicupdatingand\neningbymodelingtemporalrelevance. Beyonddi- extrinsic updating. Intrinsic Updating operates\nalogueagenttask-basedsystems(Parketal.,2025) through internal mechanisms without explicit ex-\nincorporateepisodicwhat-where-whenmemories ternal feedback. Techniques such as selective\ntohierarchicallyorganizelong-termknowledgefor editing (Bae et al., 2022) manage memory by se-\naction planning. Together, these works illustrate lectively deleting outdated information, while re-\nagrowingefforttointegratehuman-likememory cursivesummarization(Wangetal.,2025b)com-\nconsolidationprocessesintoLLM-basedagents. pressesdialoguehistoriesthroughiterativesumma-\nmetsySIAniyromeM\nmreTgnoL\nConsolidation MyAgent(Houetal.,2024),MemoChat(Luetal.,2023)\nIndexing HippoRAG(Gutiérrezetal.,2024),LongMemEval(Wuetal.,2024a)\nManagement\nUpdating NLI-transfer(Baeetal.,2022),RCSum(Wangetal.,2025b)\nForgetting FLOW-RAG(Wangetal.,2024e),MemoryBank(Zhongetal.,2024)\nRetrieval LoCoMo(Maharanaetal.,2024),MemoChat(Luetal.,2023)\nMoT(LiandQiu,2023),SCM(Wangetal.,2024a),\nUtilization Integration Optimus-1(Lietal.,2024f),A-MEM(Xuetal.,2025)\nMEMORAG(Qianetal.,2024),ReadAgent(Leeetal.,2024c),\nGeneration\nCOMEDY(Chenetal.,2024b)\nAdaptation MALP(Zhangetal.,2024a),Per-Pcs(Tanetal.,2024b)\nPersonalization\nAugmentation EMG(Wangetal.,2024m),LDAgent(Lietal.,2024a)\ntxetnoCgnoL\nKVcacheDropping\nH2O(Zhangetal.,2023d),StreamingLLM(Xiaoetal.,2024),\nSnapKV(Lietal.,2024e)\nParametric KVCacheStoring LESS(Dongetal.,2024),KVQuant(Hooperetal.,2024),\nEfficiency Optimization KIVI(Liuetal.,2024f)\nKVCacheSelection\nQUEST(Tangetal.,2024),RetrievalAttention(Liuetal.,2024c)\nOptimization\nContextRetrieval GraphReader(Heetal.,2025),Ziya-Reader(Heetal.,2024b),\nContextual\nUtilization Context RECOMP(Xuetal.,2024a),xRAG(Chengetal.,2024),\nCompression LongLLMLingua(Jiangetal.,2024a)\nnoitacfiitoMcirtemaraP\nLocatingthen ROME(Mengetal.,2022a),MEMIT(Mengetal.,2022b),\nEditing AlphaEdit(Fangetal.,2024)\nKE(DeCaoetal.,2021),MEND(Mitchelletal.,2022a),\nMetaLearning\nEditing DAFNET(Zhangetal.,2024c)\nPrompt IKE(Zhengetal.,2023),MeLLo(Zhongetal.,2023)\nAdditionalParameters CaliNET(Dongetal.,2022),SERAC(Mitchelletal.,2022c)\nLocatingthen DEPN(Wuetal.,2023),MemFlex(Tianetal.,2024),\nUnlearning WAGLE(Jiaetal.,2024a)\nFLAT(Wangetal.,2024h),GA+Mismatch(Yaoetal.,2024),\nTrainingObjective\nUnlearning SOUL(Jiaetal.,2024b)\nPrompt ICUL(Pawelczyketal.,2023),ECO(Liuetal.,2024b)\nAdditionalParameters ULD(Jietal.,2024),EUL(ChenandYang,2023)\nRegularization-basedLearning TaSL(Fengetal.,2024),SELF-PARAM(Wangetal.)\nLifelong\n(Continual) Replay-BasedLearning DSI++(Mehtaetal.,2022)\nLearning\nInteractiveLearning LSCS(Wangetal.,2024j)\necruos-itluM\nReasoning StructRAG(Lietal.,2024g),ChatDB(Huetal.,2023)\nCross-Textual\nIntegration\nConflict RKC-LLM(Wangetal.,2023a),BGC-KC(Tanetal.,2024a)\nFusion LifelongMemory(Wangetal.,2023b),Ma-llm(Heetal.,2024a)\nMulti-modal\nCoordination\nRetrieval VISTA(Zhouetal.,2024),IGSR(Wangetal.,2025a)\nFigure2: Operation-drivensystem-leveltopicsinAIsystems.\nrization. Memory blending and refinement (Kim ing. These approaches emphasize balancing self-\netal.,2024b)furtherevolvememorybymerging organizedmemoryupdatesanduser-drivenadapta-\npastandpresentrepresentations,andself-reflective tionsforscalablelong-termmemory.\nmemoryevolution(Sunetal.,2024)updatesmem-\nory based on evidence retrieval and verification,\nMemory Forgetting involves the removal of\nenhancingfactualconsistencyovertime. Extrinsic\npreviously consolidated long-term memory rep-\nUpdating relies on external signals, particularly\nresentations. Forgetting can occur naturally over\nuser feedback. For instance, dynamic feedback\ntime, for example, following the Ebbinghaus for-\nincorporation (Dalvi Mishra et al., 2022) stores\ngettingcurve(Zhongetal.,2024),wherememory\nuser corrections into memory, enabling contin-\ntraces decay gradually. In contrast, active forget-\nualsystemimprovementwithoutrequiringretrain-\ntingstrategies(Chenetal.,2024b;Mitchelletal.,\n2022c) have been developed to intentionally re- on retrieving and combining static memory en-\nmovespecificinformationfrommemorysystems. tries at inference time to enrich context and im-\nThisisparticularlyimportantwhenlong-termmem- provereasoningconsistency. Incontrast,dynamic\norystoressensitiveorpotentiallyharmfulcontent. memoryevolutionapproaches,exemplifiedbyA-\nTherefore, enabling systems to intentionally re- MEM (Hou et al., 2024), Synapse (Zheng et al.,\nmovespecificcontentforreasonssuchasprivacy, 2024),R2I(Samsamietal.,2024)andSCM(Wang\nsafety, or compliance has become a major focus etal.,2024a),emphasizeenablingmemorytogrow,\n(Liuetal.,2024e;EldanandRussinovich,2024;Ji adapt, and restructure over the course of interac-\netal.,2024;Lietal.,2025;Liuetal.,2025b). tions,eitherthroughdynamiclinkingorcontrolled\nmemoryupdates. Whilestaticintegrationenhances\n4.1.2 Utilization\nimmediate contextual grounding, dynamic evolu-\nUtilization refers to the process of generating re-\ntioniscrucialforbuildingmoreadaptive,lifelong\nsponsesconditionedoncurrentinputsandrelevant\nlearningagents.\nmemorycontent,typicallyinvolvingmemoryrout-\ning,integration,andreading. MemoryGroundedGeneration referstoutiliz-\ning retrieved memory content that has been inte-\nMemory Retrieval focuses on the selection of\ngratedtoguidethegenerationofresponses. Exist-\nthemostrelevantmemoryentriesbasedonagiven\ningmethodscanbebroadlycategorizedintothree\nquery. To systematize recent progress, retrieval\ntypes based on how memory influences genera-\nmethods can be broadly categorized into three\ntion. First, Self-Reflective Reasoning methods,\nparadigms: (1) query-centered retrieval, which\nsuch as MoT (Li and Qiu, 2023) and StructRAG\nfocusesonimprovingqueryformulationandadap-\n(Lietal.,2024g),retrieveself-generatedorstruc-\ntation,suchasforward-lookingqueryrewritingin\nturedmemorytracestoguideintermediatereason-\nFLARE (Jiang et al., 2023b) and iterative refine-\ning steps, enhancing multi-hop inference during\nmentinIterCQR(Jangetal.,2024);(2)memory-\ndecoding. Second,Feedback-GuidedCorrection\ncenteredretrieval,whichenhancestheorganiza-\napproaches, includingthemethod ofMemoRAG\ntion and ranking of memory candidates, includ-\n(Qianetal.,2024)andRepair(Tandonetal.,2021),\ning better indexing strategies (Wu et al., 2024a)\nleveragefeedbackmemoriesormemory-informed\nand reranking methods (Du et al., 2024); and (3)\ncluestoconstraingeneration,preventingrepeated\nevent-centeredretrieval,whichretrievesmemo-\nerrors and improving output robustness. Third,\nries based on temporal and causal structures, as\nContextually-Aligned Long-Term Generation\nexploredinLoCoMo(Maharanaetal.,2024),CC\ntechniques,exemplifiedbyCOMEDY(Chenetal.,\n(Jang et al., 2023) and MSC (Xu et al., 2021).\n2024b),MemoChat(Luetal.,2023),andReadA-\nOthertechniques,suchasmulti-hopgraphtraversal\ngent (Lee et al., 2024c), integrate compressed or\n(Gutiérrezetal.,2024)andmemorygraphevolu-\nextractedmemorysummariesintothegeneration\ntion(Qianetal.,2024),furtherenrichtheretrieval\nprocesstomaintaincoherenceoverlongdialogues\nprocess. These approaches highlight the impor-\nor extended documents. These methods collec-\ntanceofadaptiveretrievalforeffectivelong-term\ntivelyenhancegenerationquality,consistency,and\nmemoryaccess,althoughreasoningoverevolving\nreasoning depth, though challenges like noise in\nmemorysequencesremainsanopenchallenge.\nmemory and reliability of retrieved memories re-\nmaintobeaddressed.\nMemoryIntegration referstotheprocessofse-\nlectively combining retrieved memory with the\n4.1.3 Personalization\nmodel context to enable coherent reasoning or\nPersonalizationisakeybutchallengingaspectof\ndecision-makingduringinference. Integrationmay\nlong-term memory, constrained by data sparsity,\nspanmultiplememorysources(e.g.,long-termdi-\nprivacy, and changing user preferences. Current\nalogue histories, external knowledge bases) and\nmethodscanbebroadlycategorizedintotwolines:\nmodalities(e.g.,text,images,orvideos),enabling\nmodel-leveladaptationandmemory-levelaugmen-\nricherandcontextuallygroundedgeneration. Re-\ntation.\ncenteffortsonmemoryintegrationcanbebroadly\ncategorizedintotwostrategies. Staticcontextual Model-Level Adaptation encodes user prefer-\nintegrationapproaches,suchasEWE(Chenetal., ences into model parameters via fine-tuning or\n2024a) and Optimus-1 (Li et al., 2024f), focus lightweight updates. Some methods embed user\ntraitsinlatentspace—e.g.,CLV(Tangetal.,2023a) 4.2 Long-context\nuses contrastive learning to cluster persona de-\nManagingvastquantitiesofmulti-sourcedexternal\nscriptions for guiding generation. Others adopt\nmemory in conversational search presents signif-\nparameter-efficientstrategies: RECAP(Liuetal.,\nicant challenges in long-context language under-\n2023b)injectsretrieveduserhistoriesviaaprefix\nstanding. While advancements in model design\nencoder,whilePer-Pes(Tanetal.,2024b)assem-\nand long-context training have enabled LLMs to\nblesmodularadapterstoreflectuserbehaviors. In\nprocessmillionsofinputtokens,effectivelymanag-\nspecializeddomains,MaLP(Zhangetal.,2023c)\ningmemorywithinsuchextensivecontextsremains\nintroduces a dual-process memory for modeling\nacomplexissue. Thesechallengescanbebroadly\nshort- and long-term personalization in medical\ncategorized into two main aspects: 1) Paramet-\ndialogues. Thesemethodsshowhowlightweight\nric Efficiency, which focuses on optimizing the\nadaptationcanpersonalizemodelswithoutcompro-\nKVcache(parametricmemory)toenableefficient\nmisingefficiencyorgeneralizability.\nlongcontextdecodingandContextualUtilization\noptimizestheutilizationofLLMstomanagevari-\nousexternalmemory(contextualmemory). Inthis\nMemory-Level Augmentation personalizes\nsection,wesystematicallyrevieweffortsmadein\nLLMs by retrieving user-specific information\nhandlingthesechallenges. Adetailedoverviewof\nfrom external memory at inference time. Based\nrelevant datasets are discussed in Table 3, while\non the memory format, existing methods can\nanexpandsummaryofhighlightedworksaredis-\nbe categorized into structured, unstructured,\ncussedinTable8andTable9.\nand hybrid approaches. Structured memories,\nsuch as user profiles or knowledge graphs, are 4.2.1 ParametricEfficiency\nused in LaMP (Salemi et al., 2023) to construct\nTomanageextensiveamountsofmulti-sourcedex-\npersonalizedpromptsandinPerKGQA(Duttetal.,\nternal memory, Large Language Models (LLMs)\n2022)forquestionansweringoverindividualized\nmust be optimized to efficiently process lengthy\nsubgraphs. Unstructured memories, including\ncontexts. In this section, we discuss approaches\ndialogue histories and narrative personas, are\nforefficientlyprocessinglong-contextfrommem-\nretrieved in LAPDOG (Huang et al., 2024) to\noryperspective,whichfocusesonKey-Value(KV)\nenrich sparse profiles and aligned with input\ncacheoptimization. KVcacheaimstominimizeun-\ncontexts via dual learning in Fu et al. (2022).\nnecessarykey-valuecomputationsbystoringpast\nHybrid methods like SiliconFriend (Zhong et al.,\nkey-value pairs as external parametric memory.\n2024) and LD-Agent (Li et al., 2024a) maintain\nHowever, as context length increases, the mem-\npersistent memory across sessions. While these\nory requirement for storing these memory grows\napproaches demonstrate scalability, they often\nquadratically,makingitinfeasibleforhandlingex-\ntreat long-term memory as a passive buffer,\ntremelylongcontexts.\nleaving its potential for proactive planning and\ndecision-makingunderexplored. KVCacheDropping aimstoreducecachesize\nbyeliminatingunnecessaryKVcache. Staticdrop-\nping approaches select unnecessary cache with\nLong-term assistants mainly extract or fixedpattern. Forinstance,StreamingLLM(Xiao\nsummarize memory, but rarely model the etal.,2024)andLM-Infinite(Hanetal.,2024)use\nevolution of personalized memory with anΛ-shapedsparsepattern. Incontrast,dynamic\nchanginguserinterestsandexternalknowl- droppingapproachesaremoreflexible,whichde-\nedge. cidetheKVcachetobeeliminatedwithrespectto\nthe query (e.g., H O (Zhang et al., 2023d), Fast-\nTime-indexedmemoryimprovesretrieval, 2\nGen (Ge et al., 2024), Keyformer (Adnan et al.,\nbutitsuseforlong-termreasoningremains\n2024),Radar(Haoetal.,2025),NACL(Chenetal.,\nlimited.\n2024c)) or the model behavior (attention weight)\nWhile personalization leverages long-\nduringinference(e.g.,SnapKV(Lietal.,2024e),\ntermmemory,itsreuseandintegrationinto\nHeadKV(Fuetal.,2025),Scissorhands(Liuetal.,\nmemory-guided planning remain underex-\n2023d)). KVcachedroppingmethodspreventthe\nplored.\nquadratic grow of memory cost for storing KV\ncache,butalsointroducetheriskofpotentialinfor- tions,encompassingcontextretrievalandcontext\nmationlosswhendiscardingKVcache. compressionacrossmemoryoperations.\nKVCacheStoringOptimization considersthe\nContextRetrieval aimstoenhanceLLM’sabil-\npotentialinformationlosswhenremovinglessim-\nityinidentifyingandlocatingkeyinformationfrom\nportantelements,andfocusonhowtopreservethe\nthecontextualmemory. Graph-basedapproaches\nentireKVcacheatasmallerfootprint. Forinstance,\nlikeCGSN(Nieetal.,2022)andGraphReader(Li\nLESS(Dongetal.,2024)compresslessimportant\net al., 2024c)) decompose documents into graph\ncacheentriesintolow-rankrepresentations,while\nstructure for effective context selection. Token-\nFlexGen (Sheng et al., 2023), Atom (Zhao et al.,\nlevelcontextselectionapproaches(e.g.,TRAMS\n2024c),KVQuant(Hooperetal.,2024)andKIVI\n(Yuetal.,2023),Selection-p(Chungetal.,2024))\n(Liuetal.,2024f)dynamicallyquantizeKVcache\nassign scores to individual tokens, pruning and\nto reduce memory allocation. These approaches\nselecting those deemed most important. In con-\nprovidelessperformancedropcomparedwithKV\ntrast, methods such as NBCE (Su et al., 2024),\ncachedroppingmethodsbutremainlimiteddueto\nFragRel(Yueetal.,2024),andSparseRAG(Zhu\nthe quadratic nature of the growing memory. Fu-\netal.,2025)performcontextselectionatthefrag-\ntureworksshouldcontinuefocusingonthetrade-\nment level, choosing the relevant context frag-\noffbetweenlessmemorycostandlessperformance\nmentsbasedontheirimportancetothespecifictask.\ndrop.\nFurthermore,training-basedapproacheslikeZiya-\nKVCacheSelection referstoselectivelyloading Reader(Heetal.,2024b)trainLLMswithspecial-\nrequiredKVcachetospeeduptheinference,which ized data to help improve their context selection\nfocusonmemoryretrievaluponKVcache. QUEST ability. Other methods like Neurocache (Safaya\n(Tang et al., 2024) and TokenSelect (Wu et al., andYuret,2024)andAWESOME(CaoandWang,\n2025) adapt query-aware KV cache selection to 2024)preserveanexternalvectormemorycacheto\nretrievecriticalKVcacheforaccelerateinference. effectivelystoreandretrievefirstencodeexternal\nSimilarly, RetrievalAttention (Liu et al., 2024c) memory into vector space, and this external vec-\nadoptsApproximateNearestNeighbor(ANN)to tormemorycanbeeffectivelyupdatedorretrieved\nsearch critical KV cache. These methods offer toenablelong-termmemoryutilization. Together\ngreater flexibility as they avoid evicting the KV with these methods, LLMs are allowed to better\ncacheandhavethepotentialtointegratewithstor- identifykeyinformationinthecontextviamemory\nageoptimizationtechniques(e.g.,Tangetal.(2024) retrieval.\nshows QUEST is compatible with Atom (Zhao\nContextCompression utilizesmemorycompres-\netal.,2024c)).\nsion operation to optimize contextual memory\n4.2.2 ContextualUtilization utilization, which generally involves two major\nApartfromoptimizinglanguagemodelstoobtain approaches: soft prompt compression and hard\nlong-contextabilities,optimizingmemoryutiliza- promptcompression(Lietal.,2024h). Softprompt\ntion raises another important challenge. Despite compressionfocusesoncompressingchunksofin-\nclaims that context length can extend to millions puttokensintothecontinuousvectorstoreducethe\noftokens,long-contextLLMshavebeenfoundto inputsequencelength. Forinstance,xRAG(Cheng\nmisscrucialinformationinthemiddleofthecon- et al., 2024), which use a off-the-shelf sentence\ntext during tasks such as question answering and encoderplusatrainedprojectortoconvertcontext\nkey-valueretrieval(Liuetal.,2024d;Ravautetal., documentintoadocumentembedding. Whilehard\n2024). This“lostinthemiddle”issueisespecially promptcompressiondirectlycompresslonginput\ncritical when managing vast amounts of external chunksintoshorternaturallanguagechunks. Lee\nmemory,asessentialinformationmaybelocated etal.(2024c)utilizeLLMtoshortencontextpage\natvariouspositionswithinthelongcontext. Inad- intonaturallanguagememorygist. Similarly,Xu\ndition, though higher recall can be obtained with etal.(2024a)introduceRECOMPcomposedwith\nlargerretrievalset,thehard-negativeinformation twocompressortosummarizeretrieveddocuments\nwillmisleadLLMsandharmthegenerationquality andreducecontextlength. Meanwhile,Jiangetal.\n(Jin et al., 2025). Effective contextual utilization (2023a) and Jiang et al. (2024a) introduce itera-\nbecomeakeychallengeinaddressingtheselimita- tivetoken-levelpromptcompressiontocompress\nprompttosupportlongmemoryutilization. With Mitchelletal.,2022c;Wangetal.,2024i;Dasetal.,\nboth soft prompt and hard prompt, LLMs are al- 2024)addexternalparametricmemorymodulesto\nlowed to more effectively utilize the context via adjust behavior without touching model weights.\nmemorycompression. Theseapproachesvaryinefficiencyandscalability,\nthoughmostfocusonentity-leveledits.\nBalancingthetrade-offbetweenreduced\n4.3.2 Unlearning\nmemoryusageandminimizedperformance\nParametric memory unlearning enables selective\ndegradationinKVcacheoptimizationrepre-\nforgettingbyremovingspecificmemorywhilere-\nsentsanexcitingareaforfutureresearch.\ntainingunrelatedmemory. Recentworkexplores\nContextual utilization with complex en-\nseveralstrategies. Additional-parametermethods\nvironment (e.g., multi-source memory) is a\nadd components such as logit difference mod-\npivotalresearchdirectionforadvancingthe\nules (Ji et al., 2024) or unlearning layers (Chen\ndevelopmentofintelligentagents.\nand Yang, 2023) to adjust memory without re-\ntraining the whole model. Prompt-based meth-\n4.3 ParametricMemoryModification ods manipulate inputs (Liu et al., 2024b) or use\nModifying parametric memory, which refers to ICL(Pawelczyketal.,2024)toexternallytrigger\nknowledge encoded within the parameters of forgetting. Locating-then-unlearningmethods(Jia\nLLMs,iscrucialfordynamicallyadaptingstored et al., 2024a; Tian et al., 2024; Wu et al., 2023)\nmemory. Methodsforparametricmemorymodifi- firstidentifyresponsibleparametricmemory,then\ncationcanbebroadlycategorizedintothreetypes: applytargetedupdatesordeactivations. Training\n(1) Editing refers to the localized modification objective-basedmethods(Wangetal.,2025d;Jia\nofmodelparameterswithoutrequiringfullmodel etal.,2024b;Yaoetal.,2024)modifythetraining\nretraining; (2) Unlearning, which selectively re- lossfunctionsoroptimizationstrategiesexplicitly\nmovesunwantedorsensitiveinformation;and(3) toencouragememoryforgetting. Theseapproaches\nContinualLearning,whichincrementallyincorpo- aimtoerasememorywhengivenexplicitforgetting\nratesnewknowledgewhilemitigatingcatastrophic targets,whilepreservingnon-targetedknowledge\nforgetting. Thissectionsystematicallyreviewsre- andbalancingefficiencyandprecision.\ncentresearchinthesecategories,withdetailedanal-\n4.3.3 ContinualLearning\nysesandcomparisonspresentedinsubsequentsub-\nContinual learning (Wang et al., 2024b) enables\nsections. A comprehensive overview of relevant\nlong-termmemorypersistencebymitigatingcatas-\ndatasetsispresentedinTable4,andextendedsum-\ntrophicforgettinginmodelparameters. Twomain\nmariesofkeymethodsareprovidedinTables10,\napproaches are regularization-based and replay-\nTable11andTable12.\nbasedmethods. Regularizationconstrainsupdates\n4.3.1 Editing toimportantweights,preservingvitalparametric\nParametricmemoryeditingupdatesspecificknowl- memory; methods like TaSL (Feng et al., 2024),\nedgestoredintheparametricmemorywithoutfull SELF-PARAM (Wang et al.), EWC (Kirkpatrick\nretraining. Thepredominantapproachislocating- etal.,2017), andPOCL(Wuetal.,2024b)apply\nthen-editing method (Meng et al., 2022a, 2023; such constraints to embed knowledge without re-\nMela et al., 2024; Fang et al., 2025), which uses play. Incontrast,replay-basedmethodsreinforce\nattributionortracingtofindwherefactsarestored, memorybyreintroducingpastsamples,andarepar-\nthenmodifiestheidentifiedmemorydirectly. An- ticularlysuitedtoincorporatingretrievedexternal\nothermajormethodismeta-learning(DeCaoetal., knowledgeorhistoricalexperiencesduringtraining.\n2021;Mitchelletal.,2022b;Zhangetal.,2024c), Forexample,DSI++(Mehtaetal.,2022)leverages\nwhereaneditornetworklearnstopredicttargeted generative memory to supplement learning with\nweight changes for quick and robust corrections. pseudoqueries,maintainingretrievalperformance\nOtherstrategiesavoideditingtheoriginalparam- without full retraining. Beyond these paradigms,\neters. Prompt-basedmethods(Zhengetal.,2023; agent-basedworksuchasLifeSpanCognitiveSys-\nZhong et al., 2023) use crafted prompts like ICL tem(LSCS)(Wangetal.,2024j)extendscontinual\nto steer outputs indirectly. Additional-parameter learningintoaninteractivesetting,enablingagents\nmethods (Wang et al., 2024c; Dong et al., 2022; toincrementallyacquireandconsolidatememory\nthroughreal-timeexperience. LSCSprovidesvalu- ument sources has also been studied, as seen in\nable insights into how external memory can be DelTA(Wangetal.,2025e)anddynamic-MT(Du\nencodedintomodelparametersinacontinualman- et al., 2022). Additionally, several studies (Li\nner. etal.,2024g;Leeetal.,2024a;Zhaoetal.,2024b;\nXuetal.,2024c)haveinvestigatedheterogeneous\nknowledge integration by retrieving information\nCurrenteditingmethodsmainlyfocuson\nfrombothstructuredandunstructuredsources. De-\nentityreplacementbutlackmechanismsto\nspiteprogressincombiningparameterizedandex-\nmodelbroadermemoryevolutionfollowing\nternalmemories,unifiedreasoningoverheteroge-\ndiverseknowledgeupdates.\nneous, multi-source knowledge remains a major\nCurrent unlearning methods typically\nchallenge, particularly in integrating parameter-\ntarget specific sequences, but future chal-\nizedmemorywithbothstructuredandunstructured\nlengesinvolveerasingallparametricmemo-\nsources.\nrieslinkedtogivenkeywordswithoutneed-\ningexplicitcontentspecification. Conflict inmulti-sourcememoryreferstofactual\nor semantic inconsistencies that arise during the\nCurrent agents accumulate memory\nretrievalandreasoningoverheterogeneousmem-\nthrough interaction, but future continual\nory representations by AI systems. In memory-\nlearningshouldavoidoverwritingpersistent\naugmentedarchitectures,conflictstypicallyemerge\nmemoryinmodelparameters.\nduringtheintegrationofmemoryrepresentations\noriginating from heterogeneous sources, includ-\n4.4 Multi-sourceMemory\ning parametric and contextual memories as well\nMulti-source memory is essential for real-world asstructuredandunstructuredknowledgesuchas\nAI deployment, where systems must reason over triples,tables,andfreetext(Xuetal.,2024b). Ex-\ninternalparametersandexternalknowledgebases, istingwork(Wangetal.,2023a;Tanetal.,2024a)\nspanningstructureddata(e.g.,knowledgegraphs, hasprimarilyfocusedontheidentificationandlo-\ntables)andunstructuredmulti-modalcontent(e.g., calization of conflicts. For example, RKC-LLM\ntext, audio, images, videos). This section exam- (Wangetal.,2023a)proposesanevaluationframe-\nineskeychallengesacrosstwodimensions: cross- work for assessing models’ ability to detect and\ntextualintegrationandmulti-modalcoordination. localizecontextualcontradictions,whileBGC-KC\nA detailed overview of the datasets and an ex- (Tanetal.,2024a)revealsasystematicbiastoward\npandedsummaryofthemethodsdiscussedarepro- internal knowledge over external sources, under-\nvidedinAppendixTable5,Table13andTable14, scoring the need for trust calibration and source\nrespectively. attribution. However,resolvingmemoryconflicts\nremainsanopenchallenge,requiringnotonlyfac-\n4.4.1 Cross-textualIntegration\ntualverificationbutalsothesemanticalignmentof\nCross-textualintegrationenablesAIsystemstoper-\nmemories across structurally and temporally het-\nformdeeperreasoningandresolveconflictsfrom\nerogeneoussources.\nmultipletextualsourcestosupportmorecontextu-\nallygroundedresponses. 4.4.2 Multi-ModalCoordination.\nAs memory-augmented systems evolve toward\nReasoning focusesonintegratingmulti-format\nmulti-modal settings, a key challenge lies in fu-\nmemorytogeneratefactuallyandsemanticallycon-\nsion and retrieval over heterogeneous modalities\nsistentresponses. Onelineofresearchinvestigates\nsuchastext,image,audioandvideo.\nreasoningovermemoriesfromdifferentdomains,\nparticularly through the precise manipulation of Fusion referstoaligningtheretrievedinforma-\nstructured symbolic memories, as demonstrated tion across diverse modalities. From a memory\nby ChatDB (Hu et al., 2023) and Neurosymbolic perspective,fusionservesasakeymechanismfor\n(Wangetal.,2024f). Otherworks(Nogueirados integratingcross-modalinformationovertime. Ex-\nSantos et al., 2024; Wu et al., 2022) explore the isting approaches can be broadly into two lines.\ndynamic integration of domain-specific parame- Thefirstfocusesonunifiedsemanticprojection,\nterized memories to enable more flexible reason- where models such as UniTransSeR (Ma et al.,\ning. Multi-source reasoning across diverse doc- 2022), MultiInstruct (Xu et al., 2023), PaLM-E\n(Driessetal.,2023),andNExT-Chat(Zhangetal., 5 Tools\n2023a)embedheterogeneousinputsintoashared\nA layered ecosystem of memory-centric AI sys-\nrepresentationspaceforreuseandquery. Thesec-\ntems has emerged to support long-term context\nondlineemphasizeslong-termcross-modalmem-\nmanagement,usermodeling,knowledgeretention,\nory integration. For example, LifelongMemory\nandadaptivebehavior. Thisecosystemspansfour\n(Wangetal.,2023b)introducesatransformerwith\ntiers: foundationalcomponents(e.g.,vectorstores,\npersistent memory to accumulate visual-textual\nLLMs,retrievers),modularframeworksformem-\nknowledgeacrosspatientrecords. Similarly,MA-\noryoperations,memorylayersystemsfororches-\nLMM (He et al., 2024a) maintains a multimodal\ntrationandpersistence,andend-user-facingprod-\nmemorybanktoextendtemporalunderstandingin\nucts.\nlongvideos. Whileeffectiveataligningmodalities,\nComponents. Foundational components pro-\ncurrentfusionmethodsoftenfallshortinlong-term\nvidetheinfrastructureuponwhichmemory-centric\nmulti-modalmemorymanagement. Keychallenges\nsystemsarebuilt. Theseincludevectordatabases\nincludedynamicupdatesandmaintainingconsis-\nsuch as FAISS (Douze et al., 2024), graph\ntencyacrossheterogeneoussources.\ndatabaseslikeNeo4j(Neo4j,2012),andlargelan-\nguage models (LLMs) such as Llama (Touvron\nRetrieval in multi-modal systems enables ac-\net al., 2023), GPT-4 (Achiam et al., 2023), and\ncess to stored knowledge across modalities such\nDeepSeek (Liu et al., 2024a). Retrieval mecha-\nastext,image,andvideo. Mostexistingmethods\nnisms—includingBM25(Robertsonetal.,1995),\nrelyonembedding-basedsimilaritycomputation,\nContriever(Izacardetal.,2021),andOpenAIem-\ngroundedinvision-languagemodelslikeQwenVL\nbeddings(OpenAI,2025)—enablesemanticaccess\n(Bai et al., 2023), CLIP (Radford et al., 2021)\nto external memory. These components serve as\nor other multi-modal models (Li et al., 2024d).\nthecomputationalsubstrateforbuildingmemory\nThese models project heterogeneous inputs into\ncapabilities such as grounding, similarity search,\nasharedsemanticspace,allowingforcross-modal\nandlong-contextunderstanding.\nretrieval. Forinstance,VISTA(Zhouetal.,2024)\nenhancesretrievalviavisualtokenrepresentations, Frameworks. On top of core infrastructure,\nwhile UniVL-DR (Liu et al., 2023c) integrates frameworks offer modular interface for memory-\nvideoandlanguagethroughaunifieddualencoder. relatedoperations. ExamplesincludeGraphiti(He\nMorerecently,IGSR(Wangetal.,2025a)extends etal.,2025),LlamaIndex(Liu,2022),LangChain\nretrieval to multi-session conversations by intro- (Chase,2022),LangGraph(Inc.,2025),EasyEdit\nducingintent-awarestickerretrieval,thoughitre- (Wang et al., 2024d), CrewAI (Duan and Wang,\nmainsanchoredinsimilarity-basedretrieval. How- 2024), and Letta (Packer et al., 2023). These\never,thesemethodsarelimitedtoshallowembed- frameworks abstract complex memory processes\ndingsimilarityandlackmemory-based,reasoning- intoconfigurablepipelines,enablingdevelopersto\nawareretrieval. Modalitieslikeaudioandsensori- construct multi-modal, persistent, and updatable\nmotorsignalsremainunderexplored,despitetheir memorymodulesthatinteractwithLLMagents.\nroleingroundingandlong-terminteractioninem- MemoryLayerSystems. Thesesystemsopera-\nbodied,multi-turnsettings. tionalizememoryasaservicelayer,providingor-\nchestration,persistence,andlifecyclemanagement.\nTools like Mem0 (Taranjeet Singh, 2024), Zep\nJoint reasoning over short-term, struc- (Rasmussenetal.,2025),Memary(kingjulio8238,\ntured, and unstructured long-term mem- 2025), and Memobase (kingjulio8238, 2025) fo-\nory remains an open and relatively under- cusonmaintainingtemporalconsistency,indexing\nexploredchallenge. memorybysessionortopic,andensuringefficient\nTemporalconsistencyconflictsinmemory recall. These platforms often combine symbolic\nremain understudied, despite the intrinsic andsub-symbolicmemoryrepresentationsandpro-\nconnectionbetweenmemoryandtime. videinternalAPIsformemoryaccessandmanipu-\nlationovertime.\nMultimodal memory remains relatively\nProducts. At the application layer, memory-\nunderexplored, particularly in retrieval\nenabled AI is being deployed in user-facing sys-\nalignmentandcross-modalrepresentation.\ntems that emphasize personalization, user state\nretention, and lifelong learning. Examples in- histories, offers flexible retrieval but requires dy-\ncludeMe.bot3,Tencentima.copilot4,Coze(Coze, namic compression and relevance filtering (Bae\n2024),Grok(xAI,2023),andChatGPT(OpenAI, etal.,2022). Integratingthesememorytypesunder\n2022). Theseproductsdemonstratehowstructured acontinuallearningframework—withmechanisms\nmemory pipelines can enhance user engagement, like consolidation, selective forgetting, and inter-\nenable long-term dialog continuity, and support leavedtraining—isessentialforbuildingadaptive,\nuser-specificreasoningacrossinteractions. personalizedlifelongagentscapableoflong-term\nThemoredetailsareshownintables: Table15 memorymanagement.\n(Components), Table 16 (Frameworks), Table 17 Brain-InspiredMemoryModels. Memoryin\n(Memory Layer Systems), and Table 18 (Prod- biologicalsystemsofferskeyinsightsforbuilding\nucts). Each table describes the tool’s applicable more resilient and adaptive AI memory architec-\nmemorytype,supportedoperations,input/output tures. The brain manages the stability–plasticity\nformats, core functionality, usage scenarios, and dilemmathroughcomplementarylearningsystems:\nsourcetype. thehippocampusencodesfast-changingepisodic\nexperiences,whilethecortexslowlyintegratessta-\n6 OpenChallengesandFutureDirections\nble long-term memory (McClelland et al., 1995;\nKumaran et al., 2016). Inspired by this, AI mod-\nSpatio-temporal Memory captures not only the\nelsincreasinglyadoptdual-memoryarchitectures,\nstructuralrelationshipsamonginformationbutalso\nsynaptic consolidation, and experience replay to\ntheirtemporalevolution,enablingagents(Leietal.,\nmitigateforgetting(Ritteretal.,2018;Wangetal.,\n2025) to adaptively update knowledge while pre-\n2021). Cognitive concepts like memory recon-\nservinghistoricalcontext(Zhaoetal.,2025). For\nsolidation (Dudaiet al.,2015), bounded memory\nexample,anAIsystemmayrecordthatauseronce\ncapacity (Cowan, 2001), and compartmentalized\ndislikedbroccolibutlateradjustitsmemorybased\nknowledge (Franklin et al., 2020) further inform\non recent purchase patterns. By maintaining ac-\nstrategiesforupdate-awarerecall,efficientstorage,\ncess to both historical and current states, spatio-\nandcontext-sensitivegeneralization.\ntemporal memory supports temporally informed\nreasoningandnuancedpersonalization. However, Meanwhile,theK-LineTheory(Minsky,1980)\nefficientlymanagingandreasoningoverlong-term pointsoutthathierarchicalmemorystructuresare\nspatio-temporalmemoryremainsakeychallenge. fundamentaltobiologicalcognition. Thesestruc-\nParametricMemoryRetrieval. Whilerecent turesenablehumanstoefficientlyorganizemem-\nknowledge editing methods (Fang et al., 2024; oryacrossdifferentlevelsofabstraction.—asseen\nWang et al., 2024c) claim they can localize and inhowinfantsgroupspecificobjectslike\"apple\"\nmodifyspecificrepresentations,enablingmodels and \"banana\" into broader categories like \"fruit\"\nto selectively retrieve knowledge from their own and\"food.\"OrganizingthememoryofAIsystems\nparameters remains an open challenge. Efficient with hierarchy structures for scalability and effi-\nretrievalandintegrationoflatentknowledgecould ciencyraisesnewchallenges(Wangetal.,2024l;\nsignificantly enhance memory utilization and re- Hanetal.,2025)andfuturedirections(Wangetal.,\nducedependenceonexternalindexingandmemory 2024k;Hongetal.,2024)formemoryresearch.\nmanagement. UnifiedMemoryRepresentation. Whilepara-\nLifelong Learning requires AI agents to con- metricmemory(Yangetal.,2024)providescom-\ntinuallyintegratenewinformationwhileretaining pact and implicit knowledge storage, and exter-\npriorknowledge(Fengetal.,2024),necessitating nal memory (Zhong et al., 2024) offers explicit\nrobust memory systems to balance stability and and interpretable information, unifying their rep-\nplasticity. Parametric memory (Tian et al., 2024) resentational spaces and establishing joint index-\nenablesin-weightknowledgeadaptationbutisvul- ingmechanismsisessentialforeffectivememory\nnerabletoforgetting,whilestructuralmemory(e.g., consolidationandretrieval. Futureworkshouldfo-\nknowledge graph, tables) supports modular, tar- cusondevelopingunifiedmemoryrepresentation\ngetedupdates(Rasmussenetal.,2025). Unstruc- frameworks that support shared indexing, hybrid\nturedmemory,suchasvectorstoresorrawdialogue storage,andmemoryoperationsacrossmodalities\nandknowledgeforms.\n3https://www.me.bot\n4https://ima.qq.com Multi-agent Memory. In multi-agent sys-\ntems, memory is not only individual but also dis- Sung. 2022. Keep me updated! memory manage-\ntributed—agentsmustmanagetheirowninternal mentinlong-termconversations. InFindingsofthe\nAssociationforComputationalLinguistics: EMNLP\nmemorieswhileinteractingwithandlearningfrom\n2022, pages 3769–3787, Abu Dhabi, United Arab\nothers. Thisraisesuniquechallengessuchasmem-\nEmirates.AssociationforComputationalLinguistics.\norysharing,alignment,conflictresolution,andcon-\nsistencyacrossagents. Effectivemulti-agentmem- Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinanTan, PengWang, JunyangLin, ChangZhou,\norysystemsshouldsupportbothlocalretentionof\nand Jingren Zhou. 2023. Qwen-vl: A versatile\npersonalizedexperiencesandglobalcoordination\nvision-languagemodelforunderstanding, localiza-\nthroughsharedmemoryspacesorcommunication tion, text reading, and beyond. arXiv preprint\nprotocols. Futureworkmayexploredecentralized arXiv:2308.12966.\nmemory architectures, cross-agent memory syn-\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nchronization,andcollectivememoryconsolidation JiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao\nto enable collaborative planning, reasoning, and Liu,AohanZeng,LeiHou,YuxiaoDong,JieTang,\nlong-termcoordination. andJuanziLi.2024. LongBench: Abilingual,multi-\ntaskbenchmarkforlongcontextunderstanding. In\nMemoryThreats&Safety. Whilememorysig-\nProceedingsofthe62ndAnnualMeetingoftheAs-\nnificantlyenhancestheutilityofLLMsbyenabling sociationforComputationalLinguistics(Volume1:\nup-to-dateandpersonalizedresponses,itsmanage- LongPapers),pages3119–3137,Bangkok,Thailand.\nment remains a critical safety concern. Memory AssociationforComputationalLinguistics.\noftenstoressensitiveandconfidentialdata,making\nYushiBai,ShangqingTu,JiajieZhang,HaoPeng,Xi-\noperationslikeaddingorremovinginformationfar aozhiWang,XinLv,ShulinCao,JiazhengXu,Lei\nfromtrivial. Recentresearchhasexposedserious Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2025.\nLongbenchv2: Towardsdeeperunderstandingand\nvulnerabilities in memory handling, particularly\nreasoningonrealisticlong-contextmultitasks.\ninmachineunlearningtechniquesdesignedtose-\nlectively erase data. Multiple studies (Liu et al., Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen\n2025b;Barezetal.,2025)havedemonstratedthat Casper,AmartyaSanyal,AdelBibi,AidanO’Gara,\nRobert Kirk, Ben Bucknall, Tim Fist, Luke Ong,\nthesemethodsarepronetomaliciousattackswhich\nPhilipTorr,Kwok-YanLam,RobertTrager,David\nstrengthenstheneedformoresecureandreliable\nKrueger,SörenMindermann,JoséHernandez-Orallo,\nmemoryoperations. MorGeva,andYarinGal.2025. Openproblemsin\nmachineunlearningforaisafety.\nVincent-Pierre Berges, Barlas Og˘uz, Daniel Haziza,\nReferences\nWen tau Yih, Luke Zettlemoyer, and Gargi Ghosh.\n2024. Memorylayersatscale.\nJoshAchiam,StevenAdler,SandhiniAgarwal,Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nShuyangCaoandLuWang.2024. AWESOME:GPU\nDiogoAlmeida,JankoAltenschmidt,SamAltman,\nmemory-constrainedlongdocumentsummarization\nShyamalAnadkat,etal.2023. Gpt-4technicalreport.\nusingmemorymechanismandglobalsalientcontent.\narXivpreprintarXiv:2303.08774.\nInProceedingsofthe2024ConferenceoftheNorth\nAmericanChapteroftheAssociationforComputa-\nMuhammad Adnan, Akhil Arunkumar, Gaurav Jain,\ntionalLinguistics: HumanLanguageTechnologies\nPrashantJ.Nair,IlyaSoloveychik,andPurushotham\n(Volume1: LongPapers),pages5925–5941,Mexico\nKamath. 2024. Keyformer: Kv cache reduction\nCity, Mexico. Association for Computational Lin-\nthroughkeytokensselectionforefficientgenerative\nguistics.\ninference. InProceedingsofMachineLearningand\nSystems,volume6,pages114–127.\nZhiweiCao,QianCao,YuLu,NingxinPeng,Luyang\nHuang,ShanboCheng,andJinsongSu.2024. Retain-\nChenxin An, Shansan Gong, Ming Zhong, Xingjian\ningkeyinformationunderhighcompressionratios:\nZhao, Mukai Li, Jun Zhang, Lingpeng Kong, and\nQuery-guidedcompressorforLLMs. InProceedings\nXipengQiu.2024. L-eval: Institutingstandardized\nof the 62nd Annual Meeting of the Association for\nevaluationforlongcontextlanguagemodels. InPro-\nComputationalLinguistics(Volume1: LongPapers),\nceedingsofthe62ndAnnualMeetingoftheAssocia-\npages12685–12695,Bangkok,Thailand.Association\ntionforComputationalLinguistics(Volume1: Long\nforComputationalLinguistics.\nPapers), pages 14388–14411, Bangkok, Thailand.\nAssociationforComputationalLinguistics. Harrison Chase. 2022. Langchain. https://www.\nlangchain.com. Accessed: 2025-04-17.\nSanghwan Bae, Donghyun Kwak, Soyoung Kang,\nMinYoungLee,SungdongKim,YuinJeong,Hyeri Jiaao Chen and Diyi Yang. 2023. Unlearn what you\nKim,Sang-WooLee,WoomyoungPark,andNako wanttoforget: Efficientunlearningforllms. InPro-\nceedingsofthe2023ConferenceonEmpiricalMeth- 2024. Larimar:Largelanguagemodelswithepisodic\nodsinNaturalLanguageProcessing,pages12041– memorycontrol. InICML.\n12052.\nRonaldLDavisandYiZhong.2017. Thebiologyof\nMingdaChen,YangLi,KarthikPadthe,RulinShao,Ali-\nforgetting—aperspective. Neuron,95(3):490–503.\nciaSun,LukeZettlemoyer,GargiGhosh,andWen-\ntau Yih. 2024a. Improving factuality with explicit\nN De Cao, W Aziz, and I Titov. 2021. Editing fac-\nworkingmemory. arXivpreprintarXiv:2412.18069. tual knowledge in language models. In EMNLP\n2021-2021ConferenceonEmpiricalMethodsinNat-\nNuo Chen, Hongguang Li, Juhua Huang, Baoyuan\nuralLanguageProcessing,Proceedings,pages6491–\nWang, and Jia Li. 2024b. Compress to impress:\n6506.\nUnleashingthepotentialofcompressivememoryin\nreal-worldlong-termconversations. arXivpreprint\nXuanwenDing,JieZhou,LiangDou,QinChen,Yuan-\narXiv:2402.11975.\nbinWu,ArleneChen,andLiangHe.2024. Boosting\nWenhuChen,ZhihaoHe,YuSu,YunyaoYu,William large language models with continual learning for\nWang,andXifengYan.2021. Hybridqa: Adataset aspect-basedsentimentanalysis. InFindingsofthe\nof multi-hop question answering over tabular and AssociationforComputationalLinguistics: EMNLP\ntextual data. In Proceedings of the International 2024,pages4367–4377,Miami,Florida,USA.Asso-\nConferenceonLearningRepresentations(ICLR). ciationforComputationalLinguistics.\nYilong Chen, Guoxia Wang, Junyuan Shang, Shiyao HarryDong,XinyuYang,ZhenyuZhang,Zhangyang\nCui,ZhenyuZhang,TingwenLiu,ShuohuanWang, Wang,YuejieChi,andBeidiChen.2024. Getmore\nYu Sun, Dianhai Yu, and Hua Wu. 2024c. NACL: withLESS:SynthesizingrecurrencewithKVcache\nA general and effective KV cache eviction frame- compressionforefficientLLMinference. InProceed-\nwork for LLM at inference time. In Proceedings ingsofthe41stInternationalConferenceonMachine\nof the 62nd Annual Meeting of the Association for Learning, volume 235 of Proceedings of Machine\nComputationalLinguistics(Volume1: LongPapers), LearningResearch,pages11437–11452.PMLR.\npages7913–7926,Bangkok,Thailand.Association\nforComputationalLinguistics. Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,\nZhifangSui, andLeiLi.2022. Calibratingfactual\nXinCheng, XunWang, XingxingZhang, TaoGe, Si-\nknowledgeinpretrainedlanguagemodels. InFind-\nQingChen,FuruWei,HuishuaiZhang,andDongyan\ningsoftheAssociationforComputationalLinguistics:\nZhao. 2024. xrag: Extreme context compression\nEMNLP2022,pages5937–5947.\nfor retrieval-augmented generation with one token.\narXivpreprintarXiv:2405.13792.\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng,\nTsz Ting Chung, Leyang Cui, Lemao Liu, Xinting Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel\nHuang, Shuming Shi, and Dit-Yan Yeung. 2024. Mazaré,MariaLomeli,LucasHosseini,andHervé\nSelection-p: Self-supervised task-agnostic prompt Jégou. 2024. The faiss library. arXiv preprint\ncompressionforfaithfulnessandtransferability. In arXiv:2401.08281.\nFindingsoftheAssociationforComputationalLin-\nguistics: EMNLP2024,pages11057–11070,Miami, DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,\nFlorida, USA. Association for Computational Lin- AakankshaChowdhery,BrianIchter,AyzaanWahid,\nguistics. Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: Anembodiedmultimodallanguage\nNelsonCowan.2001. Themagicalnumber4inshort- model. arXivpreprintarXiv:2303.03378.\ntermmemory: Areconsiderationofmentalstorage\ncapacity. BehavioralandBrainSciences,24(1):87– XinyaDu,ShaLi,andHengJi.2022. Dynamicglobal\n114. memoryfordocument-levelargumentextraction. In\nProceedings of the60th Annual Meeting of the As-\nCoze.2024. Coze: Buildyourownaiagent. https:\nsociationforComputationalLinguistics(Volume1:\n//www.coze.cn/. Accessed: April19,2025.\nLongPapers),pages5264–5275,Dublin,Ireland.As-\nsociationforComputationalLinguistics.\nBhavanaDalviMishra,OyvindTafjord,andPeterClark.\n2022. Towardsteachablereasoningsystems: Usinga\nYimingDu,HongruWang,ZhengyiZhao,BinLiang,\ndynamicmemoryofuserfeedbackforcontinualsys-\nBaojunWang,WanjunZhong,ZezhongWang,and\ntemimprovement. InProceedingsofthe2022Con-\nKam-FaiWong.2024. Perltqa: Apersonallong-term\nferenceonEmpiricalMethodsinNaturalLanguage\nmemorydatasetformemoryclassification,retrieval,\nProcessing, pages 9465–9480, Abu Dhabi, United\nandsynthesisinquestionanswering. arXivpreprint\nArabEmirates.AssociationforComputationalLin-\narXiv:2402.16288.\nguistics.\nPayel Das, Subhajit Chaudhury, Elliot Nelson, Igor Zhihua Duan and Jialin Wang. 2024. Explo-\nMelnyk,SarathkrishnaSwaminathan,SihuiDai,Au- ration of llm multi-agent application implementa-\nrélieC.Lozano,GeorgiosKollias,VijilChenthama- tion based on langgraph+ crewai. arXiv preprint\nrakshan,JiríNavrátil,SohamDan,andPin-YuChen. arXiv:2411.18241.\nYadinDudai,AviKarni,andJanBorn.2015. Thecon- Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong,\nsolidationandtransformationofmemory. Neuron, Yu Chen, Heng Ji, and Sinong Wang. 2024. LM-\n88(1):20–32. infinite: Zero-shotextremelengthgeneralizationfor\nlargelanguagemodels. InProceedingsofthe2024\nRitamDutt,KasturiBhattacharjee,RashmiGangadhara- Conference of the North American Chapter of the\niah, Dan Roth, and Carolyn Rose. 2022. Perkgqa: AssociationforComputationalLinguistics: Human\nQuestion answering over personalized knowledge Language Technologies (Volume 1: Long Papers),\ngraphs. InFindingsoftheAssociationforComputa- pages3991–4008,MexicoCity,Mexico.Association\ntionalLinguistics: NAACL2022,pages253–268. forComputationalLinguistics.\nRonenEldanandMarkRussinovich.2024. Who’sharry KaiqiaoHan,TianqingFang,ZhaoweiWang,Yangqiu\npotter? approximateunlearningforLLMs. Song,andMarkSteedman.2025. Concept-reversed\nWinogradschemachallenge: Evaluatingandimprov-\nJunfeng Fang, Houcheng Jiang, Kun Wang, Yunshan ing robust reasoning in large language models via\nMa, Jie Shi, Xiang Wang, Xiangnan He, and Tat- abstraction. InProceedingsofthe2025Conference\nSengChua.2025. Alphaedit: Null-spaceconstrained oftheNationsoftheAmericasChapteroftheAsso-\nmodeleditingforlanguagemodels. InTheThirteenth ciationforComputationalLinguistics: HumanLan-\nInternational Conference on Learning Representa- guageTechnologies(Volume2: ShortPapers),pages\ntions. 229–243, Albuquerque, New Mexico. Association\nforComputationalLinguistics.\nJunfeng Fang, Houcheng Jiang, Kun Wang, Yunshan\nMa,XiangWang,XiangnanHe,andTat-sengChua. Yongchang Hao, Mengyao Zhai, Hossein Hajimir-\n2024. Alphaedit: Null-space constrained knowl- sadeghi,SepidehsadatHosseini,andFrederickTung.\nedge editing for language models. arXiv preprint 2025. Radar: Fast long-context decoding for any\narXiv:2410.02355. transformer. InTheThirteenthInternationalConfer-\nenceonLearningRepresentations.\nYujie Feng, Xu Chu, Yongxin Xu, Guangyuan Shi,\nBoLiu,andXiao-MingWu.2024. TaSL:Continual Shirley Anugrah Hayati, Dongyeop Kang, Qingxi-\ndialog state tracking via task skill localization and aoyang Zhu, Weiyan Shi, and Zhou Yu. 2020. IN-\nconsolidation. In Proceedings of the 62nd Annual SPIRED: Toward sociable recommendation dialog\nMeeting of the Association for Computational Lin- systems. InProceedingsofthe2020Conferenceon\nguistics(Volume1: LongPapers),pages1266–1279, EmpiricalMethodsinNaturalLanguageProcessing\nBangkok,Thailand.AssociationforComputational (EMNLP),pages8142–8152,Online.Associationfor\nLinguistics. ComputationalLinguistics.\nBo He, Hengduo Li, Young Kyun Jang, Menglin Jia,\nNicholasTFranklin,KennethANorman,CharanRan-\nXuefeiCao,AshishShah,AbhinavShrivastava,and\nganath, Jeffrey M Zacks, and Samuel J Gershman.\nSer-NamLim.2024a. Ma-lmm:Memory-augmented\n2020. Structuredeventmemory: Aneuro-symbolic\nlargemultimodalmodelforlong-termvideounder-\nmodel of event cognition. Psychological Review,\nstanding. InProceedingsoftheIEEE/CVFConfer-\n127(3):327–361.\nenceonComputerVisionandPatternRecognition,\npages13504–13514.\nTingchenFu,XueliangZhao,ChongyangTao,Ji-Rong\nWen, and Rui Yan. 2022. There are a thousand\nJunqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang\nhamlets in a thousand people’s eyes: Enhancing\nSong,LiuYiBoLiuYiBo,QianguosunQianguosun,\nknowledge-groundeddialoguewithpersonalmemory.\nYuxinLiang,HaoWang,EnmingZhang,andJiaxing\narXivpreprintarXiv:2204.02624.\nZhang. 2024b. Never lost in the middle: Master-\ninglong-contextquestionansweringwithposition-\nYuFu,ZefanCai,AbedelkadirAsi,WayneXiong,Yue\nagnosticdecompositionaltraining. InProceedings\nDong, and Wen Xiao. 2025. Not all heads matter:\nof the 62nd Annual Meeting of the Association for\nA head-level KV cache compression method with\nComputationalLinguistics(Volume1: LongPapers),\nintegratedretrievalandreasoning. InTheThirteenth\npages13628–13642,Bangkok,Thailand.Association\nInternational Conference on Learning Representa-\nforComputationalLinguistics.\ntions.\nYangHe,RuijieFang,IsilDillig,andYuepengWang.\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang,\n2025. Graphiti: Bridging graph and relational\nJiaweiHan,andJianfengGao.2024. Modeltellsyou\ndatabasequeries. arXivpreprintarXiv:2504.03182.\nwhattodiscard: AdaptiveKVcachecompressionfor\nLLMs. InTheTwelfthInternationalConferenceon Zihong He, Weizhe Lin, Hao Zheng, Fan Zhang,\nLearningRepresentations. MattW.Jones,LaurenceAitchison,XuhaiXu,Miao\nLiu,PerOlaKristensson,andJunxiaoShen.2024c.\nBernalJiménezGutiérrez,YihengShu,YuGu,Michi- Human-inspiredperspectives: Asurveyonailong-\nhiro Yasunaga, and Yu Su. 2024. Hipporag: Neu- termmemory. arXivpreprintarXiv:2411.00489.\nrobiologicallyinspiredlong-termmemoryforlarge\nlanguagemodels. InTheThirty-eighthAnnualCon- XanhHo,Anh-KhoaDuongNguyen,SakuSugawara,\nferenceonNeuralInformationProcessingSystems. andAkikoAizawa.2020. Constructingamulti-hop\nqadatasetforcomprehensiveevaluationofreasoning LangChainInc.2025. Langgraph: Buildresilientlan-\nsteps. arXivpreprintarXiv:2011.01060. guageagentsasgraphs. https://github.com/\nlangchain-ai/langgraph. Accessed: 2025-\nRuixinHong,HongmingZhang,XiaomanPan,Dong 04-17.\nYu, and Changshui Zhang. 2024. Abstraction-of-\nthoughtmakeslanguagemodelsbetterreasoners. In Kai Tzu iunn Ong, Namyoung Kim, Minju Gwak,\nFindingsoftheAssociationforComputationalLin- Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung\nguistics: EMNLP 2024, pages 1993–2027, Miami, wonHwang,DonghaLee,andJinyoungYeo.2025.\nFlorida, USA. Association for Computational Lin- Towardslifelongdialogueagentsviatimeline-based\nguistics. memorymanagement. InProceedingsofthe2025\nConference of the North American Chapter of the\nColemanHooper,SehoonKim,HivaMohammadzadeh, AssociationforComputationalLinguistics: Human\nMichael W. Mahoney, Yakun Sophia Shao, Kurt LanguageTechnologies,MexicoCity,Mexico.Asso-\nKeutzer, and Amir Gholami. 2024. Kvquant: To- ciationforComputationalLinguistics.\nwards10millioncontextlengthllminferencewith\nkvcachequantization. InAdvancesinNeuralInfor- GautierIzacard,MathildeCaron,LucasHosseini,Se-\nmationProcessingSystems,volume37,pages1270– bastian Riedel, Piotr Bojanowski, Armand Joulin,\n1303.CurranAssociates,Inc. andEdouardGrave.2021. Unsuperviseddensein-\nformationretrievalwithcontrastivelearning. arXiv\nYukiHou,HarukiTamoto,andHomeiMiyashita.2024. preprintarXiv:2112.09118.\n\"my agent understands me better\": Integrating dy-\nnamichuman-likememoryrecallandconsolidation JihyoungJang, MinseongBoo, andHyounghunKim.\nin llm-based agents. In Extended Abstracts of the 2023. Conversationchronicles: Towardsdiversetem-\nCHI Conference on Human Factors in Computing poralandrelationaldynamicsinmulti-sessioncon-\nSystems,pages1–7.ACM. versations. arXivpreprintarXiv:2310.13420.\nZhijian Hou, Lei Ji, Difei Gao, Wanjun Zhong, Kun YunahJang, Kang-ilLee, HyunkyungBae, Hwanhee\nYan, Chao Li, Wing-Kwong Chan, Chong-Wah Lee,andKyominJung.2024. IterCQR:Iterativecon-\nNgo, Nan Duan, and Mike Zheng Shou. 2023. versationalqueryreformulationwithretrievalguid-\nGroundnlq@ ego4d natural language queries chal- ance. InProceedingsofthe2024Conferenceofthe\nlenge2023. arXivpreprintarXiv:2306.15255. NorthAmericanChapteroftheAssociationforCom-\nputationalLinguistics: HumanLanguageTechnolo-\nChenxuHu,JieFu,ChenzhuangDu,SimianLuo,Junbo gies (Volume 1: Long Papers), pages 8121–8138,\nZhao,andHangZhao.2023. Chatdb: Augmenting MexicoCity,Mexico.AssociationforComputational\nllmswithdatabasesastheirsymbolicmemory. Linguistics.\nLuyangHuang,ShuyangCao,NikolausParulian,Heng Jiabao Ji, Yujian Liu, Yang Zhang, Gaowen Liu, Ra-\nJi,andLuWang.2021. Efficientattentionsforlong manaKompella,SijiaLiu,andShiyuChang.2024.\ndocumentsummarization. InProceedingsofthe2021 Reversingtheforget-retainobjectives: Anefficient\nConference of the North American Chapter of the llmunlearningframeworkfromlogitdifference. Ad-\nAssociationforComputationalLinguistics: Human vances in Neural Information Processing Systems,\nLanguageTechnologies,pages1419–1436,Online. 37:12581–12611.\nAssociationforComputationalLinguistics.\nJinghanJia,JianchengLiu,YihuaZhang,ParikshitRam,\nQiushiHuang,ShuaiFu,XuboLiu,WenwuWang,Tom NathalieBaracaldoAngel,andSijiaLiu.2024a. Wa-\nKo, Yu Zhang, and Lilian Tang. 2023a. Learning gle: Strategic weight attribution for effective and\nretrievalaugmentationforpersonalizeddialoguegen- modularunlearninginlargelanguagemodels. InAn-\neration. InProceedingsofthe2023Conferenceon nualConferenceonNeuralInformationProcessing\nEmpiricalMethodsinNaturalLanguageProcessing, Systems.\npages2523–2540,Singapore.AssociationforCom-\nputationalLinguistics. Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng\nLiu, Bharat Runwal, James Diffenderfer, Bhavya\nQiushiHuang,ShuaiFu,XuboLiu,WenwuWang,Tom Kailkhura, and Sijia Liu. 2024b. Soul: Unlocking\nKo, Yu Zhang, and Lilian Tang. 2024. Learning thepowerofsecond-orderoptimizationforllmun-\nretrievalaugmentationforpersonalizeddialoguegen- learning. arXivpreprintarXiv:2404.18239.\neration. arXivpreprintarXiv:2406.18847.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing\nYunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Yang,andLiliQiu.2023a. LLMLingua: Compress-\nTaolueChen,ZenanLi,YuanYao,XiaoxingMa,Li- ing prompts for accelerated inference of large lan-\njuanYang,HaoChen,etal.2023b. Advancingtrans- guagemodels. InProceedingsofthe2023Confer-\nformer architecture in long-context large language enceonEmpiricalMethodsinNaturalLanguagePro-\nmodels: A comprehensive survey. arXiv preprint cessing,pages13358–13376,Singapore.Association\narXiv:2311.12351. forComputationalLinguistics.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dong- JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,\nshengLi,Chin-YewLin,YuqingYang,andLiliQiu. JoelVeness,GuillaumeDesjardins,AndreiARusu,\n2024a. LongLLMLingua: Acceleratingandenhanc- Kieran Milan, John Quan, Tiago Ramalho, Ag-\ningLLMsinlongcontextscenariosviapromptcom- nieszka Grabska-Barwinska, et al. 2017. Over-\npression. InProceedingsofthe62ndAnnualMeeting coming catastrophic forgetting in neural networks.\noftheAssociationforComputationalLinguistics(Vol- Proceedings of the national academy of sciences,\nume1: LongPapers),pages1658–1677,Bangkok, 114(13):3521–3526.\nThailand.AssociationforComputationalLinguistics.\nTomášKocˇiský,JonathanSchwarz,PhilBlunsom,Chris\nXunJiang,FengLi,HanZhao,JiayingWang,JunShao, Dyer,KarlMoritzHermann,GáborMelis,andEd-\nShihaoXu,ShuZhang,WeilingChen,XavierTang, wardGrefenstette.2018. TheNarrativeQAreading\nYizeChen,MengyueWu,WeizhiMa,MengdiWang, comprehensionchallenge. TransactionsoftheAsso-\nand Tianqiao Chen. 2024b. Long term memory: ciationforComputationalLinguistics,6:317–328.\nThefoundationofaiself-evolution. arXivpreprint\narXiv:2410.15665. DharshanKumaran,DemisHassabis,andJamesLMc-\nClelland.2016. Whatlearningsystemsdointelligent\nZhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, agentsneed?complementarylearningsystemstheory\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie updated. TrendsinCognitiveSciences, 20(7):512–\nCallan,andGrahamNeubig.2023b. Activeretrieval 534.\naugmentedgeneration. InProceedingsofthe2023\nConference on Empirical Methods in Natural Lan- TomKwiatkowski, JennimariaPalomaki, OliviaRed-\nguageProcessing,pages7969–7992,Singapore.As- field,MichaelCollins,AnkurParikh,ChrisAlberti,\nsociationforComputationalLinguistics. DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-\ntonLee,etal.2019. Naturalquestions: abenchmark\nBowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O\nforquestionansweringresearch. Transactionsofthe\nArik.2025. Long-contextLLMsmeetRAG:Over-\nAssociation for Computational Linguistics, 7:453–\ncomingchallengesforlonginputsinRAG. InThe\n466.\nThirteenth International Conference on Learning\nRepresentations.\nDongkyuLee,ChandanaSatyaPrakash,JackFitzGer-\nald, and Jens Lehmann. 2024a. Matter: Memory-\nZhuoranJin,PengfeiCao,ChenhaoWang,ZhitaoHe,\naugmentedtransformerusingheterogeneousknowl-\nHongbangYuan,JiachunLi,YuboChen,KangLiu,\nedge sources. In Findings of the Association for\nand Jun Zhao. 2024. RWKU: Benchmarking real-\nComputationalLinguistics: ACL2024,pages16110–\nworldknowledgeunlearningforlargelanguagemod-\n16121.\nels. In The Thirty-eight Conference on Neural In-\nformationProcessingSystemsDatasetsandBench-\nKuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John\nmarksTrack.\nCanny, and Ian Fischer. 2024b. A human-inspired\nreadingagentwithgistmemoryofverylongcontexts.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nInProceedingsofthe41stInternationalConference\nZettlemoyer.2017. TriviaQA:Alargescaledistantly\nonMachineLearning, volume235ofProceedings\nsupervisedchallengedatasetforreadingcomprehen-\nofMachineLearningResearch,pages26396–26415.\nsion. InProceedingsofthe55thAnnualMeetingof\nPMLR.\ntheAssociationforComputationalLinguistics(Vol-\nume1: LongPapers),pages1601–1611,Vancouver,\nKuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John\nCanada.AssociationforComputationalLinguistics.\nCanny, and Ian Fischer. 2024c. A human-inspired\nreadingagentwithgistmemoryofverylongcontexts.\nChristopherKileyandColleenMParks.2022. Mech-\narXivpreprintarXiv:2402.09727.\nanisms of memory updating: State dependency vs.\nreconsolidation. Journalofcognition,5(1):7.\nMingcong Lei, Yiming Zhao, Ge Wang, Zhixin Mai,\nJiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Shuguang Cui, Yatong Han, and Jinke Ren. 2025.\nKyung, Hyunseung Chung, Eunbyeol Cho, Yohan Stma: A spatio-temporal memory agent for long-\nJo,andEdwardChoi.2024a. Dialsim: Areal-time horizon embodied task planning. arXiv preprint\nsimulator for evaluating long-term multi-party dia- arXiv:2502.10177.\nlogueunderstandingofconversationalagents. arXiv\npreprintarXiv:2406.13144. HaoLi,ChenghaoYang,AnZhang,YangDeng,Xiang\nWang,andTat-SengChua.2024a. Helloagain! llm-\nSeoHyunKim, KeumminKa, YohanJo, Seung-won poweredpersonalizedagentforlong-termdialogue.\nHwang, Dongha Lee, and Jinyoung Yeo. 2024b. arXivpreprintarXiv:2406.05925.\nEver-evolvingmemorybyblendingandrefiningthe\npast. arXivpreprintarXiv:2403.04787. Na Li, Chunyi Zhou, Yansong Gao, Hui Chen, Zhi\nZhang,BoyuKuang,andAnminFu.2025. Machine\nkingjulio8238. 2025. Memary. https://github. unlearning: Taxonomy, metrics, applications, chal-\ncom/kingjulio8238/Memary. Accessed: lenges,andprospects. IEEETransactionsonNeural\n2025-04-17. NetworksandLearningSystems,pages1–21.\nNathaniel Li, Alexander Pan, Anjali Gopal, Sum- Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nmer Yue, Daniel Berrios, Alice Gatti, Justin D Li, BochaoWu,ChengdaLu,ChenggangZhao,Chengqi\nAnn-KathrinDombrowski,ShashwatGoel,Gabriel Deng, Chenyu Zhang, Chong Ruan, et al. 2024a.\nMukobi,etal.2024b. Thewmdpbenchmark: mea- Deepseek-v3 technical report. arXiv preprint\nsuringandreducingmalicioususewithunlearning. arXiv:2412.19437.\nInProceedingsofthe41stInternationalConference\nonMachineLearning,pages28525–28550. Chris Liu, Yaxuan Wang, Jeffrey Flanigan, and Yang\nLiu. 2024b. Large language model unlearning via\nShilongLi,YanchengHe,HangyuGuo,XingyuanBu, embedding-corruptedprompts. AdvancesinNeural\nGe Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yang- InformationProcessingSystems,37:118198–118266.\nguangLi,WanliOuyang,WenboSu,andBoZheng.\n2024c. GraphReader: Buildinggraph-basedagentto DiLiu,MengChen,BaotongLu,HuiqiangJiang,Zhen-\nenhancelong-contextabilitiesoflargelanguagemod- hua Han, Qianxi Zhang, Qi Chen, Chengruidong\nels. InFindingsoftheAssociationforComputational Zhang,BailuDing,KaiZhang,ChenChen,FanYang,\nLinguistics: EMNLP2024,pages12758–12786,Mi- YuqingYang,andLiliQiu.2024c. Retrievalattention:\nami,Florida,USA.AssociationforComputational Accelerating long-context llm inference via vector\nLinguistics. retrieval.\nXiaonanLiandXipengQiu.2023. Mot: Memory-of-\nJerry Liu. 2022. Llamaindex. https://www.\nthoughtenableschatgpttoself-improve. InProceed-\nllamaindex.ai. Accessed: 2025-04-17.\ningsofthe2023ConferenceonEmpiricalMethods\ninNaturalLanguageProcessing,pages6354–6374,\nJiahongLiu,ZexuanQiu,ZhongyangLi,QuanyuDai,\nSingapore.AssociationforComputationalLinguis-\nJiemingZhu,MindaHu,MenglinYang,andIrwin\ntics.\nKing. 2025a. A survey of personalized large lan-\nXingxuanLi,RuochenZhao,YewKenChia,Bosheng guagemodels: Progressandfuturedirections. arXiv\nDing, Shafiq Joty, Soujanya Poria, and Lidong preprintarXiv:2502.11528.\nBing.2023. Chain-of-knowledge: Groundinglarge\nlanguage models via dynamic knowledge adapt- Minqian Liu, Shiyu Chang, and Lifu Huang. 2022a.\ning over heterogeneous sources. arXiv preprint Incremental prompting: Episodic memory prompt\narXiv:2305.13269. for lifelong event detection. arXiv preprint\narXiv:2204.07275.\nYongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie,\nWenjie Li, and Tat-Seng Chua. 2024d. Generative NelsonFLiu,KevinLin,JohnHewitt,AshwinParan-\ncross-modal retrieval: Memorizing images in mul- jape,MicheleBevilacqua,FabioPetroni,andPercy\ntimodal language models for retrieval and beyond. Liang. 2024d. Lost in the middle: How language\nIn Proceedings of the 62nd Annual Meeting of the modelsuselongcontexts. TransactionsoftheAsso-\nAssociationforComputationalLinguistics(Volume1: ciationforComputationalLinguistics,12:157–173.\nLongPapers),pages11851–11861,Bangkok,Thai-\nland.AssociationforComputationalLinguistics. ShuaiLiu,HyundongCho,MarjorieFreedman,Xuezhe\nMa,andJonathanMay.2023a. RECAP:Retrieval-\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat enhancedcontext-awareprefixencoderforpersonal-\nVenkitesh,AcyrLocatelli,HanchenYe,TianleCai, izeddialogueresponsegeneration. InProceedings\nPatrick Lewis, and Deming Chen. 2024e. Snapkv: of the 61st Annual Meeting of the Association for\nLlmknowswhatyouarelookingforbeforegenera- ComputationalLinguistics(Volume1: LongPapers),\ntion. InAdvancesinNeuralInformationProcessing pages8404–8419,Toronto,Canada.Associationfor\nSystems,volume37,pages22947–22970.CurranAs- ComputationalLinguistics.\nsociates,Inc.\nShuai Liu, Hyundong J Cho, Marjorie Freedman,\nZaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen,\nXuezhe Ma, and Jonathan May. 2023b. Recap:\nDongmeiJiang,andLiqiangNie.2024f. Optimus-\nretrieval-enhancedcontext-awareprefixencoderfor\n1: Hybrid multimodal memory empowered agents\npersonalized dialogue response generation. arXiv\nexcel in long-horizon tasks. arXiv preprint\npreprintarXiv:2306.07206.\narXiv:2408.03615.\nZhuoqunLi,XuanangChen,HaiyangYu,HongyuLin, ZhenghaoLiu,ChenyanXiong,YuanhuiyiLv,Zhiyuan\nYaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei Han, Liu,andGeYu.2022b. Universalvision-language\nLeSun,andYongbinLi.2024g. Structrag: Boosting dense retrieval: Learning a unified representation\nknowledgeintensivereasoningofllmsviainference- space for multi-modal retrieval. arXiv preprint\ntimehybridinformationstructurization. InTheThir- arXiv:2209.00179.\nteenthInternationalConferenceonLearningRepre-\nsentations. ZhenghaoLiu,ChenyanXiong,YuanhuiyiLv,Zhiyuan\nLiu,andGeYu.2023c. Universalvision-language\nZongqianLi,YinhongLiu,YixuanSu,andNigelCol- dense retrieval: Learning a unified representation\nlier.2024h. Promptcompressionforlargelanguage spaceformulti-modalretrieval. InProceedingsof\nmodels: Asurvey. ICLR.\nZheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Pratyush Maini, Zhili Feng, Avi Schwarzschild,\nTian, andMengJiang.2024e. Towardssaferlarge Zachary Chase Lipton, and J Zico Kolter. 2024.\nlanguage models through machine unlearning. In TOFU:AtaskoffictitiousunlearningforLLMs. In\nFindingsoftheAssociationforComputationalLin- FirstConferenceonLanguageModeling.\nguisticsACL2024,pages1817–1829.\nKarttikeyaMangalam,RaiymbekAkshulakov,andJi-\nZichang Liu, Aditya Desai, Fangshuo Liao, Weitao tendraMalik.2023. Egoschema:Adiagnosticbench-\nWang,VictorXie,ZhaozhuoXu,AnastasiosKyril- markforverylong-formvideolanguageunderstand-\nlidis, and Anshumali Shrivastava. 2023d. Scis- ing. InAdvancesinNeuralInformationProcessing\nsorhands: Exploitingthepersistenceofimportance Systems(NeurIPS).\nhypothesisforLLMKVcachecompressionattest\ntime. InThirty-seventhConferenceonNeuralInfor- JamesLMcClelland,BruceLMcNaughton,andRan-\nmationProcessingSystems. dallCO’Reilly.1995. Whytherearecomplementary\nlearningsystemsinthehippocampusandneocortex:\nZirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Insightsfromthesuccessesandfailuresofconnec-\nZhaozhuoXu,VladimirBraverman,BeidiChen,and tionistmodelsoflearningandmemory. Psychologi-\nXiaHu.2024f. KIVI:Atuning-freeasymmetric2bit calReview,102(3):419–457.\nquantization for KV cache. In Proceedings of the\n41stInternationalConferenceonMachineLearning, SanketVaibhavMehta,JaiGupta,YiTay,MostafaDe-\nvolume 235 of Proceedings of Machine Learning hghani, Vinh Q Tran, Jinfeng Rao, Marc Najork,\nResearch,pages32332–32344.PMLR. EmmaStrubell,andDonaldMetzler.2022. Dsi++:\nUpdatingtransformermemorywithnewdocuments.\narXivpreprintarXiv:2212.09744.\nZiyao Liu, Huanyi Ye, Chen Chen, Yongsen Zheng,\nand Kwok-Yan Lam. 2025b. Threats, attacks, and\nDanielMela,AitorGonzález-Agirre,JavierHernando,\ndefenses in machine unlearning: A survey. IEEE\nand Marta Villegas. 2024. Mass-editing memory\nOpenJournaloftheComputerSociety,6:413–425.\nwith attention in transformers: A cross-lingual ex-\nplorationofknowledge. InFindingsoftheAssocia-\nJunruLu,SiyuAn,MingbaoLin,GabrielePergola,Yu-\ntionforComputationalLinguisticsACL2024,pages\nlanHe,DiYin,XingSun,andYunshengWu.2023.\n5831–5847.\nMemochat: Tuning llms to use memos for consis-\ntent long-range open-domain conversation. arXiv\nmemodbio.2025. Memobase: Profile-basedlong-term\npreprintarXiv:2308.08239.\nmemory for ai applications. https://github.\ncom/memodb-io/memobase. Accessed: 2025-\nKun Luo, Zheng Liu, Shitao Xiao, Tong Zhou, Yubo\n04-26.\nChen, Jun Zhao, and Kang Liu. 2024. Landmark\nembedding: Achunking-freeembeddingmethodfor\nKevinMeng,DavidBau,AlexAndonian,andYonatan\nretrievalaugmentedlong-contextlargelanguagemod-\nBelinkov. 2022a. Locating and editing factual as-\nels. InProceedingsofthe62ndAnnualMeetingof\nsociations in gpt. Advances in neural information\ntheAssociationforComputationalLinguistics(Vol-\nprocessingsystems,35:17359–17372.\nume1: LongPapers),pages3268–3281,Bangkok,\nThailand.AssociationforComputationalLinguistics.\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2022b. Mass-\nZhiyuan Ma, Jianjun Li, Guohui Li, and Yongjing editing memory in a transformer. arXiv preprint\nCheng. 2022. UniTranSeR: A unified transformer arXiv:2210.07229.\nsemanticrepresentationframeworkformultimodal\ntask-orienteddialogsystem. InProceedingsofthe Kevin Meng, Arnab Sen Sharma, Alex J Andonian,\n60thAnnualMeetingoftheAssociationforCompu- Yonatan Belinkov, and David Bau. 2023. Mass-\ntationalLinguistics(Volume1: LongPapers),pages editing memory in a transformer. In The Eleventh\n103–114,Dublin,Ireland.AssociationforComputa- International Conference on Learning Representa-\ntionalLinguistics. tions.\nAruMaekawa,HidetakaKamigaito,KotaroFunakoshi, StephenMerity,CaimingXiong,JamesBradbury,and\nandManabuOkumura.2023. Generativereplayin- RichardSocher.2017. Pointersentinelmixturemod-\nspiredbyhippocampalmemoryindexingforcontin- els. InInternationalConferenceonLearningRepre-\nual language learning. In Proceedings of the 17th sentations.\nConferenceoftheEuropeanChapteroftheAssocia-\ntionforComputationalLinguistics,pages930–942. MarvinMinsky.1980. K-lines: Atheoryofmemory.\nCognitiveScience,4(2):117–133.\nAdyasha Maharana, Dong-Ho Lee, Sergey Tulyakov,\nMohit Bansal, Francesco Barbieri, and Yuwei EricMitchell,CharlesLin,AntoineBosselut,Chelsea\nFang. 2024. Evaluating very long-term conver- Finn,andChristopherDManning.2022a. Fastmodel\nsational memory of llm agents. arXiv preprint editing at scale. In International Conference on\narXiv:2402.17753. LearningRepresentations.\nEricMitchell,CharlesLin,AntoineBosselut,Chelsea Junyeong Park, Junmo Cho, and Sungjin Ahn. 2025.\nFinn, and Christopher D Manning. 2022b. Fast Mr.steve: Instruction-followingagentsinminecraft\nmodeleditingatscale. InInternationalConference with what-where-when memory. In International\nonLearningRepresentations. ConferenceonLearningRepresentations(ICLR). Ac-\nceptedasaposteratICLR2025.\nEricMitchell,CharlesLin,AntoineBosselut,Christo-\nMartin Pawelczyk, Seth Neel, and Himabindu\npherDManning,andChelseaFinn.2022c. Memory-\nLakkaraju.2023. In-contextunlearning: Language\nbasedmodeleditingatscale. InInternationalCon-\nmodels as few shot unlearners. arXiv preprint\nferenceonMachineLearning,pages15817–15831.\narXiv:2310.07579.\nPMLR.\nMartin Pawelczyk, Seth Neel, and Himabindu\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nLakkaraju.2024. In-contextunlearning: Language\nÇag˘lar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-\nmodelsasfew-shotunlearners. InInternationalCon-\ntivetextsummarizationusingsequence-to-sequence\nferenceonMachineLearning,pages40034–40050.\nRNNs and beyond. In Proceedings of the 20th\nPMLR.\nSIGNLLConferenceonComputationalNaturalLan-\nguage Learning, pages 280–290, Berlin, Germany. USVSN Sai Prashanth, Alvin Deng, Kyle O’Brien,\nAssociationforComputationalLinguistics. JyothirSV,MohammadAflahKhan,JaydeepBorkar,\nChristopherAChoquette-Choo,JacobRayFuehne,\nNeo4j. 2012. Neo4j - the world’s leading graph Stella Biderman, Tracy Ke, et al. 2024. Re-\ndatabase. Accessed: 2025-04-25. cite, reconstruct, recollect: Memorization in lms\nas a multifaceted phenomenon. arXiv preprint\nYuxiangNie,HeyanHuang,WeiWei,andXian-Ling arXiv:2406.17746.\nMao.2022. Capturingglobalstructuralinformation\ninlongdocumentquestionansweringwithcompres- HongjinQian,PeitianZhang,ZhengLiu,KelongMao,\nsivegraphselectornetwork. InProceedingsofthe and Zhicheng Dou. 2024. Memorag: Moving to-\n2022 Conference on Empirical Methods in Natu- wardsnext-genragviamemory-inspiredknowledge\nral Language Processing, pages 5036–5047, Abu discovery. arXivpreprintarXiv:2409.05591.\nDhabi,UnitedArabEmirates.AssociationforCom-\nZhangchengQiang,WeiqingWang,andKerryTaylor.\nputationalLinguistics.\n2023. Agent-om:Leveragingllmagentsforontology\nmatching. arXivpreprintarXiv:2312.00326.\nCiceroNogueiradosSantos, JamesLee-Thorp, Isaac\nNoble,Chung-ChingChang,andDavidUthus.2024. AlecRadford,JongWookKim,ChrisHallacy,Aditya\nMemoryaugmentedlanguagemodelsthroughmix- Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-\nture of word experts. In Proceedings of the 2024 try, Amanda Askell, Pamela Mishkin, Jack Clark,\nConference of the North American Chapter of the GretchenKrueger,andIlyaSutskever.2021. Learn-\nAssociationforComputationalLinguistics: Human ingtransferablevisualmodelsfromnaturallanguage\nLanguage Technologies (Volume 1: Long Papers), supervision. InProceedingsofthe38thInternational\npages4425–4438,MexicoCity,Mexico.Association Conference on Machine Learning, volume 139 of\nforComputationalLinguistics. ProceedingsofMachineLearningResearch,pages\n8748–8763.PMLR.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan\nPeshterliev,DmytroOkhonko,MichaelSchlichtkrull, Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nSonalGupta,YasharMehdad,andScottYih.2022. mar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nUniK-QA:Unifiedrepresentationsofstructuredand Compressivetransformersforlong-rangesequence\nunstructured knowledge for open-domain question modelling. InInternationalConferenceonLearning\nanswering. InFindingsoftheAssociationforCompu- Representations.\ntationalLinguistics:NAACL2022,pages1535–1546,\nPrestonRasmussen,PavloPaliychuk,TravisBeauvais,\nSeattle,UnitedStates.AssociationforComputational\nJackRyan,andDanielChalef.2025. Zep: Atempo-\nLinguistics.\nralknowledgegrapharchitectureforagentmemory.\narXivpreprintarXiv:2501.13956.\nOpenAI.2022. Chatgpt: Optimizinglanguagemodels\nfor dialogue. https://openai.com/blog/\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nchatgpt.\nRaghavGupta,andPranavKhaitan.2020. Towards\nscalable multi-domain conversational agents: The\nOpenAI.2025. Openaiplatformdocumentation: Em-\nschema-guideddialoguedataset. InProceedingsof\nbeddingsguide. https://platform.openai.\ntheAAAIConferenceonArtificialIntelligence,vol-\ncom/docs/guides/embeddings. Accessed:\nume34,pages8689–8696.\n2025-04-17.\nMathieuRavaut, AixinSun, NancyChen, andShafiq\nCharles Packer, Vivian Fang, Shishir_G Patil, Kevin Joty. 2024. On context utilization in summariza-\nLin,SarahWooders,andJoseph_EGonzalez.2023. tion with large language models. In Proceedings\nMemgpt: Towardsllmsasoperatingsystems. of the 62nd Annual Meeting of the Association for\nComputationalLinguistics(Volume1: LongPapers), JianlinSu,MurtadhaAhmed,BoWen,LuoAo,Min-\npages2764–2781,Bangkok,Thailand.Association gren Zhu, and Yunfeng Liu. 2024. Naive Bayes-\nforComputationalLinguistics. basedcontextextensionforlargelanguagemodels.\nInProceedingsofthe2024ConferenceoftheNorth\nSteven Ritter, Jane X Wang, Zeb Kurth-Nelson, Sid- AmericanChapteroftheAssociationforComputa-\ndhantJayakumar,CharlesBlundell,andTimothyLill- tionalLinguistics: HumanLanguageTechnologies\nicrap.2018. Meta-learningthroughhebbianplasticity (Volume1: LongPapers),pages7791–7807,Mexico\ninrandomnetworks. InAdvancesinNeuralInforma- City, Mexico. Association for Computational Lin-\ntionProcessingSystems(NeurIPS),volume31. guistics.\nStephen E Robertson, Steve Walker, Susan Jones, XinSu,TiepLe,StevenBethard,andPhillipHoward.\nMichelineMHancock-Beaulieu,MikeGatford,etal. 2023. Semi-structured chain-of-thought: Inte-\n1995. Okapiattrec-3. NistSpecialPublicationSp, grating multiple sources of knowledge for im-\n109:109. proved language model reasoning. arXiv preprint\narXiv:2311.08505.\nAliSafayaandDenizYuret.2024. Neurocache: Effi-\ncientvectorretrievalforlong-rangelanguagemod- HaoSun,HengyiCai,BoWang,YingyanHou,Xiaochi\neling. In Proceedings of the 2024 Conference of Wei,ShuaiqiangWang,YanZhang,andDaweiYin.\ntheNorthAmericanChapteroftheAssociationfor 2024. Towardsverifiabletextgenerationwithevolv-\nComputationalLinguistics: HumanLanguageTech- ingmemoryandself-reflection. InProceedingsof\nnologies(Volume1: LongPapers),pages870–883, the2024ConferenceonEmpiricalMethodsinNatu-\nMexicoCity,Mexico.AssociationforComputational ralLanguageProcessing,pages8211–8227,Miami,\nLinguistics. Florida, USA. Association for Computational Lin-\nguistics.\nAlirezaSalemi,ShesheraMysore,MichaelBendersky,\nAlonTalmorandJonathanBerant.2018. Thewebas\nandHamedZamani.2023. Lamp: Whenlargelan-\naknowledge-baseforansweringcomplexquestions.\nguagemodelsmeetpersonalization. arXivpreprint\nInProceedingsofthe2018ConferenceoftheNorth\narXiv:2304.11406.\nAmericanChapteroftheAssociationforComputa-\nMohammadRezaSamsami,ArtemZholus,Janarthanan tionalLinguistics: HumanLanguageTechnologies,\nRajendran, and Sarath Chandar. 2024. Mastering pages641–651.AssociationforComputationalLin-\nmemorytaskswithworldmodels. InTheTwelfthIn- guistics.\nternationalConferenceonLearningRepresentations.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathanBerant.2019. CommonsenseQA:Aques-\nGabrielSarch,LawrenceJang,MichaelTarr,WilliamW\ntion answering challenge targeting commonsense\nCohen,KennethMarino,andKaterinaFragkiadaki.\nknowledge. InProceedingsofthe2019Conference\n2024. Vlmagentsgeneratetheirownmemories: Dis-\noftheNorthAmericanChapteroftheAssociationfor\ntillingexperienceintoembodiedprogramsofthought.\nComputationalLinguistics: HumanLanguageTech-\nAdvancesinNeuralInformationProcessingSystems,\nnologies,Volume1(LongandShortPapers),pages\n37:75942–75985.\n4149–4158,Minneapolis,Minnesota.Associationfor\nComputationalLinguistics.\nYingSheng,LianminZheng,BinhangYuan,Zhuohan\nLi,MaxRyabinin,BeidiChen,PercyLiang,Christo-\nHexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang,\npherRé,IonStoica,andCeZhang.2023. Flexgen:\nQiCao,andXueqiCheng.2024a. Blindedbygen-\nhigh-throughput generative inference of large lan-\neratedcontexts: Howlanguagemodelsmergegen-\nguagemodelswithasinglegpu. InProceedingsof\nerated and retrieved contexts for open-domain qa?\nthe40thInternationalConferenceonMachineLearn-\narXive-prints,pagesarXiv–2401.\ning,ICML’23.JMLR.org.\nZhaoxuanTan,ZheyuanLiu,andMengJiang.2024b.\nHaizhouShiandHaoWang.2023. Aunifiedapproach\nPersonalizedpieces: Efficientpersonalizedlargelan-\ntodomainincrementallearningwithmemory: The-\nguagemodelsthroughcollaborativeefforts. InPro-\noryandalgorithm. AdvancesinNeuralInformation\nceedingsofthe2024ConferenceonEmpiricalMeth-\nProcessingSystems,36:15027–15059.\nods in Natural Language Processing, pages 6459–\n6475,Miami,Florida,USA.AssociationforCompu-\nWeijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika\ntationalLinguistics.\nMalladi, Jieyu Zhao, Ari Holtzman, Daogao Liu,\nLuke Zettlemoyer, Noah A Smith, and Chiyuan NiketTandon,AmanMadaan,PeterClark,andYiming\nZhang. 2024. Muse: Machine unlearning six-way Yang.2021. Learningtorepair:Repairingmodelout-\nevaluation for language models. arXiv preprint puterrorsafterdeploymentusingadynamicmemory\narXiv:2407.06460. offeedback. arXivpreprintarXiv:2112.09737.\nLarry R Squire, Lisa Genzel, John T Wixted, and JiamingTang,YilongZhao,KanZhu,GuangxuanXiao,\nRichard G Morris. 2015. Memory consolida- BarisKasikci,andSongHan.2024. QUEST:Query-\ntion. Cold Spring Harbor perspectives in biology, aware sparsity for efficient long-context LLM in-\n7(8):a021766. ference. In Proceedings of the 41st International\nConference on Machine Learning, volume 235 of JaneXWang,ZebKurth-Nelson,DharshanKumaran,\nProceedingsofMachineLearningResearch,pages DhruvaTirumala,HubertSoyer,JoelZLeibo,Demis\n47901–47911.PMLR. Hassabis, and Matthew Botvinick. 2021. Dual-\nsystemepisodiccontrol: Integratingepisodicmem-\nYihongTang,BoWang,MiaoFang,DongmingZhao, oryandreinforcementlearning. NatureHumanBe-\nKunHuang,RuifangHe,andYuexianHou.2023a. haviour,5(3):293–307.\nEnhancing personalized dialogue generation with\ncontrastivelatentvariables: Combiningsparseand LiyuanWang,XingxingZhang,HangSu,andJunZhu.\ndensepersona. arXivpreprintarXiv:2305.11482. 2024b. Acomprehensivesurveyofcontinuallearn-\ning: Theory,methodandapplication. IEEETransac-\nYihongTang,BoWang,MiaoFang,DongmingZhao, tionsonPatternAnalysisandMachineIntelligence.\nKunHuang,RuifangHe,andYuexianHou.2023b.\nEnhancing personalized dialogue generation with Liyuan Wang, Xingxing Zhang, Kuo Yang, Longhui\ncontrastivelatentvariables: Combiningsparseand Yu, ChongxuanLi, LanqingHong, ShifengZhang,\ndense persona. In Proceedings of the 61st Annual ZhenguoLi,YiZhong,andJunZhu.2022. Memory\nMeeting of the Association for Computational Lin- replaywithdatacompressionforcontinuallearning.\nguistics(Volume1: LongPapers),pages5456–5468, arXivpreprintarXiv:2202.06592.\nToronto,Canada.AssociationforComputationalLin-\nPengWang,ZexiLi,NingyuZhang,ZiwenXu,Yunzhi\nguistics.\nYao,YongJiang,PengjunXie,FeiHuang,andHua-\njunChen.2024c. Wise: Rethinkingtheknowledge\nDeshraj Yadav Taranjeet Singh. 2024. Mem0: The\nmemory layer for your ai agents. https:// memoryforlifelongmodeleditingoflargelanguage\ngithub.com/mem0ai/mem0. models. AdvancesinNeuralInformationProcessing\nSystems,37:53764–53797.\nYiTay,MostafaDehghani,SamiraAbnar,YikangShen,\nPengWang,NingyuZhang,BozhongTian,ZekunXi,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nYunzhiYao,ZiwenXu,MengruWang,ShengyuMao,\nSebastianRuder,andDonaldMetzler.2021. Long\nXiaohanWang,SiyuanCheng,KangweiLiu,Yuan-\nrangearena: Abenchmarkforefficienttransformers.\nshengNi,GuozhouZheng,andHuajunChen.2024d.\nInInternationalConferenceonLearningRepresenta-\nEasyEdit: Aneasy-to-useknowledgeeditingframe-\ntions.\nworkforlargelanguagemodels. InProceedingsof\nthe62ndAnnualMeetingoftheAssociationforCom-\nBozhongTian,XiaozhuanLiang,SiyuanCheng,Qing-\nputationalLinguistics(Volume3: SystemDemonstra-\nbinLiu,MengruWang,DianboSui,XiChen,Hua-\ntions),pages82–93,Bangkok,Thailand.Association\njun Chen, and Ningyu Zhang. 2024. To forget or\nforComputationalLinguistics.\nnot? towards practical knowledge unlearning for\nlarge language models. In Findings of the Associ-\nQingyueWang,YananFu,YananCao,ShiWang,Zhil-\nationforComputationalLinguistics: EMNLP2024,\niangTian,andLiangDing.2025b. Recursivelysum-\npages1524–1537.\nmarizingenableslong-termdialoguememoryinlarge\nlanguagemodels. Neurocomputing,page130193.\nHugoTouvron,ThibautLavril,GautierIzacard,Xavier\nMartinet,Marie-AnneLachaux,TimothéeLacroix,\nQingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nZhiliang Tian, and Liang Ding. 2025c. Recur-\nFaisal Azhar, et al. 2023. Llama: Open and effi-\nsivelysummarizingenableslong-termdialoguemem-\ncient foundation language models. arXiv preprint\nory in large language models. Neurocomputing,\narXiv:2302.13971.\n639:130193.\nHarshTrivedi,NiranjanBalasubramanian,TusharKhot,\nShang Wang, Tianqing Zhu, Dayong Ye, and Wan-\nand Ashish Sabharwal. 2022. MuSiQue: Multi-\nlei Zhou. 2024e. When machine unlearning\nhopquestionsviasingle-hopquestioncomposition.\nmeets retrieval-augmented generation (rag): Keep\nTransactionsoftheAssociationforComputational\nsecret or forget knowledge? arXiv preprint\nLinguistics,10:539–554.\narXiv:2410.15267.\nBing Wang, Xinnian Liang, Jian Yang, Hui Huang, Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang\nShuangzhiWu, PeihaoWu, LuLu, ZejunMa, and Ren. 2024f. Symbolic working memory enhances\nZhoujunLi.2024a. Enhancinglargelanguagemodel languagemodelsforcomplexruleapplication. arXiv\nwithself-controlledmemoryframework. preprintarXiv:2408.13654.\nBingbingWang,YimingDu,BinLiang,ZhixinBai,Min SongWang,YaochenZhu,HaochenLiu,ZaiyiZheng,\nYang, Baojun Wang, Kam-Fai Wong, and Ruifeng Chen Chen, and Jundong Li. 2024g. Knowledge\nXu.2025a. Anewformulaforstickerretrieval: Re- editingforlargelanguagemodels: Asurvey. ACM\nply with stickers in multi-modal and multi-session ComputingSurveys,57(3):1–37.\nconversation. InProceedingsoftheAAAIConference\nonArtificialIntelligence,volume39,pages25327– YaxuanWang,JiahengWei,ChrisYuhaoLiu,Jinlong\n25335. Pang,QuanLiu,AnkitShah,YujiaBao,YangLiu,\nandWeiWei.2025d. LLMunlearningvialossad- ZhengWang,ZhongyangLi,ZerenJiang,DandanTu,\njustment with only forget data. In The Thirteenth andWeiShi.2024m. Craftingpersonalizedagents\nInternational Conference on Learning Representa- throughretrieval-augmentedgenerationoneditable\ntions. memory graphs. In Proceedings of the 2024 Con-\nferenceonEmpiricalMethodsinNaturalLanguage\nYaxuanWang,JiahengWei,ChrisYuhaoLiu,Jinlong Processing,pages4891–4906,Miami,Florida,USA.\nPang,QuanLiu,AnkitParagShah,YujiaBao,Yang AssociationforComputationalLinguistics.\nLiu,andWeiWei.2024h. Llmunlearningvialoss\nadjustment with only forget data. arXiv preprint DiWu,HongweiWang,WenhaoYu,YuweiZhang,Kai-\narXiv:2410.11143. Wei Chang, and Dong Yu. 2024a. Longmemeval:\nBenchmarkingchatassistantsonlong-terminterac-\nYike Wang, Shangbin Feng, Heng Wang, Weijia tivememory. arXivpreprintarXiv:2410.10813.\nShi, VidhishaBalachandran, TianxingHe, andYu-\nlia Tsvetkov. 2023a. Resolving knowledge con- WeiWu,ZhuoshiPan,ChaoWang,LiyiChen,Yunchu\nflicts in large language models. arXiv preprint Bai, Tianfu Wang, Kun Fu, Zheng Wang, and Hui\narXiv:2310.00935. Xiong.2025. Tokenselect: Efficientlong-contextin-\nferenceandlengthextrapolationforllmsviadynamic\nYingWang,YanlaiYang,andMengyeRen.2023b. Life- token-levelkvcacheselection.\nlongmemory: Leveragingllmsforansweringqueries\nin long-form egocentric videos. arXiv preprint XinweiWu,JunzhuoLi,MinghuiXu,WeilongDong,\narXiv:2312.05269. ShuangzhiWu, ChaoBian, andDeyiXiong.2023.\nDepn: Detectingandeditingprivacyneuronsinpre-\nYu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, trainedlanguagemodels. InProceedingsofthe2023\nShiyangLi,JingfengYang,QingyuYin,ZhengLi, Conference on Empirical Methods in Natural Lan-\nXianLi,BingYin,etal.2024i. Memoryllm: towards guageProcessing,pages2875–2886.\nself-updatable large language models. In Proceed-\ningsofthe41stInternationalConferenceonMachine YichenWu, HongWang, PeilinZhao, YefengZheng,\nLearning,pages50453–50466. YingWei,andLong-KaiHuang.2024b. Mitigating\ncatastrophicforgettinginonlinecontinuallearning\nYu Wang, Chi Han, Tongtong Wu, Xiaoxin He, bymodelingprevioustaskinterrelationsviapareto\nWangchunshuZhou,NafisSadeq,XiusiChen,Zexue optimization. InForty-firstInternationalConference\nHe, Wei Wang, Gholamreza Haffari, et al. 2024j. onMachineLearning.\nTowardslifespancognitivesystems. arXivpreprint\narXiv:2409.13265. Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Min-\nervini,PontusStenetorp,andSebastianRiedel.2022.\nYuWang,XinshuangLiu,XiusiChen,SeanO’Brien, An efficient memory-augmented transformer for\nJundaWu,andJulianMcAuley. Self-updatablelarge knowledge-intensiveNLPtasks. InProceedingsof\nlanguagemodelsbyintegratingcontextintomodel the2022ConferenceonEmpiricalMethodsinNat-\nparameters. InTheThirteenthInternationalConfer- uralLanguageProcessing,pages5184–5196,Abu\nenceonLearningRepresentations. Dhabi,UnitedArabEmirates.AssociationforCom-\nputationalLinguistics.\nYutongWang,JialiZeng,XueboLiu,DerekF.Wong,\nFandong Meng, Jie Zhou, and Min Zhang. 2025e. xAI.2023. Grok. https://grok.com. Accessed:\nDelta: An online document-level translation agent 2025-04-19.\nbasedonmulti-levelmemory. InInternationalCon-\nferenceonLearningRepresentations(ICLR). Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan,andMikeLewis.2024. Efficientstreaminglan-\nZhaoweiWang,WeiFan,QingZong,HongmingZhang, guagemodelswithattentionsinks. InTheTwelfth\nSehyunChoi,TianqingFang,XinLiu,YangqiuSong, International Conference on Learning Representa-\nGinnyWong, andSimonSee.2024k. AbsInstruct: tions.\nElicitingabstractionabilityfromLLMsthroughex-\nplanationtuningwithplausibilityestimation. InPro- FangyuanXu,WeijiaShi,andEunsolChoi.2024a. RE-\nceedingsofthe62ndAnnualMeetingoftheAssocia- COMP: Improving retrieval-augmented LMs with\ntionforComputationalLinguistics(Volume1: Long contextcompressionandselectiveaugmentation. In\nPapers),pages973–994,Bangkok,Thailand.Associ- The Twelfth International Conference on Learning\nationforComputationalLinguistics. Representations.\nZhaoweiWang, HaochenShi, WeiqiWang, Tianqing JingXu, ArthurSzlam, andJasonWeston.2021. Be-\nFang,HongmingZhang,SehyunChoi,XinLiu,and yondgoldfishmemory: Long-termopen-domaincon-\nYangqiuSong.2024l. AbsPyramid: Benchmarking versation. arXivpreprintarXiv:2107.07567.\ntheabstractionabilityoflanguagemodelswithauni-\nfiedentailmentgraph. InFindingsoftheAssociation RongwuXu,ZehanQi,ZhijiangGuo,CunxiangWang,\nforComputationalLinguistics: NAACL2024,pages Hongru Wang, Yue Zhang, and Wei Xu. 2024b.\n3991–4010, Mexico City, Mexico. Association for Knowledge conflicts for llms: A survey. arXiv\nComputationalLinguistics. preprintarXiv:2403.08319.\nWujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Jun- theAssociationforComputationalLinguistics: ACL\ntao Tan, and Yongfeng Zhang. 2025. A-mem: 2024,pages16348–16361,Bangkok,Thailand.As-\nAgentic memory for llm agents. arXiv preprint sociationforComputationalLinguistics.\narXiv:2502.12110.\nAoZhang,YuanYao,WeiJi,ZhiyuanLiu,andTat-Seng\nXinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Chua.2023a. Next-chat: Anlmmforchat,detection\nNiu, Hua Wu, Haifeng Wang, and Shihang Wang. andsegmentation. arXivpreprintarXiv:2311.04498.\n2022. Long time no see! open-domain conversa-\ntionwithlong-termpersonamemory. arXivpreprint AoZhang,YuanYao,WeiJi,ZhiyuanLiu,andTat-Seng\narXiv:2203.05797. Chua.2023b. Next-chat: Anlmmforchat,detection\nandsegmentation. arXivpreprintarXiv:2311.04498.\nYaoXu,ShizhuHe,JiabeiChen,ZihaoWang,Yangqiu\nSong, Hanghang Tong, Guang Liu, Kang Liu, Kai Zhang, Yangyang Kang, Fubang Zhao, and Xi-\nand Jun Zhao. 2024c. Generate-on-graph: Treat aozhong Liu. 2023c. Llm-based medical assistant\nllm as both agent and kg in incomplete knowl- personalization with short-and long-term memory\nedge graph question answering. arXiv preprint coordination. arXivpreprintarXiv:2309.11696.\narXiv:2404.14741.\nKai Zhang, Yangyang Kang, Fubang Zhao, and Xi-\nZhiyangXu,YingShen,andLifuHuang.2023. Multi- aozhongLiu.2024a. LLM-basedmedicalassistant\nInstruct: Improvingmulti-modalzero-shotlearning personalization with short- and long-term memory\nviainstructiontuning. InProceedingsofthe61stAn- coordination. InProceedingsofthe2024Conference\nnualMeetingoftheAssociationforComputational oftheNorthAmericanChapteroftheAssociationfor\nLinguistics(Volume1: LongPapers),pages11445– ComputationalLinguistics: HumanLanguageTech-\n11465,Toronto,Canada.AssociationforComputa- nologies(Volume1:LongPapers),pages2386–2398,\ntionalLinguistics. MexicoCity,Mexico.AssociationforComputational\nLinguistics.\nHongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu,\nZhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, NingyuZhang,YunzhiYao,BozhongTian,PengWang,\nZeyunTang,ShichaoSong,etal.2024. memory3: Shumin Deng, Mengru Wang, Zekun Xi, Shengyu\nLanguage modeling with explicit memory. arXiv Mao,JintianZhang,YuanshengNi,etal.2024b. A\npreprintarXiv:2407.01178. comprehensivestudyofknowledgeeditingforlarge\nlanguagemodels. arXivpreprintarXiv:2401.01286.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio,WilliamWCohen,RuslanSalakhutdinov,and Taolin Zhang, Qizhou Chen, Dongyang Li, Chengyu\nChristopherDManning.2018. Hotpotqa: Adataset Wang, Xiaofeng He, Longtao Huang, Jun Huang,\nfordiverse,explainablemulti-hopquestionanswer- etal.2024c. Dafnet: Dynamicauxiliaryfusionfor\ning. arXivpreprintarXiv:1809.09600. sequentialmodeleditinginlargelanguagemodels.\nIn Findings of the Association for Computational\nYuanshunYao,XiaojunXu,andYangLiu.2024. Large LinguisticsACL2024,pages1588–1602.\nlanguage model unlearning. Advances in Neural\nInformationProcessingSystems,37:105425–105475. ZeyuZhang,XiaoheBo,ChenMa,RuiLi,XuChen,\nQuanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-\nWen-tauYih,Ming-WeiChang,XiaodongHe,andJian- RongWen.2024d. Asurveyonthememorymech-\nfengGao.2016. Thevalueofsemanticparselabeling anismoflargelanguagemodelbasedagents. arXiv\nforknowledgebasequestionanswering. InProceed- preprintarXiv:2404.13501.\ningsofthe54thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume1: LongPapers), Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong\npages 2019–2029. Association for Computational Chen,LianminZheng,RuisiCai,ZhaoSong,Yuan-\nLinguistics. dongTian,ChristopherRe,ClarkBarrett,Zhangyang\nWang, and Beidi Chen. 2023d. H2o: Heavy-hitter\nPeterYoung,AliceLai,MicahHodosh,andJuliaHock- oracleforefficientgenerativeinferenceoflargelan-\nenmaier. 2014. From image descriptions to visual guagemodels. InThirty-seventhConferenceonNeu-\ndenotations: Newsimilaritymetricsforsemanticin- ralInformationProcessingSystems.\nferenceovereventdescriptions. Transactionsofthe\nAssociationforComputationalLinguistics,2:67–78. Wenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip\nYu, Shafiq Joty, Yingbo Zhou, and Semih Yavuz.\nHaofei Yu, Cunxiang Wang, Yue Zhang, and Wei Bi. 2024a. DIVKNOWQA: Assessing the reasoning\n2023. TRAMS:Training-freememoryselectionfor abilityofLLMsviaopen-domainquestionanswering\nlong-range language modeling. In Findings of the over knowledge base and text. In Findings of the\nAssociationforComputationalLinguistics: EMNLP AssociationforComputationalLinguistics: NAACL\n2023,pages4966–4972,Singapore.Associationfor 2024,pages51–68,MexicoCity,Mexico.Associa-\nComputationalLinguistics. tionforComputationalLinguistics.\nXihangYue,LinchaoZhu,andYiYang.2024. FragRel: WentingZhao, YeLiu, TongNiu, YaoWan, PhilipS.\nExploiting fragment-level relations in the external Yu, Shafiq Joty, Yingbo Zhou, and Semih Yavuz.\nmemory of large language models. In Findings of 2024b. DIVKNOWQA: Assessing the reasoning\nabilityofLLMsviaopen-domainquestionanswering publication age into account to prevent bias be-\nover knowledge base and text. In Findings of the tweenoriginalcitationsfromdifferentpublication\nAssociationforComputationalLinguistics: NAACL\ndates. TheageA ofapaperp iscomputedas:\n2024,pages51–68.AssociationforComputational i i\nLinguistics.\nA = T −Year +1 (1)\ni\nYilongZhao,Chien-YuLin,KanZhu,ZihaoYe,Lequn\nChen,SizeZheng,LuisCeze,ArvindKrishnamurthy, ,whereT isthedatewhenthecitationiscollected\nTianqiChen,andBarisKasikci.2024c. Atom: Low- (20th April 2025) and Year is the year where\ni\nbitquantizationforefficientandaccuratellmserving.\npaperiisfirstpublished. Thus,wecanmodelthe\nInMLSys.\nrelationbetweencitationnumberC andageA of\ni i\nZhengyiZhao, ShuboZhang, YimingDu, BinLiang, paperp as:\ni\nBaojunWang,ZhongyangLi,BinyangLi,andKam-\nFaiWong.2025. Eventweave:Adynamicframework log(C +1) = β+αlogA +ϵ (2)\ni i i\nforcapturingcoreandsupportingeventsindialogue\nsystems. arXivpreprintarXiv:2503.23078.\nWe collect papers from past 3 years (2022 to\nCeZheng,LeiLi,QingxiuDong,YuxuanFan,Zhiyong 2025) from Top NLP and ML conferences (i.e.,\nWu, Jingjing Xu, and Baobao Chang. 2023. Can ACL,NAACL,EMNLP,NeurIPS,ICML,ICLR).\nwe edit factual knowledge by in-context learning? Toreducethebiasfromdifferentresearcharea,we\nIn Proceedings of the 2023 Conference on Empiri-\nuseGPTtoscoretherelevanceofapaperwiththe\ncalMethodsinNaturalLanguageProcessing,pages\nfourchallengesdiscussedinthepaper. Wepickall\n4862–4876.\nthepaperswithscoreequalandhigherthan8and\nLongtao Zheng, Rundong Wang, Xinrun Wang, and collecttheirpublicationdateandcitationnumbers\nBo An. 2024. Synapse: Trajectory-as-exemplar\nfromSemanticScholarAPI5.Forpaperswithout\nprompting with memory for computer control. In\nProceedings of the International Conference on publicationdatefield,weusethefirstconference\nLearningRepresentations(ICLR). dayasthepublicationdate. Wegatheratotalnum-\nberof3,932validpapersaftertheprocessingand\nWanjunZhong,LianghongGuo,QiqiGao,HeYe,and compute the estimated beˆta and αˆ accordingly6.\nYanlinWang.2024. Memorybank: Enhancinglarge\nlanguage models with long-term memory. In Pro-\nAfterthat7,weareabletoobtaintheexpectedcita-\nceedingsoftheAAAIConferenceonArtificialIntelli- tionnumberCˆ ofpaperp withageA as:\ni i i\ngence,volume38,pages19724–19731.\nCˆ = exp(βˆ)+Aαˆ (3)\nZexuanZhong,ZhengxuanWu,ChristopherDManning, i i\nChristopherPotts,andDanqiChen.2023. Mquake:\nThenwecomputetherelativecitationindexRCI\nAssessingknowledgeeditinginlanguagemodelsvia i\nmulti-hopquestions. InProceedingsofthe2023Con- ofpaperp i as:\nferenceonEmpiricalMethodsinNaturalLanguage\nProcessing,pages15686–15702. C i\nRCI = (4)\ni Cˆ\ni\nJunjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and\nYongping Xiong. 2024. Vista: Visualized text em- When RCI >= 1, we consider this paper over-\ni\nbeddingforuniversalmulti-modalretrieval. InPro-\ncitedthanitsexpectations,andviceversa. Inthis\nceedingsofthe62ndAnnualMeetingoftheAssocia-\ntionforComputationalLinguistics(Volume1: Long paper,wefocusonthepaperwithRCI >= 1,for\nPapers),pages3185–3200. whichwebelievehasmoreinfluence.\nYunZhu,Jia-ChenGu,CaitlinSikora,HoKo,Yinxiao\nLiu,Chu-ChengLin,LeiShu,LiangchenLuo,Lei\nMeng,BangLiu,andJindongChen.2025. Acceler-\natinginferenceofretrieval-augmentedgenerationvia\nsparsecontextselection. InTheThirteenthInterna-\ntionalConferenceonLearningRepresentations.\nAppendix\n5https://www.semanticscholar.org/\nproduct/api\nA RelativeCitationIndex\n6Notedthatnotallpapersmentionedinthisworkarecon-\nsideredinestimatingβˆandαˆ,buttheywillbeassignedaRCI\nInthiswork,weidentifyimpactfulworksbyRel-\nscorebasedonthepublicationage.\nativeCitationIndex(RCI)metirc,whichtakethe 7Theestimationis:βˆ=1.878,αˆ=1.297\nDatasets Mo Operations DS Per TR Metrics Purpose Year Access\nType\nLongMemEval text Indexing, MS Recall@K, Benchmarkchatassistantsonlong-term 2025 [LINK]\n(Wuetal.,2024a) Retrieval, NDCG@K, memoryabilities, includingtemporal\nCompression Accuracy reasoning.\nLoCoMo text + Indexing, MS Accuracy, Evaluatelong-termmemoryinLLMs 2024 [LINK]\n(Maharanaetal.,2024) image Retrieval, ROUGE,Preci- acrossQA,eventsummarization,and\nCompression sion,Recall,F1 multimodaldialoguetasks.\nMemoryBank text Updating, MS Accuracy, Hu- EnhanceLLMswithlong-termmemory 2024 [LINK]\n(Zhongetal.,2024) Retrieval manEval capabilities,adaptingtouserpersonali-\ntiesandcontexts.\nPerLTQA text Retrieval MS MAP, Recall, Toexplorepersonallong-termmemory 2024 [LINK]\n(Duetal.,2024) Precision, questionansweringability.\nF1, Accuracy,\nGPT4score\nMALP text Retrieval, QA ROUGE,Accu- Preference-conditioneddialoguegener- 2024 [LINK]\n(Zhangetal.,2024a) Compression racy,WinRate ation. Parameter-efficientfine-tuning\n(PEFT)forcustomization.\nDialSim text Retrieval MS Accuracy Toevaluatedialoguesystemsunderreal- 2024 [LINK]\n(Kimetal.,2024a) istic,real-time,andlong-contextmulti-\npartyconversationconditions.\nCC text Retrieval MS BLEU, Forlong-termdialoguemodelingwith 2023 [LINK]\n(Jangetal.,2023) ROUGE timeandrelationshipcontext.\nLAMP text Consolidation, MS Accuracy, F1, Multipleentriesperuser.Supportsboth 2023 [LINK]\n(Salemietal.,2023) Retrieval, ROUGE user-basedsplitsandtime-basedsplits,\nCompression enablingevaluationofshort-termand\nlong-termpersonalization.\nMSC text Consolidation, MS PPL Toevaluateandimprovelong-termdia- 2022 [LINK]\n(Xuetal.,2021) Retrieval, loguemodelsviamulti-sessionhuman-\nCompression human chats with evolving shared\nknowledge.\nDuLeMon text Consolidation, MS Accuracy, F1, Fordynamicpersonatrackingandcon- 2022 [LINK]\n(Xuetal.,2022) Updating Recall, Pre- sistentlong-termhuman-botinteraction.\nRetrieval, cision, PPL,\nCompression BLEU, DIS-\nTINCT\n2WikiMultiHopQA table + Consolidation, QA EM,F1 Multi-hopQAcombiningstructuredand 2020 [LINK]\n(Hoetal.,2020) knowl- Indexing, unstructureddatawithreasoningpaths.\nedge Retrieval,\nbase + Compression\ntext\nNQ text Retrieval, QA EM,F1 Open-domainQAbasedonrealGoogle 2019 [LINK]\n(Kwiatkowski et al., Compression searchqueries.\n2019)\nHotpotQA text Retrieval, QA EM,F1 Multi-hopQAwithexplainablereason- 2018 [LINK]\n(Yangetal.,2018) Compression ingandsentence-levelsupportingfacts.\nTable2: Datasetsusedforevaluatinglong-termmemory. “Mo”denotesmodality. “Ops”denotesoperability\n(placeholder). “DSType”indicatesdatasettype(QA–questionanswering,MS–multi-sessiondialogue). “Per”\nand“TR”indicatewhetherpersonaandtemporalreasoningarepresent.\nDatasets Modality Operations Metrics Purpose Year Access\nWikiText-103 text compression PPL Corpuswith100milliontokensextracted 2016 [LINK]\n(Merityetal.,2017) fromthesetofverifiedarticlesonWikipedia\nforlongcontextlanguagemodeling.\nPG-19 text compression PPL Corpusconstructedwithbooksextracted 2019 [LINK]\n(Raeetal.,2020) fromtheProjectGutenbergbookslibrary\nforlongcontextlanguagemodeling.\nLRA text+image compression, Acc Benchmark constructed with 6 identical 2020 [LINK]\n(Tayetal.,2021) retrieval tasksforevaluatingefficientlongcontext\nlanguagemodels.\nNarrativeQA text retrieval Bleu-1,Bleu-4,Meteor, QuestionAnsweringdatasetcouldbeused 2017 [LINK]\n(Kocˇiskýetal.,2018) Rouge-L,MRR forevaluatinglongcontextQAability.\nTriviaQA text retrieval EM,F1 QuestionAnsweringdatasetcouldbeused 2017 [LINK]\n(Joshietal.,2017) forevaluatinglongcontextQAability.\nNaturalQuestions text retrieval EM,F1 QuestionAnsweringdatasetcouldbeused 2019 [LINK]\n(Kwiatkowski et al., forevaluatinglongcontextQAability.\n2019)\nMusiQue text retrieval F1 Challengingmulti-hopQuestionAnswering 2021 [LINK]\n(Trivedietal.,2022) datasetforevaluatinglongcontextreasoning\nandQAability.\nCNN/DailyMail text compression Rouge-1, Rouge-2, Over300knewsarticlesfromCNNandDai- 2016 [LINK]\n(Nallapatietal.,2016) Rouge-L lyMailforevaluatinglongdocumentsum-\nmarization\nGovReport text compression Rouge-1, Rouge-2, Reports written by government research 2021 [LINK]\n(Huangetal.,2021) Rouge-L,BertScore agenciesforevaluatinglongdocumentsum-\nmarization\nL-Eval text compression, Rouge-L,F1,GPT4 Benchmark containing 20 sub-tasks spe- 2023 [LINK]\n(Anetal.,2024) retrieval ciallydesignedforevaluatinglongcontext\nlanguagemodelsfromdifferentaspect.\nLongBench text compression, F1,Rouge-L,Accuracy, Benchmarkcontaining14Englishtasks,5 2023 [LINK]\n(Baietal.,2024) retrieval EM,EditSim Chinesetasks,and2codetasksforsystem-\naticallongcontextevaluation.\nLongBenchv2 text+table+KG compression, Acc UpdatedversionofLongBenchwhichis 2024 [LINK]\n(Baietal.,2025) retrieval muchlongerandmorechallenging,with\nconsistentmulti-choiceformatforreliable\nevaluation\nTable3: Datasetsforlong-contextmemoryevaluation.\nDataset Modality Operations Metrics Purpose Year Access\nEditSuccess, Consistsof6datasets.Providea\nKnowEdit\ntext updating Portability,Locality, comprehensiveevaluationcoveringknowledge 2024 [LINK]\n(Zhangetal.,2024b)\nandFluency insertion,modification,anderasure.\nToevaluatethepropagationofcounterfactual\nEdit-wiseSuccessRate, knowledgeeditingaffectsthroughmulti-hop\nMQUAKE-CF\ntext updating Instance-wiseAccuracy, reasoning,extendingupto4hops,wherea 2023 [LINK]\n(Zhongetal.,2023)\nMulti-hopAccuracy singlereasoningchainmaycontainmultiple\nedits.\nToevaluatethepropagationoftemporal\nEdit-wiseSuccessRate,\nMQUAKE-T knowledgeeditingaffectsthroughmulti-hop\ntext updating Instance-wiseAccuracy, 2023 [LINK]\n(Zhongetal.,2023) reasoning,extendingupto4hops,withonlyone\nMulti-hopAccuracy\neditperreasoningchain.\nEfficacyScore,Efficacy\nMagnitude,Paraphrase\nToevaluate substantialandimprobable\nScores,Paraphrase\nCounterfact factualchangesoversuperficialedits,\ntext updating Magnitude, 2022 [LINK]\n(Mengetal.,2022a) especiallythosepreviouslydeemedunlikelyby\nNeighborhoodScore,\namodel.\nNeighborhood\nMagnitude\nSuccessRate,Retain\nzsRE Accuracy,Equivalence Oneoftheearliestdatasetusedtoevaluate\ntext updating 2021 [LINK]\n(DeCaoetal.,2021) Accuracy,Performance knowledgeediting.\nDeterioration\nAcomprehensivemachineunlearning\nMUSE VerbMem,KnowMem, evaluationbenchmarkthatenumeratessix\ntext forgetting 2024 [LINK]\n(Shietal.,2024) PrivLeak diversedesirablepropertiesforunlearned\nmodels.\nAbenchmarkcontainingcopyrightedcontent\nUnlearnSuccess,\nKnowUnDo anduserprivacydomainstoevaluateifthe\ntext forgetting RetentionSuccess, 2024 [LINK]\n(Tianetal.,2024) unlearningprocessinadvertentlyerases\nPerplexity,ROUGE-L\nessentialknowledge.\nToevaluatereal-worldknowledgeunlearning\nRWKU\ntext forgetting ROUGE-L underpractical,corpus-freeconditionsusing 2024 [LINK]\n(Jinetal.,2024)\nreal-worldtargetsandadversarialassessments.\nServeasaproxymeasurementofhazardous\nWMDP\ntext forgetting QAaccuracy knowledgeinbiosecurity,cybersecurity,and 2024 [LINK]\n(Lietal.,2024b)\nchemicalsecurity.\nTOFU Probability,ROUGE, Anovelunlearningdatasetwithfactsabout200\ntext forgetting 2024 [LINK]\n(Mainietal.,2024) TruthRatio fictitiousauthors.\nABSA Adatasetforaspect-basedsentimentanalysisto\ntext Consolidation F1 2024 [LINK]\n(Dingetal.,2024) evaluateLLMsincontinuallearningsettings.\nJGA,FWT(Forward\nSGD Amulti-turntask-orienteddialoguedatasetthat\ntext Consolidation Transfer),BWT 2020 [LINK]\n(Rastogietal.,2020) supportsevolvinguserintents.\n(BackwardTransfer)\nJGA,FWT(Forward\nINSPIRED Amulti-turntask-orienteddialoguedatasetthat\ntext Consolidation Transfer),BWT 2020 [LINK]\n(Hayatietal.,2020) supportsevolvinguserintents.\n(BackwardTransfer)\nNaturalQuestion Amulti-purposedatasetthatoffersindexed\nIndexingAccuracy,\n(Kwiatkowskietal., text Consolidation documentsandsupportscontinuallearning 2019 [LINK]\nHits@1\n2019) acrossevolvingdocumentcollections.\nTable4: Datasetsforparametricmemoryevaluation.\nDatasets Mo Ops Src# Mod# Task Metrics Purpose Year Access\nMultiChat text + Retrieval 2 2 Retrieval Precision,mAP, Image-groundedstickerretrievalwith 2025 [LINK]\n(Wangetal.,2025a) image GPT-4 cross-sessionimage-textdialoguecon-\ntext.\nContext-conflicting text Compression 2 1 Conflict DiffGR, EM, Designedtoevaluateamodel’sability 2024 [LINK]\n(Tanetal.,2024a) Similarity tohandleconflictingevidenceacross\nsources.\nEgoSchema video+ Retrieval, 3 2 Fusion Accuracy Combinesepisodicvideomemory,so- 2023 [LINK]\n(Mangalametal.,2023) text Compression cialschema,andconversationforlong-\ntermmemoryQA.\nEgo4DNLQ video+ Retrieval, 2 2 Fusion Recall@K VideoQAtaskfocusingonnaturallan- 2022 [LINK]\n(Houetal.,2023) text Compression guage queries over egocentric video\nwithtemporalmemory.\n2WikiMultihopQA text Indexing, 2 1 ReasoningEM,F1 Multi-hop QA requiring reasoning 2020 [LINK]\n(Hoetal.,2020) Retrieval, across two Wikipedia passages with\nCompression sentence-levelsupportingevidence.\nHybridQA text Retrieval 2 1 ReasoningEM,F1 QA requiring reasoning across struc- 2020 [LINK]\n(Chenetal.,2021) Compression turedtablesandunstructuredtext.\nCommonsenseVQA text + Retrieval 2 2 Fusion Accuracy Commonsensequestionansweringover 2019 [LINK]\n(Talmoretal.,2019) image Compression visualscenesrequiringvisual-textualfu-\nsion.\nNaturalQuestions text Retrieval >1* 1 Conflict EM,F1 Real-worldQAoverGooglesearchsnip- 2019 [LINK]\n(Kwiatkowskietal.,2019) Compression pets;oftenusedassourceforcontradic-\ntionanalysis.\nComplexWebQuestions text Retrieval >1* 1 ReasoningEM,F1 CompositionalQArequiringmulti-step 2018 [LINK]\n(TalmorandBerant,2018) Compression reasoningacrosswebsnippets.\nHotpotQA text Retrieval 2 1 Conflict EM, F1, Sup- Multi-hop QA with paragraph-level 2018 [LINK]\n(Yangetal.,2018) Compression portingFactAc- source documents and sentence-level\ncuracy supportingfacts.\nTriviaQA text Retrieval ≥6 1 Conflict EM,F1 QA over trivia-style questions with 2017 [LINK]\n(Joshietal.,2017) Compression noisywebsources;usefulforsourcedis-\nagreementanalysis.\nWebQuestionsSP text Indexing >1* 1 ReasoningF1,Accuracy EnhancedversionofWebQuestionswith 2016 [LINK]\n(Yihetal.,2016) Retrieval structuredreasoningchains.\nCompression\nFlickr30K text + Retrieval 2 2 Retrieval Similarity Image-caption pairs widely used for 2014 [LINK]\n(Youngetal.,2014) image Compression cross-modal retrieval and alignment\ntasks.\nTable 5: Datasets used for evaluating multi-source memory. “Mo” denotes data modality. “Ops” indicates\noperations. “Src#” = number of information sources per instance; “Mod#” = number of modalities; “Task” =\nretrieval,fusion,reasoning,orconflictresolution.\nMethod Type TF RE Input Output LMs Ops Features Year Code\nPERKGQA Retrieved& long-termdialoguemodeling,\n(Duttetal., Augmentation Knowledge Response RoBERTa Retrieval event&personamemory, 2022 [LINK]\n2022) Graph+Query mudularagentarchitecture\nCLV contrastivelearning,\nPersona+\n(Tangetal., Adaption Response GPT-2 Consolidation clustereddensepersona, 2023 [LINK]\nQuery\n2023b) dialoguegeneration\nRECAP Retrieved& hierarchicaltransformer\n(Liuetal., Augmentation Context+ Response Transformers Retrieval retriever,context-awareprefix 2023 [LINK]\n2023a) Query encoder\nConsolidation,\nSiliconFriend Retrieved& ChatGLM-6B,\nUpdating, fine-tuning,\n(Zhongetal., Augmentation Context+ Response BELLE-7B, 2024 [LINK]\nForgetting, RAG,EbbinghausForgetting\n2024) Query gpt-3.5-turbo\nRetrieval\nmemorycoordination,\nMALP Retrieved& GPT3.5,\nConsolidation, computationalbionicmemory\n(Zhangetal., Adaption Context+ Response LLaMA-7B, 2024 [LINK]\nRetrieval mechanism,patientprofile,\n2024a) Query LLaMA-13B\nself-chat\nPERPCS modularPEFTsharing,\n(Tanetal., Adaption UserHistory / Llama-2-7B Consolidation collaborativepersonalization, 2024 [LINK]\n2024b) userhistoryassembly\nLAPDOG Retrieved& Consolidation,\nStory-basedpersonaretrieval,\n(Huangetal., Augmentation Context+ Response T5 Updating, 2024 [LINK]\njointretriever-generatortraining\n2023a) Query Retrieval\nLD-Agent Retrieved& ChatGLM, Consolidation, long-termdialoguemodeling,\n(Lietal., Augmentation Context+ Response BlenderBot, Updating, event&personamemory, 2025 [LINK]\n2024a) Query ChatGPT Retrieval mudularagentarchitecture\nTable6: Overviewofmethodsforlong-termmemoryinpersonalization. \"TF\"(TrainingFree)denoteswhether\nthe method operates without additional gradient-based updates. \"RE\" (Retrieval Module) denotes whether the\nmethodneedsRetrieval.\nMethod Type TF RE DS Input Output LMs Ops Features Year Code\n( M Lu em et o a C l. h , a 2 t 023) Consolidation D H Q i i u a s e t l r o o y r g y ue + Response V G 33 I P B c T u 4 , n , T a C - 5 7 h B a , tG 13 P B T , , C R o et n r s ie o v li a d l ation, S m o cy r t e i r c z u m l a e c t o t i u r o y r n e - – d d r r e m iv tr e e i m n ev o d a s i l a , – l r o e g s u p e o , n m se em- 2023 [LINK]\n( M 2 Z 0 h e 2 m o 4 n ) o g r e y t B a a l n ., k Consolidation R C Q e o u t n e r r t i e y e x v t ed + & Response C B gp h E t a L - t 3 G L .5 E L - - t M 7 u B r - b 6 , o B, C U F R o o e p r t n d g r s a i e e o t t v i l t n i i a d n g l a g , t , ion, fi R n A e G -t , u E ni b n b g i , nghausForgetting 2024 [LINK]\n( N B L a I e - e T t r a a l n ., s 2 fe 0 r 22) Updating M D H i i e a s m t l o o o r g y r u y e + Response T5 C U o p n d s a o ti l n id g a , t R io e n tr , ieval S ev e o ss lv io in n g -le d v ia e l l o m gu e e m s o y r s y te t m racking, 2022 [LINK]\n( F 2 W 0 L 2 a O 4 n e W g ) e - t R a A l. G , Updating K Ba n s o e w + le Q dg u e ery Response G lla P m T a 4 2 o - , 7 G B e - m ch i a n t i, forgetting RAG-basedunlearning 2024 [LINK]\n( F 2 J 0 L ia 2 A n 3 g R b) e E tal., Retrieval D Q a u t e a r b y ase+ Response WebGPT,WebCPM retrieval A g q e u c n e ti e r v y r e a p t r i r e o e t n d r , i i e c f v t o i a r o l w n d a u rd ri - n lo g oking 2023 [LINK]\n( H 2 G 0 i 2 u p 4 t p i ) é o r R re A z G etal., Retrieval C Q o u n er te y xt+ Response C G Ll o P a l T m B - a E 3 - . R 5 3 T - .1 t v u - 2 8 rb , B o , , 70B Indexing H m gr i u a p p l p t h i o -h c in a o t m p eg p Q r a a A l t - i , i o n K n s n p o ir w ed le r d e g tr e ieval, 2024 [LINK]\n( I 2 J t 0 a e 2 n r 4 C g ) Q et R al., Retrieval D H Q i i u a s e t l r o o y r g y ue + R R e e t s r u ie lt v s ed Transformer++ Retrieval I c t o e n ra te ti x v t e -a q w u a e r r e y q r u ef e o ry rm re u w la r t i i t o in n g , 2024 [LINK]\n( E 2 C 0 W h 2 e 4 E n a) etal., M Ge e n m er o a r t y io G n rounded Context Response Llama-3.1-70B,8B Updating,Retrieval E o fa n x c l p t i u n li a e c l i f t l a o w c n t o - g c r - k h fo i e n r c m g ki m n g g e e n m f e e o r e a r d y t b i , o a n ck, 2025 [LINK]\n( M 2 Q 0 E 2 ia 4 M n ) e O t R al A ., G M Ge e n m er o a r t y io G n rounded C Q o u n er te y xt+ Response M P in h s i i s t - r t 3 u ra - c m l t 7 , i B G n - i P - I 1 n T 2 s - t 4 8 r o K uc - t, R C e o t m ri p ev re a s l s , ion G m Fe l e o e m d b b a o l a ry c m k c e - o g m m u o i p d ry r e e d r s e s g t i r e o i n n e e , v r a a l t , i K on V 2024 [LINK]\n( R 2 L 0 e e 2 a e 4 d c e A ) t g a e l. n , t Generation C Q o u n er te y xt+ R P S a u e s m t s ri a m e g v a e e r s d y /- PaLM2 Updating,Retrieval E m co p e n i m s te o o x d r t i y c w r g i e n i t s r d t i o e m w va e l m ,e o x ry te , n d d y e n d amic 2024 [LINK]\n( I 2 S C 0 a 2 A r 4 c L ) hetal., Generation T E a x s a k m I p n l s e tr s u + ction T Tr h a o j u ec g t h o t r s y+ GPT4V,Qwen2VL Updating T m co r u a r l r j t e e i c c -m t t i o o o r n y da a l b , s it t e ra ra c t t i i v o e n r m ea e s m on o i r n y g , 2025 [LINK]\nTable7: Overviewofmethodsforlong-termmemoryinmemorymanagementandutilization. \"TF\"(Training\nFree)denoteswhetherthemethodoperateswithoutadditionalgradient-basedupdates. \"RE\"(RetrievalModule)\ndenotes whether the method needs Retrieval. \"DS\" (Dialogue System) denotes whether the method aims for a\ndialoguetask.\nMethod Type TF DF Operations LMs Features Year Code\nStreamingLLM KVCache StaticKVcachedropping,Attention\nCompression Llama-2,MPT,PyThia,Falcon 2023 [LINK]\n(Xiaoetal.,2024) Dropping sinkintheinitialtokens\nFastGen KVCache Adaptiveprofiling-basedKVcache\nCompression Llama-17B/13B/30B/65B 2023 [LINK]\n(Geetal.,2024) Dropping dropping\nH2O\nKVCache DynamicaKVcachedropping,Retain\n(Zhangetal., Compression OPT,Llama-1,GPT-NeoX 2023 [LINK]\nDropping HeavyHittertokens\n2023d)\nLWM-Text-Chat-1M,\nSnapKV KVCache LongChat-7b-v1.5-32k, Head-wiseKVcachedropping,\nCompression 2024 [LINK]\n(Lietal.,2024e) Dropping Mistral-7B-Instruct-v0.2, Attentionheadbehavior\nMixtral-8x7B-Instruct-v0.1\nKVCache\nLESS Low-rankKVcachestorage,enable\nStoring Compression Llama-213B,Falcon7B 2024 [LINK]\n(Dongetal.,2024) queryingalltokens\nOptimization\nKVCache\nKIVI Llama-27B/13B,Llama-38B,\nStoring Compression AsymmetricalKVcachequantization 2024 [LINK]\n(Liuetal.,2024f) Falcon7B,Mistral-7B\nOptimization\nLLaMA-7B/13B/30B/65B,\nKVQuant KVCache\nLlama-2-7B/13B/70B,\n(Hooperetal., Storing Compression KVcachequantization 2024 [LINK]\nLlama-3-8B/70B,and\n2024) Optimization\nMistral-7B\nQUEST KVCache LongChat-7B-v1.5-32K,\nRetrieval Query-awareKVcacheselection 2024 [LINK]\n(Tangetal.,2024) Selection Yarn-Llama2-7B-128K\nTokenSelect KVCache Qwen27B,Llama-38B, Dynamictoken-levelKVcache\nRetrieval 2024 [LINK]\n(Wuetal.,2025) Selection Yi-1.5-6B selection\nTable8: Overviewofmethodsforlong-contextmemoryinParametricEfficiency. “TF”(TrainingFree)denotes\nwhetherthemethodoperateswithoutadditionalgradient-basedupdates. “DF”(DroppingFree)denoteswhetherthe\nmethodabletomaintainalltheKVcachewithoutdropping.\nMethod Type SM TM Operations LMs Features Year Code\nGraphReader Context Graph-basedagent,Structuringlong\nT G Retrieval GPT-4-128k 2024 [LINK]\n(Lietal.,2024c) Selection contexttoagraph\nSparseRAG Context Sparsecontextselection,Reduce\nT P Retrieval Gemini 2024 N/A\n(Zhuetal.,2025) Selection involveddocumentsindecoding\nZiya-Reader Context Supervisedfinetuning,Positionagnostic\nT T Retrieval Ziya2-13B-Base 2023 [LINK]\n(Heetal.,2024b) Selection multi-stepQA\nxRAG Context\nT P Compression Mistral-7bandMixtral-8x7b Softpromptcompression 2024 [LINK]\n(Chengetal.,2024) Compression\nRECOMP Context GPT-2,GPT2-XL,GPT-J, Hardpromptcompression,extractive\nT T Compression 2023 [LINK]\n(Xuetal.,2024a) Compression Flan-UL2 compressor,abstractivecompressor\nLongLLMLingua Context GPT-3.5-Turbo-06136,\nT T Compression Hardpromptcompression 2023 [LINK]\n(Jiangetal.,2024a) Compression LongChat-13B-16k\nQGC Context LongChat-13B16K, Query-guideddynamiccontext\nT T Compression 2024 [LINK]\n(Caoetal.,2024) Compression LLaMA-2-7B compression\nTable9: Overviewofmethodsforlong-contextmemoryinContextualUtilization. “SM”(SourceModal)denotes\nthesourcemodalityofcontextualmemory. “TM”(TargetModal)denotestargetmodality(processedforselection/\naftercompression)ofcontextualmemory(T–Text,G–Graphs,P–Parametric).\nMethod Type PR TF BES SEO LMs MainAdvancement Year Code\nProtectthepreservedknowledgebyprojecting\ngpt2-xl-1.5b,\nAlphaEdit locating-then- perturbationontothenullspace.\ngpt-j-6b, 2024 [LINK]\n(Fangetal.,2025) editing Addaregularizationtermwhenoptimizingv*\nllama3-8b\nforsequentialediting.\nMEMATisexpandeduponMEMITwith\nMEMAT locating-then-\naguila-7b attentionheadscorrectionsforcross-lingual 2024 [LINK]\n(Melaetal.,2024) editing\nediting.\nOptimizearelaxedleast-squaresobjective,\nMEMIT locating-then- gpt-j-6b,\nenablingasimpleclosed-formsolutionfor 2022 [LINK]\n(Mengetal.,2023) editing gpt-neox-20b\nefficientmassivebatchediting.\nThemostclassiclocate-the-editmethod.\nROME locating-then-\ngpt2-xl-1.5b Performarank-oneupdateontheweightsofa 2022 [LINK]\n(Mengetal.,2022a) editing\nsingleMLPlayer.\nSupportssequentialeditingthrough\nDAFNET gpt-j-6b, Intra-editingAttentionFlow(withinfacts)\nmetalearning 2024 [LINK]\n(Zhangetal.,2024c) llama2-7b andInter-editingAttentionFlow(across\nfacts).\ngpt-neo\ngpt-j-6b\nMEND t5-xl MorescalableandfastthanKE.Decompose\nmetalearning 2021 [LINK]\n(Mitchelletal.,2022b) t5-xxl gradientintorank-oneouterproductform.\nbert-base\nbart-base\nThefirstoneemploysahypernetworkto\nKE bert-base, learnhowtomodifythegradient.PoseLSTM\nmetalearning 2021 [LINK]\n(DeCaoetal.,2021) bart-base toprojectthesentenceembeddingintorank-1\nmaskoverthegradient.\ngpt-j-6b,\ngpt2-xl-1.5b,\nIKE\nprompt - - gpt-neo, ThefirstuseICLtoeditknowledgeinLLMs. 2023 [LINK]\n(Zhengetal.,2023)\ngpt-neox,\nopt-175b\nMeLLo vicuna-7b,\nprompt - - QuestionDecompose+SelfCheck 2023 [LINK]\n(Zhongetal.,2023) gpt-j-6b\nIntroduceadecoupledlatentmemorymodule\nLarimar additional\ngpt2-xl,gpt-j-6b thatconditionstheLLMdecoderattesttime 2024 [LINK]\n(Dasetal.,2024) parameters\nwithoutparameterupdates.\nIntroducesafixed-sizememorypoolina\nMEMORYLLM additional\nllama2-7b frozenLLMthatisincrementallyand 2024 [LINK]\n(Wangetal.,2024i) parameters\nselectivelyupdatedwithnewknowledge.\nllama2-7b, SupportsequentialeditingbySideMemory\nWISE additional\nmistral-7b, DesignandKnowledgeShardingand 2024 [LINK]\n(Wangetal.,2024c) parameters\ngpt-j-6b Merging.\nCaliNET additional t5-base, AddtheoutputofFFN-likeCaliNETtothe\n2022 [LINK]\n(Dongetal.,2022) parameters t5-large originalFFNoutput.\nt5-large, ScopeClassifier+CounterfactualModel.\nSERAC additional\nbert-base, Sequentiallyorsimultaneouslyapplyingkedits 2022 [LINK]\n(Mitchelletal.,2022c) parameters\nblenderbot-90m yieldsthesameeditedmodel.\nTable10: Overviewofmethodsforparametricmemoryoptimizationinediting. \"PR\"(ParametricReserving)\nindicateswhetherthemethodavoidsdirectmodificationofthemodel’sinternalweights. \"TF\"(Training-Free)\ndenoteswhetherthemethodoperateswithouttraditionaliterativeoptimization. \"BES\"(BatchEditingSupport)\nreflectsthemethod’sabilitytohandlemultipleeditssimultaneously. \"SEO\"(SequentialEditingOptimization)\nspecifies whether the method introduces mechanisms tailored for sequential Editing. \"LMs\" lists the language\nmodelsusedforempiricalevaluation.\nMethod Type PR TF BUS SUO LMs MainAdvancement Year Code\nDerivetheunlearnedLLMbycomputingthe\nULD additional llama2-chat-7b,\nlogitdifferencebetweenthetargetandthe 2024 [LINK]\n(Jietal.,2024) parameters mistral-7b-instruct\nassistantLLMs.\nIntroduceunlearninglayerswhicharelearned\nEUL additional t5-base, toforgetrequesteddata.Supportsequential\n2023 [LINK]\n(ChenandYang,2023) parameters t5-3b unlearningbyusingafusionmechanismto\nmergedifferentunlearninglayers.\nECOunlearnsbycorruptingprompt\nECO 68llmsrangingfrom\nprompt embeddingsbasedonclassifierdetection 2024 [LINK]\n(Liuetal.,2024b) 0.5bto236b\nwithoutchangingthemodel.\nbloom-560m,\nICUL bloom-1.1b,\nprompt - - ThefirstuseICLforunlearninginLMs. 2023 [LINK]\n(Pawelczyketal.,2024) bloom-3b,\nllama2-7b\nWAGLEusesbi-leveloptimizationtocompute\nllama2-7b-chat,\nWAGLE locating-then- weightattributionscoresthatguideselective\nzephyr-7b-beta, 2024 [LINK]\n(Jiaetal.,2024a) unlearning fine-tuningforefficientandmodular\nllama2-7b\nunlearning.\nTable11: Overviewofmethodsforparametricmemoryoptimizationinunlearning. \"PR\"(ParametricReserving)\nindicateswhetherthemethodavoidsdirectmodificationofthemodel’sinternalweights. \"TF\"(Training-Free)\ndenoteswhetherthemethodoperateswithouttraditionaliterativeoptimization. \"BUS\"(BatchUnlearningSupport)\nreflectsthemethod’sabilitytohandlemultipleeditssimultaneously. \"SUO\"(SequentialUnlearningOptimization)\nspecifies whether the method introduces mechanisms tailored for sequential Editing. \"LMs\" lists the language\nmodelsusedforempiricalevaluation.\nMethod Type TF TB TS Domain LMs MainAdvancement Year Code\nEmploysatrainingobjectivethat\nSELF-\nRegularization- Task- Question minimizestheKullback-Leibler(KL)\nPARAM T5 2025 [LINK]\nbasedLearning Free Answering divergencebetweenthepredictionsof\n(Wangetal.)\ntheoriginalmodelandtargetmodel.\nIntegrateMaintainingasmall,\nMBPA++ randomlyselectedsubset(aslowas\nREPLAY,\n(Wangetal., Replay-based CIL None 1%)ofpastexamplesinmemorycan 2025 [LINK]\nMBPA\n2024j) achieveperformancecomparableto\nlargermemorysizes.\nIntegratemultiplestorage\nLSCS Abstracting/\nInteractive mechanismsandachieveboth\n(Wangetal., CIL Merging/ / 2025 [LINK]\nLearning abstractionandexperiencemergingand\n2024j) Retrieval\nlong-termretentionwithaccuraterecall.\nTaSL Parameter-leveltaskskilllocalization\nRegularization- Dialogue\n(Fengetal., TIL T5,Llama-7B andconsolidationenableknowledge 2024 [LINK]\nbasedLearning System\n2024) transferwithoutmemoryreplay.\nEMP\nEvent BERT-ED, Designcontinuouspromptsassociated\n(Liuetal., Replay-based CLI 2023 [LINK]\ndetection KCN witheacheventtype.\n2022a)\noEWC,SI, Introducingadaptivecoefficientsthat\nUDIL\nInteractive Event LwF,A-GEM, areoptimizedduringtrainingtoachieve\n(ShiandWang, DLI 2023 [LINK]\nLearning detection CLS-ER,ESM, tightergeneralizationerrorboundsand\n2023)\netc. betterperformanceacrossdomains.\nDSI++ Enablescontinualdocumentindexing\nInformation\n(Mehtaetal., Replay-based TIL T5 whileretainingqueryperformanceon 2022 [LINK]\nRetrieval\n2022) oldandnewdata.\nEnhancesmemoryreplayby\nMRDC\nObject LUCIR, compressingdata,balancingsample\n(Wangetal., Replay-based CIL 2022 [LINK]\ndetection PODNet qualityandquantityforcontinual\n2022)\nlearning.\nTable12: Overviewofmethodsforparametricmemorymodificationincontinuallearning. \"TB\"denotesthe\ntaskboundarywhetherexists. \"TS\"denotesthetasksettingsincludingTIL(TaskIncrementalLearning),CIL(Class\nIncrementalLearning),DIL(DomainIncrementalLearning),Task-Free.\nMethod Type TF STs SNs Input Output LMs Ops Features Year Code\nGPT-\n3.5,GPT-4,\nGoG KG+ Qwen-1.5-\nRetrieval, integrateinternaland\n(Xuetal., reasoning KG+text WebQSP,CWQ prompt+ answer 72B-Chat, 2024 [LINK]\nCompression externalknowledge\n2024c) query LLaMA3-\n70B-\nInstruct\nConflictspan\nRKC-LLM\nmodel+ localization,\n(Wangetal., conflict prompt+context entities answer ChatGPT Compression 2024 [LINK]\ntext instruction-guided\n2023a)\nconflicthandling\nGPT-4,\nBGC-KC GPT-3.5, attributiontracing\nmodel+ documents Retrieval,\n(Tanetal., conflict AIG,AIR answer Llama2- framework,evaluate 2024 [LINK]\ntext +query Compression\n2024a) 13b, LLMbias\nLlama2-7b\nSemi-structured\nSem-CoT Knowledge llama2-7b,\nWikidata,2Wiki, CoTprompt Retrieval, promptingfor\n(Suetal., reasoning Graph+ answer 13b,70b, 2023 [LINK]\nMuSiQue,TKB +Query Compression multi-sourceinput\n2023) text+Model 65b\nfusion\nWikidata,\nHeterogeneous\nWikipedia,and\nknowledgeintegration,\nCoK Database+ Wikitables,\nCoTprompt gpt-3.5- Retrieval, dynamicknowledge\n(Lietal., reasoning Tables+ Flashcard, answer 2023 [LINK]\n+Query turbo Compression retrieval,adaptivequery\n2023) Text UpToDate,\ngenerationacross\nScienceQA,\nformats\nCK-12\nTwo-hopreasoning,\nDIVKNOWQA\nKnowledge Wikidata, CoTprompt gpt-3.5- Retrieval, symbolicquery\n(Zhaoetal., reasoning answer 2023 [LINK]\nBase+text DIVKNOWQA +Query turbo Compression generationforstructured\n2024a)\ndata\nStructRAG Cognitive-inspired\nKG+Table Loong,Podcast documents Qwen2-7B, Retrieval,\n(Lietal., reasoning answer structurization,dynamic 2023 [LINK]\n+text Transcripts +query 72B Compression\n2024g) structureselection\nTable13: Overviewofmethodsformulti-sourcememoryincross-textualintegration. \"TF\"(TrainingFree)\ndenoteswhetherthemethodoperateswithoutadditionalgradient-basedupdates. \"STs\"denotesthesourcetypes.\n\"SNs\"denotesthesourcedatasetnames.\nMethod Type TF DS Mo Input Output Modeling Ops Features Year Code\nLLaVa,\nmulti-modalmemorybank,\nIGSR image- GPT4,\nstickerretrieval,intention\n(Wangetal., retrieval text+image text stickers Qwen-VL, retrieval 2025 [LINK]\nawarecross-session\n2025a) dialogue CLIP,\ndialogue\nLlama3\nVISTA CLIP,\nimage- retrieved VisualTokenInjection,\n(Zhouetal., retrieval text+image BLIP-B, retrieval 2024 [LINK]\ntextquery response composeddatafine-tuning\n2024) Pic2Word\nUniVL-DR\nimage- retrieved VinVLDPR, Modality-balancedhard\n(Liuetal., retrieval text+image retrieval 2023 [LINK]\ntextquery response CLIP-DPR negatives\n2022b)\nMultiInstruct* instruction\nCross-modaltransfer\n(Xuetal., fusion text+image + response OFA compression 2023 [LINK]\nlearning\n2023) instances\nNextChat\ntext+image image+\n(Zhangetal., fusion response CLIP compression Cross-modalalignment 2023 [LINK]\n+boxes text\n2023b)\nUniTranSeR Intention-awareresponse\nMLM+\n(Maetal., fusion text+image context response compression generation,unified 2022 [LINK]\nMPM\n2022) transformerspace\nTable14: Overviewofmethodsformulti-sourcememoryinMulti-modalCoordination. \"TF\"(TrainingFree)\ndenoteswhetherthemethodoperateswithoutadditionalgradient-basedupdates. \"DS\"(DialogueSystem)denotes\nwhetherthemethodaimsforadialoguetask. \"Mo\"denotesdatamodality(T–Text,I–Images,B–Box(Position)).\nSource\nMemoryTool Level Taxonomy Operation Function Input/Output ExampleUse Access\nType\nVectorDatabase-Indexalarge\nsetoftextembeddingsand\nFAISS Consolidation, Libraryforfaststorage,\nContextual- vector/Index, quicklyretrievethemost\n(Douzeetal., Components Indexingand indexing,andRetrievalof open [LINK]\nUnstructured relevancescore relevantdocumentsforauser’s\n2024) retrieval high-dimensionalvectors\nqueryinaretrieval-augmented\ngeneration(RAG)system.\nNodesand GraphDatabase-Modeland\nConsolidation, Nativegraphdatabase\nrelationships retrievecomplexrelationaldata\nNeo4j Contextual- Indexing, supportingACID conditional\nComponents withproperties/ forusecaseslikefraud [LINK]\n(Neo4j,2012) Structured Updating, transactionsandCypher open\nQueryresultsvia detectionandrecommendation\nRetrieval querylanguage\nCypher engines.\nAprobabilisticranking\nfunctionusedin\nBM25 Textqueries/\nContextual- informationretrievalto Enhancingsearchengineresults\n(Robertson Components Retrieval Rankedlistof open [LINK]\nUnstructured estimatetherelevanceof anddocumentretrievalsystems.\netal.,1995) documents\ndocumentstoagiven\nsearchquery.\nAnunsuperviseddense\nretrievertrainedwith\nContriever contrastivelearning, Querytext/List High-recallretrievaltasksin\nContextual-\n(Izacardetal., Components Retrieval capableofretrieving ofsimilar multilingual open [LINK]\nUnstructured\n2021) semanticallysimilar documents question-answeringsystems.\ndocumentsacross\nlanguages.\nEmbedding\nTechniquesthatconvert\nModels(e.g.\ntext,images,oraudiointo Textsimilaritycomputation,\nOpenAI Consolidation, Rawdata/Vector\nComponents Contextual densevector recommendationsystems,and open [LINK]\nembedding Retrieval embeddings\nrepresentationscapturing clusteringtasks.\n(OpenAI,\nsemanticmeaning.\n2025))\nTable15: Component-LevelToolsforMemoryManagementandUtilization.\nMemory Source\nLevel Taxonomy Operation Function Input/Output ExampleUse Access\nTool Type\nConsolidation, Frameworkforbuildingand\nGraphiti Multi-sourcedata Constructingreal-time\nContextual- Indexing, queryingtemporally-aware\n(Heetal., framework /Queryable knowledgegraphstoenhance open [LINK]\nStructured Updating, knowledgegraphstailoredforAI\n2025) knowledgegraph AIagentmemory.\nRetrieval agentsindynamicenvironments.\nConsolidation, Aflexibleframeworkforbuilding Text/Context- Developingknowledge\nLLamaIndex\nframework Contextual Indexing, knowledgeassistantsusingLLMs augmented assistantsthatprocess open [LINK]\n(Liu,2022)\nRetrieval connectedtoenterprisedata. responses complexdataformat.\nConsolidation,\nProvidesaframeworkforbuilding CreatingcomplexLLM\nLangChain Indexing, Inputprompts/\ncontext-aware,reasoning applicationslike\n(Chase, framework Contextual Updating, Multi-step open [LINK]\napplicationsbyconnectingLLMs question-answeringsystems\n2022) Forgetting, reasoningoutputs\nwithexternaldatasources. andchatbots.\nRetrieval\nConsolidation,\nConstructscontrollableagent\nIndexing, Buildingcomplextask\nLangGraph Contextual- architecturessupportinglong-term Graphstate/State\nframework Updating, workflowswithmultipleAI open [LINK]\n(Inc.,2025) Structured memoryandhuman-in-the-loop updates\nForgetting, agents.\nmulti-agentsystems.\nRetrieval\nEasyEdit Aneasy-to-useknowledgeediting\nEditinstructions/ ModifyingLLMknowledge\n(Wang frameworkforLLMs,enabling\nframework Parametric Updating Updatedmodel inspecificdomains,suchas open [LINK]\netal., efficientbehaviormodification\nbehavior updatingfactualinformation.\n2024d) withinspecificdomains.\nAplatformforbuildingand\nCrewAI Automatingworkflows\nConsolidation, deployingmulti-agentsystems, Multi-agenttasks\n(Duanand acrossagentslikeproject\nframework Contextual Indexing, supportingautomatedworkflows /Collaborative open [LINK]\nWang, managementandcontent\nRetrieval usinganyLLMandcloud results\n2024) generation.\nplatform.\nLetta Constructsstatefulagentswith\nUserinteractions\n(Packer Contextual- Consolidation, long-termmemory,advanced DevelopingAIagentsthat\nframework /Improved open [LINK]\netal., Unstructured Retrieval reasoning,andcustomtoolswithin learnandimproveovertime.\nResponse\n2023) avisualenvironment.\nTable16: Framework-LevelToolsforMemoryManagementandUtilization.\nMemory Source\nLevel Taxonomy Operation Function Input/Output ExampleUse Access\nTool Type\nEnhancingAIsystemswith\nMem0 Consolidation, Providesasmartmemorylayerfor\nUserinteractions persistentcontextfor\n(Taran- Application Contextual- Indexing, LLMs,enablingdirectaddition,\n/Personalized customersupportand open [LINK]\njeetSingh, Layer Unstructured Updating, updating,andsearchingof\nresponses personalized\n2024) Retrieval memoriesinmodels.\nrecommendations.\nZep Consolidation, Chatlogs, AugmentingAIagentswith\nIntegrateschatmessagesintoa\n(Rasmussen Application Contextual- Indexing, businessdata/ knowledgethrough\nknowledgegraph,offeringaccurate open [LINK]\netal., Layer Structured Updating, Knowledgegraph continuouslearningfrom\nandrelevantuserinformation.\n2025) Retrieval queryresults userinteractions.\nConsolidation, Anopenmemorylayerthat Agenttasks/\nMemary BuildingAIagentswith\nApplication Indexing, emulateshumanmemorytohelp Memory\n(kingjulio8238, Contextual human-likememory open [LINK]\nLayer Updating, AIagentsmanageandutilize managementand\n2025) characteristics.\nRetrieval informationeffectively. utilization\nConsolidation, Auserprofile-basedlong-term Implementingvirtual\nMemobase Userinteractions\nApplication Indexing, memorysystemdesignedto assistants,educationaltools,\n(memodb Contextual /Personalized open [LINK]\nLayer Updating, providepersonalizedexperiences andpersonalizedAI\nio,2025) responses\nRetrieval ingenerativeAIapplications. companions.\nTable17: ApplicationLayer-LevelToolsforMemoryManagementandUtilization.\nSource\nMemoryTool Level Taxonomy Operation Function Input/Output ExampleUse Access\nType\nUserinputs(text,\nConsolidation, AI-poweredpersonalassistantthat\nvoice)/ Personalproductivity\nIndexing, organizesnotes,tasks,and\nMe.bot product Contextual Organizednotes, enhancement,emotional closed [LINK]\nUpdating, memories,providingemotional\nreminders, support,ideaorganization.\nRetrieval supportandproductivitytools.\nsummaries\nUserqueries/\nConsolidation, Intelligentworkstationpoweredby\nCustomized Enhancinglearning\nIndexing, Tencent’sMixHuangmodel,\nima.copilot Product Contextual responses, efficiency,workproductivity, closed [LINK]\nUpdating, buildingapersonalknowledgebase\nknowledge knowledgemanagement.\nRetrieval forlearningandworkscenarios.\nretrieval\nUser-defined\nCoze Enablingmulti-agentcollaboration\nProduct Contextual Consolidation workflows/ Deployedchatbots,AIagents closed [LINK]\n(Coze,2024) acrossvariousplatforms.\nResponse\nAIassistantdevelopedbyxAI,\nQuery/\ndesignedtoprovidetruthful,useful, Answeringquestions,\nGrok(xAI, Retrieval, Informative\nProduct Contextual andcuriousresponses,with generatingimages,providing closed [LINK]\n2023) Compression answers,\nreal-timedataaccessandimage insights.\ngeneratedimages\ngeneration.\nConversationalAIdevelopedby\nChatGPT Userprompts/ Answeringquestions,\nConsolidation, OpenAI,capableofunderstanding\n(OpenAI, Product Contextual Generatedtext generatingimages,providing closed [LINK]\nRetrieval andgeneratinghuman-liketext\n2022) responses insights.\nbasedonprompts.\nTable18: Product-LevelToolsforMemoryUtilization."
}