{
    "original_filename": "2504.19413v1.pdf",
    "extraction_timestamp": "2025-05-11T13:50:39.581488",
    "source_file_extension": ".pdf",
    "file_size_bytes": 1144031,
    "sha256_hash": "bec870b657aa73405275a6d8fe27bcd4271799e028bc62986ab9c4cd27a3712d",
    "content": "Mem0: Building Production-Ready AI Agents with\nScalable Long-Term Memory\nPrateekChhikara,DevKhant,SaketAryan,TaranjeetSingh,andDeshrajYadav\nresearch@mem0.ai\nLargeLanguageModels(LLMs)havedemonstratedremarkableprowessingeneratingcontextuallycoherent\nresponses, yet their fixed context windows pose fundamental challenges for maintaining consistency over\nprolongedmulti-sessiondialogues. WeintroduceMem0,ascalablememory-centricarchitecturethataddresses\nthisissuebydynamicallyextracting,consolidating,andretrievingsalientinformationfromongoingconver-\nsations. Building on this foundation, we further propose an enhanced variant that leverages graph-based\nmemoryrepresentationstocapturecomplexrelationalstructuresamongconversationalelements. Through\ncomprehensiveevaluationsontheLOCOMObenchmark,wesystematicallycompareourapproachesagainstsix\nbaselinecategories: (i)establishedmemory-augmentedsystems,(ii)retrieval-augmentedgeneration(RAG)\nwith varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation\nhistory,(iv)anopen-sourcememorysolution,(v)aproprietarymodelsystem,and(vi)adedicatedmemory\nmanagementplatform. Empiricalresultsdemonstratethatourmethodsconsistentlyoutperformallexisting\nmemory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. No-\ntably,Mem0achieves26%relativeimprovementsintheLLM-as-a-JudgemetricoverOpenAI,whileMem0with\ngraphmemoryachievesaround2%higheroverallscorethanthebaseMem0configuration. Beyondaccuracy\ngains,wealsomarkedlyreducecomputationaloverheadcomparedtothefull-contextapproach. Inparticular,\nMem0attainsa91%lowerp95latencyandsavesmorethan90%tokencost,therebyofferingacompelling\nbalancebetweenadvancedreasoningcapabilitiesandpracticaldeploymentconstraints. Ourfindingshighlight\nthecriticalroleofstructured,persistentmemorymechanismsforlong-termconversationalcoherence,paving\nthewayformorereliableandefficientLLM-drivenAIagents.\nCodecanbefoundat: https://mem0.ai/research\n1. Introduction\nHuman memory is a foundation of intelligence—it shapes our identity, guides decision-making, and enables\nus to learn, adapt, and form meaningful relationships (Craik and Jennings, 1992). Among its many roles,\nmemoryisessentialforcommunication: werecallpastinteractions,inferpreferences,andconstructevolving\nmental models of those we engage with (Assmann, 2011). This ability to retain and retrieve information\nover extended periods enables coherent, contextually rich exchanges that span days, weeks, or even months.\nAI agents, powered by large language models (LLMs), have made remarkable progress in generating fluent,\ncontextually appropriate responses (Yu et al., 2024, Zhang et al., 2024). However, these systems are\nfundamentally limited by their reliance on fixed context windows, which severely restrict their ability to\nmaintain coherence over extended interactions (Bulatov et al., 2022, Liu et al., 2023). This limitation stems\nfrom LLMs’ lack of persistent memory mechanisms that can extend beyond their finite context windows.\nWhile humans naturally accumulate and organize experiences over time, forming a continuous narrative\n5202\nrpA\n82\n]LC.sc[\n1v31491.4052:viXra\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nFigure1:Illustrationofmemoryim-\nportanceinAIagents. Left: Without\npersistentmemory,thesystemforgets\ncriticaluserinformation(vegetarian,\ndairy-free preferences) between ses-\nsions, resulting in inappropriate rec-\nommendations. Right: Witheffective\nmemory, the system maintains these\ndietarypreferencesacrossinteractions,\nenablingcontextuallyappropriatesug-\ngestionsthatalignwithpreviouslyes-\ntablishedconstraints.\nof interactions, AI systems cannot inherently persist information across separate sessions or after context\noverflow. The absence of persistent memory creates a fundamental disconnect in human-AI interaction.\nWithout memory, AI agents forget user preferences, repeat questions, and contradict previously established\nfacts. Consider a simple example illustrated in Figure 1, where a user mentions being vegetarian and\navoiding dairy products in an initial conversation. In a subsequent session, when the user asks about dinner\nrecommendations, a system without persistent memory might suggest chicken, completely contradicting\nthe established dietary preferences. In contrast, a system with persistent memory would maintain this\ncriticaluserinformationacrosssessionsandsuggestappropriatevegetarian,dairy-freeoptions. Thiscommon\nscenario highlights how memory failures can fundamentally undermine user experience and trust.\nBeyond conversational settings, memory mechanisms have been shown to dramatically enhance agent\nperformance in interactive environments (Majumder et al., Shinn et al., 2023). Agents equipped with\nmemory of past experiences can better anticipate user needs, learn from previous mistakes, and generalize\nknowledge across tasks (Chhikara et al., 2023). Research demonstrates that memory-augmented agents\nimprove decision-making by leveraging causal relationships between actions and outcomes, leading to more\neffective adaptation in dynamic scenarios (Rasmussen et al., 2025). Hierarchical memory architectures\n(Packer et al., 2023, Sarthi et al., 2024) and agentic memory systems capable of autonomous evolution (Xu\net al., 2025) have further shown that memory enables more coherent, long-term reasoning across multiple\ndialogue sessions.\nUnlikehumans,whodynamicallyintegratenewinformationandreviseoutdatedbeliefs,LLMseffectively\n“reset\" once information falls outside their context window (Zhang, 2024, Timoneda and Vera, 2025). Even\nas models like OpenAI’s GPT-4 (128K tokens) (Hurst et al., 2024), o1 (200K context) (Jaech et al., 2024),\nAnthropic’s Claude 3.7 Sonnet (200K tokens) (Anthropic, 2025), and Google’s Gemini (at least 10M tokens)\n(Team et al., 2024) push the boundaries of context length, these improvements merely delay rather than\nsolve the fundamental limitation. In practical applications, even these extended context windows prove\ninsufficient for two critical reasons. First, as meaningful human-AI relationships develop over weeks or\nmonths, conversation history inevitably exceeds even the most generous context limits. Second, and perhaps\nmoreimportantly,real-worldconversationsrarelymaintainthematiccontinuity. Ausermightmentiondietary\npreferences (being vegetarian), then engage in hours of unrelated discussion about programming tasks,\nbefore returning to food-related queries about dinner options. In such scenarios, a full-context approach\nwould need to reason through mountains of irrelevant information, with the critical dietary preferences\npotentially buried among thousands of tokens of coding discussions. Moreover, simply presenting longer\ncontexts does not ensure effective retrieval or utilization of past information, as attention mechanisms\n2\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\ndegradeoverdistanttokens(Guoetal.,2024,Nelsonetal.,2024). Thislimitationisparticularlyproblematic\nin high-stakes domains such as healthcare, education, and enterprise support, where maintaining continuity\nandtrustiscrucial(Hatalisetal.,2023). Toaddressthesechallenges,AIagentsmustadoptmemorysystems\nthat go beyond static context extension. A robust AI memory should selectively store important information,\nconsolidaterelatedconcepts,andretrieverelevantdetailswhenneeded—mirroringhumancognitiveprocesses\n(He et al., 2024). By integrating such mechanisms, we can develop AI agents that maintain consistent\npersonas, track evolving user preferences, and build upon prior exchanges. This shift will transform AI from\ntransient, forgetful responders into reliable, long-term collaborators, fundamentally redefining the future of\nconversational intelligence.\nIn this paper, we address a fundamental limitation in AI systems: their inability to maintain coher-\nent reasoning across extended conversations across different sessions, which severely restricts meaningful\nlong-term interactions with users. We introduce Mem0 (pronounced as mem-zero), a novel memory archi-\ntecture that dynamically captures, organizes, and retrieves salient information from ongoing conversations.\nBuilding on this foundation, we develop Mem0g, which enhances the base architecture with graph-based\nmemory representations to better model complex relationships between conversational elements. Our\nexperimental results on the LOCOMO benchmark demonstrate that our approaches consistently outperform\nexisting memory systems—including memory-augmented architectures, retrieval-augmented generation\n(RAG) methods, and both open-source and proprietary solutions—across diverse question types, while\nsimultaneously requiring significantly lower computational resources. Latency measurements further reveal\nthatMem0operateswith91%lowerresponsetimesthanfull-contextapproaches,strikinganoptimalbalance\nbetween sophisticated reasoning capabilities and practical deployment constraints. These contributions\nrepresentameaningfulsteptowardAIsystemsthatcanmaintaincoherent,context-awareconversationsover\nextendeddurations—mirroringhumancommunicationpatternsandopeningnewpossibilitiesforapplications\nin personal tutoring, healthcare, and personalized assistance.\n2. Proposed Methods\nWe introduce two memory architectures for AI agents. (1) Mem0 implements a novel paradigm that extracts,\nevaluates, and manages salient information from conversations through dedicated modules for memory\nextraction and updation. The system processes a pair of messages between either two user participants or a\nuser and an assistant. (2) Mem0g extends this foundation by incorporating graph-based memory representa-\ntions,wherememoriesarestoredasdirectedlabeledgraphswithentitiesasnodesandrelationshipsasedges.\nThis structure enables a deeper understanding of the connections between entities. By explicitly modeling\nboth entities and their relationships, Mem0g supports more advanced reasoning across interconnected facts,\nespecially for queries that require navigating complex relational paths across multiple memories.\n2.1. Mem0\nOur architecture follows an incremental processing paradigm, enabling it to operate seamlessly within\nongoing conversations. As illustrated in Figure 2, the complete pipeline architecture consists of two phases:\nextraction and update.\nTheextractionphaseinitiatesuponingestionofanewmessagepair (m t−1 ,m t ) ,wherem t representsthe\ncurrent message and m t−1 the preceding one. This pair typically consists of a user message and an assistant\nresponse, capturing a complete interaction unit. To establish appropriate context for memory extraction, the\nsystememploystwocomplementarysources: (1)aconversationsummarySretrievedfromthedatabasethat\n3\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nFigure 2: Architectural overview of the Mem0 system showing extraction and update phase. The extraction phase\nprocesses messages and historical context to create new memories. The update phase evaluates these extracted\nmemoriesagainstsimilarexistingones,applyingappropriateoperationsthroughaToolCallmechanism. Thedatabase\nservesasthecentralrepository,providingcontextforprocessingandstoringupdatedmemories.\nencapsulates the semantic content of the entire conversation history, and (2) a sequence of recent messages\n{m t−m ,m t−m+1 ,...,m t−2 } from the conversation history, where m is a hyperparameter controlling the recency\nwindow. To support context-aware memory extraction, we implement an asynchronous summary generation\nmodulethatperiodicallyrefreshestheconversationsummary. Thiscomponentoperatesindependentlyofthe\nmain processing pipeline, ensuring that memory extraction consistently benefits from up-to-date contextual\ninformation without introducing processing delays. While S provides global thematic understanding across\nthe entire conversation, the recent message sequence offers granular temporal context that may contain\nrelevant details not consolidated in the summary. This dual contextual information, combined with the new\nmessage pair, forms a comprehensive prompt P = (S,{m t−m ,...,m t−2 },m t−1 ,m t ) for an extraction function\nϕ implemented via an LLM. The function ϕ(P) then extracts a set of salient memories Ω = {ω ,ω ,...,ω }\n1 2 n\nspecifically from the new exchange while maintaining awareness of the conversation’s broader context,\nresulting in candidate facts for potential inclusion in the knowledge base.\nFollowing extraction, the update phase evaluates each candidate fact against existing memories to\nmaintain consistency and avoid redundancy. This phase determines the appropriate memory management\noperationforeachextractedfact ω ∈ Ω. Algorithm1,mentionedinAppendixB,illustratesthisprocess. For\ni\neachfact,thesystemfirstretrievesthetopssemanticallysimilarmemoriesusingvectorembeddingsfromthe\ndatabase. These retrieved memories, along with the candidate fact, are then presented to the LLM through\na function-calling interface we refer to as a ‘tool call.’ The LLM itself determines which of four distinct\noperations to execute: ADD for creation of new memories when no semantically equivalent memory exists;\nUPDATE for augmentation of existing memories with complementary information; DELETE for removal of\nmemories contradicted by new information; and NOOP when the candidate fact requires no modification to\nthe knowledge base. Rather than using a separate classifier, we leverage the LLM’s reasoning capabilities\nto directly select the appropriate operation based on the semantic relationship between the candidate fact\nand existing memories. Following this determination, the system executes the provided operations, thereby\nmaintaining knowledge base coherence and temporal consistency.\n4\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nFigure3:Graph-basedmemoryarchitectureofMem0g illustratingentityextractionandupdatephase. Theextraction\nphaseusesLLMstoconvertconversationmessagesintoentitiesandrelationtriplets. Theupdatephaseemploysconflict\ndetectionandresolutionmechanismswhenintegratingnewinformationintotheexistingknowledgegraph.\nInourexperimentalevaluation,weconfiguredthesystemwith‘m’=10previousmessagesforcontextual\nreference and ‘s’ = 10 similar memories for comparative analysis. All language model operations utilized\nGPT-4o-miniastheinferenceengine. Thevectordatabaseemploysdenseembeddingstofacilitateefficient\nsimilarity search during the update phase.\n2.2. Mem0g\nThe Mem0g pipeline, illustrated in Figure 3, implements a graph-based memory approach that effectively\ncaptures, stores, and retrieves contextual information from natural language interactions (Zhang et al.,\n2022). In this framework, memories are represented as a directed labeled graph G = (V,E,L) , where:\n• Nodes V represent entities (e.g., Alice, San_Francisco)\n• Edges E represent relationships between entities (e.g., lives_in)\n• Labels L assign semantic types to nodes (e.g., Alice - Person, San_Francisco - City)\nEach entity node v ∈ V contains three components: (1) an entity type classification that categorizes\nthe entity (e.g., Person, Location, Event), (2) an embedding vector e that captures the entity’s semantic\nv\nmeaning, and (3) metadata including a creation timestamp t . Relationships in our system are structured as\nv\ntriplets in the form (v ,r,v ) , where v and v are source and destination entity nodes, respectively, and r is\ns d s d\nthe labeled edge connecting them.\nTheextractionprocess employs atwo-stage pipeline leveraging LLMsto transform unstructured textinto\nstructured graph representations. First, an entity extractor module processes the input text to identify a set\nof entities along with their corresponding types. In our framework, entities represent the key information\nelements in conversations—including people, locations, objects, concepts, events, and attributes that merit\nrepresentation in the memory graph. The entity extractor identifies these diverse information units by\nanalyzingthesemanticimportance,uniqueness,andpersistenceofelementsintheconversation. Forinstance,\nin a conversation about travel plans, entities might include destinations (cities, countries), transportation\nmodes, dates, activities, and participant preferences—essentially any discrete information that could be\nrelevant for future reference or reasoning.\n5\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nNext, a relationship generator component derives meaningful connections between these entities,\nestablishing a set of relationship triplets that capture the semantic structure of the information. This LLM-\nbasedmoduleanalyzestheextractedentitiesandtheircontextwithintheconversationtoidentifysemantically\nsignificantconnections. Itworksbyexamininglinguisticpatterns,contextualcues,anddomainknowledgeto\ndetermine how entities relate to one another. For each potential entity pair, the generator evaluates whether\nameaningfulrelationshipexistsand,ifso,classifiesthisrelationshipwithanappropriatelabel(e.g.,‘lives_in’,\n‘prefers’, ‘owns’, ‘happened_on’). The module employs prompt engineering techniques that guide the LLM\nto reason about both explicit statements and implicit information in the dialogue, resulting in relationship\ntriplets that form the edges in our memory graph and enable complex reasoning across interconnected\ninformation. Whenintegratingnewinformation,Mem0g employsasophisticatedstorageandupdatestrategy.\nFor each new relationship triple, we compute embeddings for both source and destination entities, then\nsearch for existing nodes with semantic similarity above a defined threshold ‘t’. Based on node existence,\nthe system may create both nodes, create only one node, or use existing nodes before establishing the\nrelationshipwithappropriatemetadata. Tomaintainaconsistentknowledgegraph,weimplementaconflict\ndetection mechanism that identifies potentially conflicting existing relationships when new information\narrives. AnLLM-basedupdate resolver determinesifcertainrelationshipsshouldbeobsolete, markingthem\nas invalid rather than physically removing them to enable temporal reasoning.\nThememoryretrievalfunctionalityinMem0gimplementsadual-approachstrategyforoptimalinformation\naccess. Theentity-centricmethodfirstidentifieskeyentitieswithinaquery,thenleveragessemanticsimilarity\ntolocatecorrespondingnodesintheknowledgegraph. Itsystematicallyexploresbothincomingandoutgoing\nrelationships from these anchor nodes, constructing a comprehensive subgraph that captures relevant\ncontextual information. Complementing this, the semantic triplet approach takes a more holistic view by\nencoding the entire query as a dense embedding vector. This query representation is then matched against\ntextual encodings of each relationship triplet in the knowledge graph. The system calculates fine-grained\nsimilarityscoresbetweenthequeryandallavailabletriplets,returningonlythosethatexceedaconfigurable\nrelevancethreshold,rankedinorderofdecreasingsimilarity. ThisdualretrievalmechanismenablesMem0g to\nhandle both targeted entity-focused questions and broader conceptual queries with equal effectiveness.\nFrom an implementation perspective, the system utilizes Neo4j1 as the underlying graph database. LLM-\nbased extractors and update module leverage GPT-4o-mini with function calling capabilities, allowing for\nstructured extraction of information from unstructured text. By combining graph-based representations with\nsemantic embeddings and LLM-based information extraction, Mem0g achieves both the structural richness\nneeded for complex reasoning and the semantic flexibility required for natural language understanding.\n3. Experimental Setup\n3.1. Dataset\nThe LOCOMO (Maharana et al., 2024) dataset is designed to evaluate long-term conversational memory in\ndialoguesystems. Itcomprises10extendedconversations,eachcontainingapproximately600dialoguesand\n26000 tokens on average, distributed across multiple sessions. Each conversation captures two individuals\ndiscussing daily experiences or past events. Following these multi-session dialogues, each conversation is\naccompanied by 200 questions on an average with corresponding ground truth answers. These questions\nare categorized into multiple types: single-hop, multi-hop, temporal, and open-domain. The dataset\noriginally included an adversarial question category, which was designed to test systems’ ability to recognize\n1\nhttps://neo4j.com/\n6\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nunanswerable questions. However, this category was excluded from our evaluation because ground truth\nanswerswereunavailable,andtheexpectedbehaviorforthisquestiontypeisthattheagentshouldrecognize\nthem as unanswerable.\n3.2. Evaluation Metrics\nOur evaluation framework implements a comprehensive approach to assess long-term memory capabilities\nin dialogue systems, considering both response quality and operational efficiency. We categorize our metrics\ninto two distinct groups that together provide a holistic understanding of system performance.\n(1)PerformanceMetrics PreviousresearchinconversationalAI(Goswami,2025,Sonietal.,2024,Singh\net al., 2020) has predominantly relied on lexical similarity metrics such as F1 Score (F ) and BLEU-1 (B ).\n1 1\nHowever, these metrics exhibit significant limitations when evaluating factual accuracy in conversational\ncontexts. Consider a scenario where the ground truth answer is ‘Alice was born in March’ and a system\ngenerates‘AliceisborninJuly.’ Despitecontainingacriticalfactualerrorregardingthebirthmonth,traditional\nmetricswouldassignrelativelyhighscoresduetolexicaloverlapintheremainingtokens(‘Alice,’‘born,’etc.).\nThis fundamental limitation can lead to misleading evaluations that fail to capture semantic correctness.\nTo address these shortcomings, we use LLM-as-a-Judge (J) as a complementary evaluation metric. This\napproach leverages a separate, more capable LLM to assess response quality across multiple dimensions,\nincluding factual accuracy, relevance, completeness, and contextual appropriateness. The judge model\nanalyzes the question, ground truth answer and the generated answer, providing a more nuanced evaluation\nthat aligns better with human judgment. Due to the stochastic nature of J evaluations, we conducted 10\nindependent runs for each method on the entire dataset and report the mean scores along with ±1 standard\ndeviation. More details about the J is present in Appendix A.\n(2)DeploymentMetrics Beyondresponsequality,practicaldeploymentconsiderationsarecrucialforreal-\nworld applications of long-term memory in AI agents. We systematically track Token Consumption, using\n‘cl100k_base’ encoding from tiktoken, measuring the number of tokens extracted during retrieval that\nserve as context for answering queries. For our memory-based models, these tokens represent the memories\nretrieved from the knowledge base, while for RAG-based models, they correspond to the total number of\ntokens in the retrieved text chunks. This distinction is important as it directly affects operational costs and\nsystemefficiency—whetherprocessingconcisememoryfactsorlargerrawtextsegments. Wefurthermonitor\nLatency, (i) search latency: which captures the total time required to search the memory (in memory-based\nsolutions) or chunk (in RAG-based solutions) and (ii) total latency: time to generate appropriate responses,\nconsisting of both retrieval time (accessing memories or chunks) and answer generation time using the LLM.\nThe relationship between these metrics reveals important trade-offs in system design. For instance,\nmore sophisticated memory architectures might achieve higher factual accuracy but at the cost of increased\ntoken consumption and latency. Our multi-dimensional evaluation methodology enables researchers and\npractitioners to make informed decisions based on their specific requirements, whether prioritizing response\nquality for critical applications or computational efficiency for real-time deployment scenarios.\n3.3. Baselines\nTo comprehensively evaluate our approach, we compare against six distinct categories of baselines that\nrepresent the current state of conversational memory systems. These diverse baselines collectively provide a\n7\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nrobustframeworkforevaluatingtheeffectivenessofdifferentmemoryarchitecturesacrossvariousdimensions,\nincluding factual accuracy, computational efficiency, and scalability to extended conversations. Where\napplicable, unless otherwise specified, we set the temperature to 0 to ensure the runs are as reproducible as\npossible.\nEstablished LOCOMO Benchmarks We first establish a comparative foundation by evaluating previously\nbenchmarked methods on the LOCOMO dataset. These include five established approaches: LoCoMo (Maha-\nrana et al., 2024), ReadAgent (Lee et al., 2024), MemoryBank (Zhong et al., 2024), MemGPT (Packer et al.,\n2023), and A-Mem (Xu et al., 2025). These established benchmarks not only provide direct comparison\npoints with published results but also represent the evolution of conversational memory architectures across\ndifferent algorithmic paradigms. For our evaluation, we select the metrics where gpt-4o-mini was used\nfor the evaluation. More details about these benchmarks are mentioned in Appendix C.\nOpen-SourceMemorySolutions Oursecondcategoryconsistsofpromisingopen-sourcememoryarchitec-\ntures such as LangMem2 (Hot Path) that have demonstrated effectiveness in related conversational tasks but\nhavenotyetbeenevaluatedontheLOCOMOdataset. Byadaptingthesesystemstoourevaluationframework,\nwe broaden the comparative landscape and identify potential alternative approaches that may offer competi-\ntive performance. We initialized the LLM with gpt-4o-mini and used text-embedding-small-3 as the\nembedding model.\nRetrieval-Augmented Generation (RAG) As a baseline, we treat the entire conversation history as a\ndocumentcollectionandapplyastandardRAGpipeline. Wefirstsegmenteachconversationintofixed-length\nchunks (128, 256, 512, 1024, 2048, 4096, and 8192 tokens), where 8192 is the maximum chunk size\nsupportedbyourembeddingmodel. AllchunksareembeddedusingOpenAI’stext-embedding-small-3\nto ensure consistent vector quality across configurations. At query time, we retrieve the top k chunks by\nsemanticsimilarityandconcatenatethemascontextforanswergeneration. Throughoutourexperimentswe\nset k∈{1,2}: with k=1 only the single most relevant chunk is used, and with k=2 the two most relevant\nchunks(upto16384tokens)areconcatenated. Weavoidk > 2 sincetheaverageconversationlength(26000\ntokens) would be fully covered, negating the benefits of selective retrieval. By varying chunk size and k, we\nsystematically evaluate RAG performance on long-term conversational memory tasks.\nFull-Context Processing We adopt a straightforward approach by passing the entire conversation history\nwithin the context window of the LLM. This method leverages the model’s inherent ability to process\nsequentialinformationwithoutadditionalarchitecturalcomponents. Whileconceptuallysimple,thisapproach\nfaces practical limitations as conversation length increases, eventually increasing token cost and latency.\nNevertheless, it establishes an important reference point for understanding the value of more sophisticated\nmemory mechanisms compared to direct processing of available context.\nProprietaryModels WeevaluateOpenAI’smemory3featureavailableintheirChatGPTinterface,specifically\nusinggpt-4o-miniforconsistency. WeingestentireLOCOMOconversationswithaprompt(seeAppendixA)\nintosinglechatsessions,promptingmemorygenerationwithtimestamps,participantnames,andconversation\n2\nhttps://langchain-ai.github.io/langmem/\n3\nhttps://openai.com/index/memory-and-new-controls-for-chatgpt/\n8\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nTable1:Performancecomparisonofmemory-enabledsystemsacrossdifferentquestiontypesintheLOCOMOdataset.\nEvaluationmetricsincludeF1score(F ),BLEU-1(B ),andLLM-as-a-Judgescore(J),withhighervaluesindicating\n1 1\nbetterperformance. A-Mem∗representsresultsfromourre-runofA-MemtogenerateLLM-as-a-Judgescoresbysetting\ntemperatureas0. Mem0g indicatesourproposedarchitectureenhancedwithgraphmemory. Bolddenotesthebest\nperformanceforeachmetricacrossallmethods. (↑)representshigherscoreisbetter.\nSingleHop Multi-Hop OpenDomain Temporal\nMethod\nF 1↑ B 1↑ J↑ F 1↑ B 1↑ J↑ F 1↑ B 1↑ J↑ F 1↑ B 1↑ J↑\nLoCoMo 25.02 19.75 – 12.04 11.16 – 40.36 29.05 – 18.41 14.77 –\nReadAgent 9.15 6.48 – 5.31 5.12 – 9.67 7.66 – 12.60 8.87 –\nMemoryBank 5.00 4.77 – 5.56 5.94 – 6.61 5.16 – 9.68 6.99 –\nMemGPT 26.65 17.72 – 9.15 7.44 – 41.04 34.34 – 25.52 19.44 –\nA-Mem 27.02 20.09 – 12.14 12.00 – 44.65 37.06 – 45.85 36.67 –\nA-Mem* 20.76 14.90 39.79±0.38 9.22 8.81 18.85±0.31 33.34 27.58 54.05±0.22 35.40 31.08 49.91±0.31\nLangMem 35.51 26.86 62.23±0.75 26.04 22.32 47.92±0.47 40.91 33.63 71.12±0.20 30.75 25.84 23.43±0.39\nZep 35.74 23.30 61.70±0.32 19.37 14.82 41.35±0.48 49.56 38.92 76.60±0.13 42.00 34.53 49.31±0.50\nOpenAI 34.30 23.72 63.79±0.46 20.09 15.42 42.92±0.63 39.31 31.16 62.29±0.12 14.04 11.25 21.71±0.20\nMem0 38.72 27.13 67.13±0.65 28.64 21.58 51.15±0.31 47.65 38.72 72.93±0.11 48.93 40.51 55.51±0.34\nMem0g 38.09 26.03 65.71±0.45 24.32 18.82 47.19±0.67 49.27 40.30 75.71±0.21 51.55 40.28 58.13±0.44\ntext. These generated memories are then used as complete context for answering questions about each\nconversation, intentionally granting the OpenAI approach privileged access to all memories rather than\nonly question-relevant ones. This methodology accommodates the lack of external API access for selective\nmemory retrieval in OpenAI’s system for benchmarking.\nMemoryProviders WeincorporateZep(Rasmussenetal.,2025),amemorymanagementplatformdesigned\nfor AI agents. Using their platform version, we conduct systematic evaluations across the LOCOMO dataset,\nmaintaining temporal fidelity by preserving timestamp information alongside conversational content. This\ntemporalanchoringensuresthattime-sensitivequeriescanbeaddressedthroughappropriatelycontextualized\nmemory retrieval, particularly important for evaluating questions that require chronological awareness.\nThis baseline represents an important commercial implementation of memory management specifically\nengineered for AI agents.\n4. Evaluation Results, Analysis and Discussion.\n4.1. Performance Comparison Across Memory-Enabled Systems\nTable1reportsF ,B andJscoresforourtwoarchitectures—Mem0andMem0g—againstasuiteofcompetitive\n1 1\nbaselines,asmentionedinSection3,onsingle-hop,multi-hop,open-domain,andtemporalquestions. Overall,\nboth of our models set new state-of-the-art marks in all the three evaluation metrics for most question types.\nSingle-Hop Question Performance Single-hop queries involve locating a single factual span contained\nwithinonedialogueturn. Leveragingitsdensememoriesinnaturallanguagetext,Mem0securesthestrongest\nresults:F =38.72,B =27.13,andJ=67.13. Augmentingthenaturallanguagememorieswithgraphmemory\n1 1\n(Mem0g) yields marginal performance drop compared to Mem0, indicating that relational structure provides\nlimitedutilitywhentheretrievaltargetoccupiesasingleturn. Amongtheexistingbaselines,thefull-context\nOpenAIrunattainsthenext-bestJscore,reflectingthebenefitsofretainingtheentireconversationincontext,\nwhileLangMemandZepbothscorearound8%relativelylessagainstourmodelsonJscore. PreviousLOCOMO\n9\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nbenchmarks such as A-mem lag by more than 25 points in J, underscoring the necessity of fine-grained,\nstructured memory indexing even for simple retrieval tasks.\nMulti-Hop Question Performance Multi-hop queries require synthesizing information dispersed across\nmultipleconversationsessions,posingsignificantchallengesinmemoryintegrationandretrieval. Mem0clearly\noutperforms other methods with an F score of 28.64 and a J score of 51.15, reflecting its capability to\n1\nefficiently retrieve and integrate disparate information stored across sessions. Interestingly, the addition\nof graph memory in Mem0g does not provide performance gains here, indicating potential inefficiencies\nor redundancies in structured graph representations for complex integrative tasks compared to dense\nnatural language memory alone. Baselines like LangMem show competitive performances, but their scores\nsubstantially trail those of Mem0, emphasizing the advantage of our refined memory indexing and retrieval\nmechanisms for complex query processing.\nOpen-DomainPerformance Inopen-domainsettings,thebaselineZepachievesthehighestF (49.56)and\n1\nJ (76.60) scores, edging out our methods by a narrow margin. In particular, Zep’s J score of 76.60 surpasses\nMem0g’s 75.71 by just 0.89 percentage points and outperforms Mem0’s 72.93 by 3.67 points, highlighting a\nconsistent,ifslight,advantageinintegratingconversationalmemorywithexternalknowledge. Mem0gremains\nastrongrunner-up,withaJof75.71reflectinghighfactualretrievalprecision,whileMem0followswith72.93,\ndemonstrating robust coherence. These results underscore that although structured relational memories (as\nin Mem0 and Mem0g) substantially improve open-domain retrieval, Zep maintains a small but meaningful\nlead.\nTemporal Reasoning Performance Temporal reasoning tasks hinge on accurate modeling of event se-\nquences, their relative ordering, and durations within conversational history. Our architectures demonstrate\nsubstantial improvements across all metrics, with Mem0g achieving the highest F (51.55) and J (58.13),\n1\nsuggesting that structured relational representations in addition to natural language memories significantly\naidintemporallygroundedjudgments. Notably,thebasevariant,Mem0,alsoprovideadecentJscore(55.51),\nsuggestingthatnaturallanguagealonecanaidintemporallygroundedjudgments. Amongbaselines,OpenAI\nnotably underperforms, with scores below 15%, primarily due to missing timestamps in most generated\nmemories despite explicit prompting in the OpenAI ChatGPT to extract memories with timestamps. Other\nbaselines such as A-Mem achieve respectable results, yet our models clearly advance the state-of-the-art,\nemphasizing the critical advantage of accurately leveraging both natural language contextualization and\nstructured graph representations for temporal reasoning.\n4.2. Cross-Category Analysis\nThe comprehensive evaluation across diverse question categories reveals that our proposed architectures,\nMem0 and Mem0g, consistently achieve superior performance compared to baseline systems. For single-hop\nqueries, Mem0 demonstrates particularly strong performance, benefiting from its efficient dense natural\nlanguage memory structure. Although graph-based representations in Mem0g slightly lag behind in lexical\noverlap metrics for these simpler queries, they significantly enhance semantic coherence, as demonstrated\nby competitive J scores. This indicates that graph structures are more beneficial in scenarios involving\nnuancedrelationalcontextratherthanstraightforwardretrieval. Formulti-hopquestions,Mem0exhibitsclear\nadvantagesbyeffectivelysynthesizingdispersedinformationacrossmultiplesessions,confirmingthatnatural\nlanguage memories provide sufficient representational richness for these integrative tasks. Surprisingly, the\n10\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nTable 2: Performance comparison of various baselines with proposed methods. Latency measurements show p50\n(median)andp95(95thpercentile)valuesinsecondsforbothsearchtime(timetakentofetchmemories/chunks)and\ntotaltime(timetogeneratethecompleteresponse). OverallLLM-as-a-Judgescore(J)representsthequalitymetricof\nthegeneratedresponsesontheentireLOCOMOdataset.\nLatency (seconds)\nOverall\nMethod\nSearch Total\nJ\nchunk size /\nK p50 p95 p50 p95\nmemory tokens\n128 0.281 0.823 0.774 1.825 47.77 ± 0.23%\n256 0.251 0.710 0.745 1.628 50.15 ± 0.16%\n512 0.240 0.639 0.772 1.710 46.05 ± 0.14%\n1 1024 0.240 0.723 0.821 1.957 40.74 ± 0.17%\n2048 0.255 0.752 0.996 2.182 37.93 ± 0.12%\n4096 0.254 0.719 1.093 2.711 36.84 ± 0.17%\n8192 0.279 0.838 1.396 4.416 44.53 ± 0.13%\nRAG\n128 0.267 0.624 0.766 1.829 59.56 ± 0.19%\n256 0.255 0.699 0.802 1.907 60.97 ± 0.20%\n512 0.247 0.746 0.829 1.729 58.19 ± 0.18%\n2 1024 0.238 0.702 0.860 1.850 50.68 ± 0.13%\n2048 0.261 0.829 1.101 2.791 48.57 ± 0.22%\n4096 0.266 0.944 1.451 4.822 51.79 ± 0.15%\n8192 0.288 1.124 2.312 9.942 60.53 ± 0.16%\nFull-context 26031 - - 9.870 17.117 72.90 ± 0.19%\nA-Mem 2520 0.668 1.485 1.410 4.374 48.38 ± 0.15%\nLangMem 127 17.99 59.82 18.53 60.40 58.10 ± 0.21%\nZep 3911 0.513 0.778 1.292 2.926 65.99 ± 0.16%\nOpenAI 4437 - - 0.466 0.889 52.90 ± 0.14%\nMem0 1764 0.148 0.200 0.708 1.440 66.88 ± 0.15%\nMem0g 3616 0.476 0.657 1.091 2.590 68.44 ± 0.17%\nexpected relational advantages of Mem0g do not translate into better outcomes here, suggesting potential\noverhead or redundancy when navigating more intricate graph structures in multi-step reasoning scenarios.\nIn temporal reasoning, Mem0g substantially outperforms other methods, validating that structured\nrelationalgraphsexcelincapturingchronologicalrelationshipsandeventsequences. Thepresenceofexplicit\nrelational context significantly enhances Mem0g’s temporal coherence, outperforming Mem0’s dense memory\nstorage and highlighting the importance of precise relational representations when tracking temporally\nsensitive information. Open-domain performance further reinforces the value of relational modeling. Mem0g,\nbenefiting from the relational clarity of graph-based memory, closely competes with the top-performing\nbaseline (Zep). This competitive result underscores Mem0g’s robustness in integrating external knowledge\nthrough relational clarity, suggesting an optimal synergy between structured memory and open-domain\ninformation synthesis.\n11\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\n(a)Comparisonofsearchlatencyatp50(median)andp95(95thpercentile)acrossdifferentmemorymethods(Mem0,Mem0g,best\nRAGvariant,Zep,LangMem,andA-Mem).ThebarheightsrepresentJscores(leftaxis),whilethelineplotsshowsearchlatencyin\nseconds(rightaxisscaledinlog).\n(b)Comparisonoftotalresponselatencyatp50andp95acrossdifferentmemorymethods(Mem0,Mem0g,bestRAGvariant,Zep,\nLangMem,OpenAI,full-context,andA-Mem).ThebarheightsrepresentJscores(leftaxis),andthelineplotscaptureend-to-end\nlatencyinseconds(rightaxisscaledinlog).\nFigure4:LatencyAnalysisofDifferentMemoryApproaches. ThesesubfiguresillustratetheJscoresandlatency\ncomparisonofvariousselectedmethodsfromTable2. Subfigure(a)highlightsthesearch/retrievallatencypriorto\nanswergeneration,whileSubfigure(b)showsthetotallatency(includingLLMinference). Bothplotsoverlayeach\nmethod’sJscoreforaholisticviewoftheiraccuracyandefficiency.\nOverall,ouranalysisindicatescomplementarystrengthsof Mem0andMem0g acrossvarioustaskdemands:\ndense, natural-language-based memory offers significant efficiency for simpler queries, while explicit rela-\ntional modeling becomes essential for tasks demanding nuanced temporal and contextual integration. These\nfindings reinforce the importance of adaptable memory structures tailored to specific reasoning contexts in\nAI agent deployments.\n12\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\n4.3. Performance Comparison of Mem0 and Mem0g Against RAG Approaches and Full-Context Model\nComparisons in Table 2, focusing on the ‘Overall J’ column, reveal that both Mem0 and Mem0g consistently\noutperformallRAGconfigurations,whichvarychunksizes(128–8192tokens)andretrieveeitherone(k=1)\nor two (k=2) chunks. Even the strongest RAG approach peaks at around 61% in the J metric, whereas\nMem0 reaches 67%—about a 10% relative improvement—and Mem0g reaches over 68%, achieving around\na 12% relative gain. These advances underscore the advantage of capturing only the most salient facts in\nmemory, rather than retrieving large chunk of original text. By converting the conversation history into\nconcise, structured representations, Mem0 and Mem0g mitigate noise and surface more precise cues to the\nLLM, leading to better answers as evaluated by an external LLM (J).\nDespite these improvements, a full-context method that ingests a chunk of roughly 26,000 tokens still\nachieves the highest J score (approximately 73%). However, as shown in Figure 4b, it also incurs a very\nhigh total p95 latency—around 17 seconds—since the model must read the entire conversation on every\nquery. Bycontrast,Mem0andMem0g significantlyreducetokenusageandthusachievelowerp95latenciesof\naround 1.44 seconds (a 92% reduction) and 2.6 seconds (a 85% reduction), respectively over full-context\napproach. Althoughthefull-contextapproachcanprovideaslightaccuracyedge,thememory-basedsystems\nofferamorepracticaltrade-off,maintainingnear-competitivequalitywhileimposingonlyafractionofthe\ntoken and latency cost. As conversation length increases, full-context approaches suffer from exponential\ngrowth in computational overhead (evident in Table 2 where total p95 latency increases significantly with\nlarger k values or chunk sizes). This increase in input chunks leads to longer response times and higher\ntokenconsumptioncosts. Incontrast,memory-focusedapproacheslikeMem0andMem0g maintainconsistent\nperformance regardless of conversation length, making them substantially more viable for production-scale\ndeployments where efficiency and responsiveness are critical.\n4.4. Latency Analysis\nTable 2 provides a comprehensive performance comparison of various retrieval and memory methodologies,\npresenting median (p50) and tail (p95) latencies for both the search phase and total response generation\nacross the LOCOMO dataset. Our analysis reveals distinct performance patterns governed by architectural\nchoices. Memory-centric architectures demonstrate different performance characteristics. A-Mem, despite its\nlargermemorystore,incurssubstantialsearchoverhead(p50: 0.668s),resultingintotalmedianlatenciesof\n1.410s. LangMem exhibits even higher search latencies (p50: 17.99s, p95: 59.82s), rendering it impractical\nfor interactive applications. Zep achieves moderate performance (p50 total: 1.292s). The full-context\nbaseline, which processes the entire conversation history without retrieval, fundamentally differs from\nretrieval-basedapproaches. Bypassingtheentireconversationcontext(26000tokens)directlytotheLLM,it\neliminates search overhead but incurs extreme total latencies (p50: 9.870s, p95: 17.117s). Similarly, the\nOpenAIimplementationdoesnotperformmemorysearch,asitprocessesmanuallyextractedmemoriesfrom\ntheir playground. While this approach achieves impressive response generation times (p50: 0.466s, p95:\n0.889s), it requires pre-extraction of relevant context, which is not reflected in the reported metrics.\nOur proposed Mem0 approach achieves the lowest search latency among all methods (p50: 0.148s, p95:\n0.200s) as illustrated in Figure 4a. This efficiency stems from our selective memory retrieval mechanism\nand infra improvements that dynamically identifies and retrieves only the most salient information rather\nthan fixed-size chunks. Consequently, Mem0 maintains the lowest total median latency (0.708s) with\nremarkably contained p95 values (1.440s), making it particularly suitable for latency-sensitive applications\nsuch as interactive AI agents. The graph-enhanced Mem0g variant introduces additional relational modeling\ncapabilities at a moderate latency cost, with search times (0.476s) still outperforming all existing memory\n13\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nsolutionsandbaselines. Despitethisincrease,Mem0g maintainscompetitivetotallatencies(p50: 1.091s,p95:\n2.590s) while achieving the highest J score (68.44%) across all methods—trailing only the computationally\nprohibitive full-context approach. This performance profile demonstrates our methods’ ability to balance\nresponse quality and computational efficiency, offering a compelling solution for production AI agents where\nboth factors are critical constraints.\n4.5. Memory System Overhead: Token Analysis and Construction Time\nWe measure the average token budget required to materialise each system’s long-term memory store.\nMem0 encodes complete dialogue turns in a natural language representation and therefore occupies only 7k\ntokens per conversation on an average. Where as Mem0g roughly doubles the footprint to 14k tokens, due to\nthe introduction of graph memories which includes nodes and corresponding relationships. In stark contrast,\nZep’s memory graph consumes in excess of 600k tokens. The inflation arises from Zep’s design choice to\ncache a full abstractive summary at every node while also storing facts on the connecting edges, leading\nto extensive redundancy across the graph. For perspective, supplying the entire raw conversation context\nto the language model—without any memory abstraction—amounts to roughly 26k tokens on average, 20\ntimeslessrelativetoZep’sgraph. Beyondtokeninefficiency,ourexperimentsrevealedsignificantoperational\nbottlenecks with Zep. After adding memories to Zep’s system, we observed that immediate memory retrieval\nattempts often failed to answer our queries correctly. Interestingly, re-running identical searches after a\ndelayofseveralhoursyieldedconsiderablybetterresults. ThislatencysuggeststhatZep’sgraphconstruction\ninvolves multiple asynchronous LLM calls and extensive background processing, making the memory system\nimpracticalforreal-timeapplications. Incontrast,Mem0graphconstructioncompletesinunderaminuteeven\nin worst-case scenarios, allowing users to immediately leverage newly added memories for query responses.\nThese findings highlight that Zep not only replicates identical knowledge fragments across multiple\nnodes,butalsointroducessignificantoperationaldelays. Ourarchitectures—Mem0andMem0g—preservethe\nsame information at a fraction of the token cost and with substantially faster memory availability, offering a\nmore memory-efficient and operationally responsive representation.\n5. Conclusion and Future Work\nWehave introducedMem0andMem0g, two complementary memory architecturesthat overcome theintrinsic\nlimitations of fixed context windows in LLMs. By dynamically extracting, consolidating, and retrieving\ncompactmemoryrepresentations,Mem0achievesstate-of-the-artperformanceacrosssingle-hopandmulti-hop\nreasoning,whileMem0g’sgraph-basedextensionsunlocksignificantgainsintemporalandopen-domaintasks.\nOn the LOCOMO benchmark, our methods deliver 5%, 11%, and 7% relative improvements in single-hop,\ntemporal, and multi-hop reasoning question types over best performing methods in respective question\ntype and reduce p95 latency by over 91% compared to full-context baselines—demonstrating a powerful\nbalance between precision and responsiveness. Mem0’s dense memory pipeline excels at rapid retrieval\nfor straightforward queries, minimizing token usage and computational overhead. In contrast, Mem0g’s\nstructured graph representations provide nuanced relational clarity, enabling complex event sequencing\nand rich context integration without sacrificing practical efficiency. Together, they form a versatile memory\ntoolkit that adapts to diverse conversational demands while remaining deployable at scale.\nFuture research directions include optimizing graph operations to reduce the latency overhead in Mem0g,\nexploring hierarchical memory architectures that blend efficiency with relational representation, and de-\nveloping more sophisticated memory consolidation mechanisms inspired by human cognitive processes.\n14\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nAdditionally, extending our memory frameworks to domains beyond conversational scenarios, such as proce-\nduralreasoningandmultimodalinteractions,wouldfurthervalidatetheirbroaderapplicability. Byaddressing\nthefundamentallimitationsoffixedcontextwindows,ourworkrepresentsasignificantadvancementtoward\nconversational AI systems capable of maintaining coherent, contextually rich interactions over extended\nperiods, much like their human counterparts.\n6. Acknowledgments\nWewouldliketoexpressoursinceregratitudetoHarshAgarwal,ShyamalAnadkat,PrithvijitChattopadhyay,\nSiddesh Choudhary, Rishabh Jain, and Vaibhav Pandey for their invaluable insights and thorough reviews of\nearly drafts. Their constructive comments and detailed suggestions helped refine the manuscript, enhancing\nboth its clarity and overall quality. We deeply appreciate their generosity in dedicating time and expertise to\nthis work.\nReferences\nAnthropic. Model card and evaluations for claude models. Technical report, Anthropic, February 2025. URL\nhttps://www.anthropic.com/news/claude-3-7-sonnet.\nJan Assmann. Communicative and cultural memory. In Cultural memories: The geographical point of view,\npages 15–27. Springer, 2011.\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural\nInformation Processing Systems, 35:11079–11091, 2022.\nPrateekChhikara,JiaruiZhang,FilipIlievski,JonathanFrancis,andKaixinMa. Knowledge-enhancedagents\nfor interactive text games. In Proceedings of the 12th Knowledge Capture Conference 2023, pages 157–165,\n2023.\nFergus IM Craik and Janine M Jennings. Human memory. 1992.\nGaurav Goswami. Dissecting the metrics: How different evaluation approaches yield diverse results for\nconversational ai. Authorea Preprints, 2025.\nTianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael Jordan, and Song Mei. Active-dormant attention\nheads: Mechanistically demystifying extreme-token phenomena in llms. In NeurIPS 2024 Workshop on\nMathematics of Modern Machine Learning, 2024.\nKostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh\nDannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term memory in\nllm-agents. In Proceedings of the AAAI Symposium Series, volume 2, pages 277–280, 2023.\nZihong He, Weizhe Lin, Hao Zheng, Fan Zhang, Matt W Jones, Laurence Aitchison, Xuhai Xu, Miao Liu,\nPer Ola Kristensson, and Junxiao Shen. Human-inspired perspectives: A survey on ai long-term memory.\narXiv preprint arXiv:2411.00489, 2024.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila\nWelihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\n15\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Alek-\nsander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720,\n2024.\nKuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. A human-inspired reading\nagent with gist memory of very long contexts. In International Conference on Machine Learning, pages\n26396–26415. PMLR, 2024.\nLei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-\nmemory: Recallingandpost-thinkingenablellmswithlong-termmemory. arXivpreprintarXiv:2311.08719,\n2023.\nAdyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang.\nEvaluatingverylong-termconversationalmemoryofllmagents. InProceedingsofthe62ndAnnualMeeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851–13870, 2024.\nBodhisattwaPrasadMajumder,BhavanaDalviMishra,PeterJansen,OyvindTafjord,NiketTandon,LiZhang,\nChrisCallison-Burch,andPeterClark. Clin: Acontinuallylearninglanguageagentforrapidtaskadaptation\nand generalization. In First Conference on Language Modeling.\nElliot Nelson, Georgios Kollias, Payel Das, Subhajit Chaudhury, and Soham Dan. Needle in the haystack for\nmemory based large language models. arXiv preprint arXiv:2407.01437, 2024.\nCharles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt:\nTowards llms as operating systems. 2023.\nPreston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. Zep: A temporal\nknowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956, 2025.\nParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,andChristopherDManning. Raptor:\nRecursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on\nLearning Representations, 2024.\nNoahShinn,FedericoCassano,AshwinGopinath,KarthikNarasimhan,andShunyuYao. Reflexion: Language\nagents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634–\n8652, 2023.\nPrabhjot Singh, Prateek Chhikara, and Jasmeet Singh. An ensemble approach for extractive text summa-\nrization. In 2020 International Conference on Emerging Trends in Information Technology and Engineering\n(ic-ETITE), pages 1–7. IEEE, 2020.\nArpita Soni, Rajeev Arora, Anoop Kumar, and Dheerendra Panwar. Evaluating domain coverage in low-\nresource generative chatbots: A comparative study of open-domain and closed-domain approaches using\nbleuscores. In2024InternationalConferenceonElectricalElectronicsandComputingTechnologies(ICEECT),\nvolume 1, pages 1–6. IEEE, 2024.\nGemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien\nVincent,ZhufengPan,ShiboWang,etal. Gemini1.5: Unlockingmultimodalunderstandingacrossmillions\nof tokens of context. arXiv preprint arXiv:2403.05530, 2024.\nJoan C Timoneda and Sebastián Vallejo Vera. Memory is all you need: Testing how model memory affects\nllm performance in annotation tasks. arXiv preprint arXiv:2503.04874, 2025.\n16\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nWujiangXu,ZujieLiang,KaiMei,HangGao,JuntaoTan,andYongfengZhang. A-mem: Agenticmemoryfor\nllm agents. arXiv preprint arXiv:2502.12110, 2025.\nYangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W Suchow,\nandKhaldounKhashanah. Finmem: Aperformance-enhancedllmtradingagentwithlayeredmemoryand\ncharacter design. In Proceedings of the AAAI Symposium Series, volume 3, pages 595–597, 2024.\nJiaruiZhang. Guidedprofilegenerationimprovespersonalizationwithlargelanguagemodels. InFindingsof\nthe Association for Computational Linguistics: EMNLP 2024, pages 4005–4016, 2024.\nJiarui Zhang, Filip Ilievski, Kaixin Ma, Jonathan Francis, and Alessandro Oltramari. A study of zero-shot\nadaptation with commonsense knowledge. In AKBC, 2022.\nZeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-\nRong Wen. A survey on the memory mechanism of large language model based agents. arXiv preprint\narXiv:2404.13501, 2024.\nWanjunZhong,LianghongGuo,QiqiGao,HeYe,andYanlinWang. Memorybank: Enhancinglargelanguage\nmodels with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,\npages 19724–19731, 2024.\n17\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nAppendix\nA. Prompts\nIn developing our LLM-as-a-Judge prompt, we adapt elements from the prompt released by Packer et al.\n(2023).\nPrompt Template for LLM as a Judge\nYour task is to label an answer to a question as \"CORRECT\" or \"WRONG\". You will be given\nthe following data: (1) a question (posed by one user to another user), (2) a ‘gold’\n(ground truth) answer, (3) a generated answer which you will score as CORRECT/WRONG.\nThe point of the question is to ask about something one user should know about the other\nuser based on their prior conversations. The gold answer will usually be a concise and\nshort answer that includes the referenced topic, for example:\nQuestion: Do you remember what I got the last time I went to Hawaii?\nGold answer: A shell necklace\nThe generated answer might be much longer, but you should be generous with your grading\n- as long as it touches on the same topic as the gold answer, it should be counted as\nCORRECT.\nFor time related questions, the gold answer will be a specific date, month, year, etc. The\ngenerated answer might be much longer or use relative time references (like ‘last Tuesday’\nor ‘next month’), but you should be generous with your grading - as long as it refers to\nthe same date or time period as the gold answer, it should be counted as CORRECT. Even if\nthe format differs (e.g., ‘May 7th’ vs ‘7 May’), consider it CORRECT if it’s the same date.\nNow it’s time for the real question:\nQuestion: {question}\nGold answer: {gold_answer}\nGenerated answer: {generated_answer}\nFirst, provide a short (one sentence) explanation of your reasoning, then finish with\nCORRECT or WRONG. Do NOT include both CORRECT and WRONG in your response, or it will break\nthe evaluation script.\nJust return the label CORRECT or WRONG in a json format with the key as \"label\".\n18\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nPrompt Template for Results Generation (Mem0)\nYou are an intelligent memory assistant tasked with retrieving accurate information from\nconversation memories.\n# CONTEXT:\nYou have access to memories from two speakers in a conversation. These memories contain\ntimestamped information that may be relevant to answering the question.\n# INSTRUCTIONS:\n1. Carefully analyze all provided memories from both speakers\n2. Pay special attention to the timestamps to determine the answer\n3. If the question asks about a specific event or fact, look for direct evidence in the\nmemories\n4. If the memories contain contradictory information, prioritize the most recent memory\n5. If there is a question about time references (like \"last year\", \"two months ago\",\netc.), calculate the actual date based on the memory timestamp. For example, if a memory\nfrom 4 May 2022 mentions \"went to India last year,\" then the trip occurred in 2021.\n6. Always convert relative time references to specific dates, months, or years. For\nexample, convert \"last year\" to \"2022\" or \"two months ago\" to \"March 2023\" based on the\nmemory timestamp. Ignore the reference while answering the question.\n7. Focus only on the content of the memories from both speakers. Do not confuse character\nnames mentioned in memories with the actual users who created those memories.\n8. The answer should be less than 5-6 words.\n# APPROACH (Think step by step):\n1. First, examine all memories that contain information related to the question\n2. Examine the timestamps and content of these memories carefully\n3. Look for explicit mentions of dates, times, locations, or events that answer the\nquestion\n4. If the answer requires calculation (e.g., converting relative time references), show\nyour work\n5. Formulate a precise, concise answer based solely on the evidence in the memories\n6. Double-check that your answer directly addresses the question asked\n7. Ensure your final answer is specific and avoids vague time references\nMemories for user {speaker_1_user_id}:\n{speaker_1_memories}\nMemories for user {speaker_2_user_id}:\n{speaker_2_memories}\nQuestion: {question}\nAnswer:\n19\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nPrompt Template for Results Generation (Mem0g)\n(same as previous)\n# APPROACH (Think step by step):\n1. First, examine all memories that contain information related to the question\n2. Examine the timestamps and content of these memories carefully\n3. Look for explicit mentions of dates, times, locations, or events that answer the\nquestion\n4. If the answer requires calculation (e.g., converting relative time references), show\nyour work\n5. Analyze the knowledge graph relations to understand the user’s knowledge context\n6. Formulate a precise, concise answer based solely on the evidence in the memories\n7. Double-check that your answer directly addresses the question asked\n8. Ensure your final answer is specific and avoids vague time references\nMemories for user {speaker_1_user_id}:\n{speaker_1_memories}\nRelations for user {speaker_1_user_id}:\n{speaker_1_graph_memories}\nMemories for user {speaker_2_user_id}:\n{speaker_2_memories}\nRelations for user {speaker_2_user_id}:\n{speaker_2_graph_memories}\nQuestion: {question}\nAnswer:\nPrompt Template for OpenAI ChatGPT\nCan you please extract relevant information from this conversation and create memory\nentries for each user mentioned? Please store these memories in your knowledge base in\naddition to the timestamp provided for future reference and personalized interactions.\n(1:56 pm on 8 May, 2023) Caroline: Hey Mel! Good to see you! How have you been?\n(1:56 pm on 8 May, 2023) Melanie: Hey Caroline! Good to see you! I’m swamped with the\nkids & work. What’s up with you? Anything new?\n(1:56 pm on 8 May, 2023) Caroline: I went to a LGBTQ support group yesterday and it was so\npowerful.\n...\n20\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nB. Algorithm\nAlgorithm 1 Memory Management System: Update Operations\n1: Input: Set of retrieved memories F, Existing memory store M = {m ,m ,...,m }\n1 2 n\n2: Output: Updated memory store M ′\n3: procedure UpdateMemory(F,M)\n4: for each fact f ∈ F do\n5: operation ← ClassifyOperation (f,M) ▷ Execute appropriate operation based on\nclassification\n6: if operation = ADD then\n7: id ← GenerateUniqueID ()\n8: M ← M∪ {(id, f, \"ADD\" )} ▷ Add new fact with unique identifier\n9: else if operation = UPDATE then\n10: m ← FindRelatedMemory (f,M)\ni\n11: if InformationContent (f) > InformationContent (m ) then\ni\n12: M ← (M\\{m }) ∪ {(id , f, \"UPDATE\" )} ▷ Replace with richer information\ni i\n13: end if\n14: else if operation = DELETE then\n15: m ← FindContradictedMemory (f,M)\ni\n16: M ← M\\{m } ▷ Remove contradicted information\ni\n17: else if operation = NOOP then\n18: No operation performed ▷ Fact already exists or is irrelevant\n19: end if\n20: end for\n21: return M\n22: end procedure\n23: function ClassifyOperation(f,M)\n24: if ¬SemanticallySimilar (f,M) then\n25: return ADD ▷ New information not present in memory\n26: else if Contradicts (f,M) then\n27: return DELETE ▷ Information conflicts with existing memory\n28: else if Augments (f,M) then\n29: return UPDATE ▷ Enhances existing information in memory\n30: else\n31: return NOOP ▷ No change required\n32: end if\n33: end function\nC. Selected Baselines\nLoCoMo The LoCoMo framework implements a sophisticated memory pipeline that enables LLM agents to\nmaintain coherent, long-term conversations. At its core, the system divides memory into short-term and\nlong-term components. After each conversation session, agents generate summaries (stored as short-term\nmemory)thatdistillkeyinformationfromthatinteraction. Simultaneously,individualconversationturnsare\n21\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\ntransformed into ‘observations’ - factual statements about each speaker’s persona and life events that are\nstoredinlong-termmemorywithreferencestothespecificdialogturnsthatproducedthem. Whengenerating\nnew responses, agents leverage both the most recent session summary and selectively retrieve relevant\nobservationsfromtheirlong-termmemory. Thisdual-memoryapproachisfurtherenhancedbyincorporating\na temporal event graph that tracks causally connected life events occurring between conversation sessions.\nBy conditioning responses on retrieved memories, current conversation context, persona information, and\nintervening life events, the system enables agents to maintain consistent personalities and recall important\ndetails across conversations spanning hundreds of turns and dozens of sessions.\nReadAgent ReadAgent addresses the fundamental limitations of LLMs by emulating how humans process\nlengthytextsthroughasophisticatedthree-stagepipeline. First,inEpisodePagination,thesystemintelligently\nsegments text at natural cognitive boundaries rather than arbitrary cutoffs. Next, during Memory Gisting,\nit distills each segment into concise summaries that preserve essential meaning while drastically reducing\ntoken count—similar to how human memory retains the substance of information without verbatim recall.\nFinally, when tasked with answering questions, the Interactive Lookup mechanism examines these gists\nand strategically retrieves only the most relevant original text segments for detailed processing. This\nhuman-inspired approach enables LLMs to effectively manage documents up to 20 times longer than their\nnormal context windows. By balancing global understanding through gists with selective attention to\ndetails,ReadAgentachievesbothcomputationalefficiencyandimprovedcomprehension,demonstratingthat\nmimicking human cognitive processes can significantly enhance AI text processing capabilities.\nMemoryBank The MemoryBank system enhances LLMs with long-term memory through a sophisticated\nthree-part pipeline. At its core, the Memory Storage component warehouses detailed conversation logs,\nhierarchical event summaries, and evolving user personality profiles. When a new interaction occurs, the\nMemory Retrieval mechanism employs a dual-tower dense retrieval model to extract contextually relevant\npast information. The Memory Updating component, provides a human-like forgetting mechanism where\nmemories strengthen when recalled and naturally decay over time if unused. This comprehensive approach\nenables AI companions to recall pertinent information, maintain contextual awareness across extended\ninteractions, and develop increasingly accurate user portraits, resulting in more personalized and natural\nlong-term conversations.\nMemGPT The MemGPT system introduces an operating system-inspired approach to overcome the context\nwindow limitations inherent in LLMs. At its core, MemGPT employs a sophisticated memory management\npipelineconsistingofthreekeycomponents: ahierarchicalmemorysystem,self-directedmemoryoperations,\nand an event-based control flow mechanism. The system divides available memory into ‘main context’\n(analogous to RAM in traditional operating systems) and ‘external context’ (analogous to disk storage).\nThe main context—which is bound by the LLM’s context window—contains system instructions, recent\nconversation history, and working memory that can be modified by the model. The external context stores\nunlimited information outside the model’s immediate context window, including complete conversation\nhistories and archival data. When the LLM needs information not present in main context, it can initiate\nfunction calls to search, retrieve, or modify content across these memory tiers, effectively ‘paging’ relevant\ninformation in and out of its limited context window. This OS-inspired architecture enables MemGPT to\nmaintain conversational coherence over extended interactions, manage documents that exceed standard\ncontext limits, and perform multi-hop information retrieval tasks—all while operating with fixed-context\n22\nMem0:BuildingProduction-ReadyAIAgentswithScalableLong-TermMemory\nmodels. The system’s ability to intelligently manage its own memory resources provides the illusion of\ninfinite context, significantly extending what’s possible with current LLM technology.\nA-Mem The A-Mem model introduces an agentic memory system designed for LLM agents. This system\ndynamically structures and evolves memories through interconnected notes. Each note captures interactions\nenriched with structured attributes like keywords, contextual descriptions, and tags generated by the\nLLM. Upon creating a new memory, A-MEM uses semantic embeddings to retrieve relevant existing notes,\nthen employs an LLM-driven approach to establish meaningful links based on similarities and shared\nattributes. Crucially, the memory evolution mechanism updates existing notes dynamically, refining their\ncontextual information and attributes whenever new relevant memories are integrated. Thus, memory\nstructurecontinuallyevolves,allowingricherandcontextuallydeeperconnectionsamongmemories. Retrieval\nfrom memory is conducted through semantic similarity, providing relevant historical context during agent\ninteractions\n23"
}